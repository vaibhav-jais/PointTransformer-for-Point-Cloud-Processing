{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "B = 2\n",
    "N = 50\n",
    "S = 5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "group_idx = torch.arange(N, dtype=torch.long).to(device).view(1, 1, N).repeat([B, S, 1])\n",
    "print(group_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "def square_distance_my(original_larger_pc, sampled_centroid_point):\n",
    "    \"\"\"\n",
    "    Calculate the pairwise distance between point in the original point cloud and each sampled centroid point.\n",
    "\n",
    "    src^T * dst = xn * xm + yn * ym + zn * zm;\n",
    "    sum(src^2, dim=-1) = xn*xn + yn*yn + zn*zn;\n",
    "    sum(dst^2, dim=-1) = xm*xm + ym*ym + zm*zm;\n",
    "    dist = (xn - xm)^2 + (yn - ym)^2 + (zn - zm)^2\n",
    "         = sum(src**2,dim=-1)+sum(dst**2,dim=-1)-2*src^T*dst\n",
    "\n",
    "    Input:\n",
    "        original_larger_point: [B, N, C]\n",
    "        sampled_centroid_point: [B, M, C]   M represent the subset of sampled points from N original larger input point cloud\n",
    "    Output:\n",
    "        pairwise_eucl_dist: per-point square distance, [B, N, M]\n",
    "    \"\"\"\n",
    "    B, N, C = original_larger_pc.shape\n",
    "    B, M, C = sampled_centroid_point.shape\n",
    "    euclidean_distance = torch.zeros((B,N,M))\n",
    "    for k in range(B):\n",
    "        for i in range(N):\n",
    "            for j in range(M):\n",
    "                euclidean_distance[k, i, j] = ((original_larger_pc[k, i, 0] - sampled_centroid_point[k,j,0])**2 + (original_larger_pc[k, i, 1] - sampled_centroid_point[k,j,1])**2)\n",
    "\n",
    "    return euclidean_distance        \n",
    "\n",
    "src = torch.tensor([[[-2,3], [3,1], [4,-2], [6,3], [5,1], [2,-3]],\n",
    "                    [[-2,3], [3,1], [4,-2], [6,3], [5,1], [2,-3]]])\n",
    "dst = torch.tensor([[[-2,3], [6,3], [2,-3]],\n",
    "                    [[-2,3], [6,3], [2,-3]]])\n",
    "print(square_distance_my(src.to(device), dst.to(device)))\n",
    "end = time.time()\n",
    "print(f\"time:\", (end - start)*1000)\n",
    "\n",
    "x = torch.randint(0, 10, (2,), dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "path = './data/train_keyframes/scene-0871_frame26.npy'\n",
    "np_array = np.load(path, 'r')\n",
    "print(np.min(np_array[:, 3]), np.max(np_array[:, 3]), np.mean(np_array[:, 3]))\n",
    "print(np.min(np_array[:, 4]), np.max(np_array[:, 4]), np.mean(np_array[:, 4]))\n",
    "print(np.min(np_array[:, 5]), np.max(np_array[:, 5]), np.mean(np_array[:, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "count = torch.cuda.device_count()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "data_root = './data/train_keyframes/'\n",
    "frame = 'scene-0122_frame26.npy'\n",
    "frame_path = os.path.join(data_root, frame)\n",
    "data = np.load(frame_path, 'r')\n",
    "#print(data[:, 6])\n",
    "print(data[:20, 3:6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "data_root = './logs/PointNet++_original_hyperparameters/2023-08-13_00-55/checkpoint_epoch66_npy_predictions/'\n",
    "#frame = 'scene-0014_frame29.npy'\n",
    "frame_path = os.path.join(data_root, frame)\n",
    "data = np.load(frame_path, 'r')\n",
    "#print(data[:, :10, :])\n",
    "print(data.shape)\n",
    "data_reshape = data.reshape(data.shape[1], data.shape[2])\n",
    "print(data_reshape.shape)\n",
    "#print(np.argmax(data_reshape, axis=1))\n",
    "index_counts = np.bincount(np.argmax(data_reshape, axis=1))\n",
    "for index, count in enumerate(index_counts):\n",
    "    print(f\"Index {index} occurs {count} times\")\n",
    "\n",
    "\n",
    "torchscript_data_root = './logs/PointNet++_original_hyperparameters/2023-08-13_00-55/torchscript_epoch66_npy_predictions/'\n",
    "torchscript_frame_path = os.path.join(torchscript_data_root, frame)\n",
    "ts_data = np.load(torchscript_frame_path, 'r')\n",
    "#print(data[:, :10, :])\n",
    "print(ts_data.shape)\n",
    "ts_data_reshape = data.reshape(ts_data.shape[1], ts_data.shape[2])\n",
    "print(ts_data_reshape.shape)\n",
    "#print(np.argmax(ts_data_reshape, axis=1))\n",
    "ts_index_counts = np.bincount(np.argmax(ts_data_reshape, axis=1))\n",
    "for ts_index, ts_count in enumerate(ts_index_counts):\n",
    "    print(f\"TS_Index {ts_index} occurs {ts_count} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(935, 18)\n",
      "float64\n",
      "tensor([[-3.7860e+00, -2.5187e+01, -7.8103e-01,  1.5000e+00,  1.0000e+00,\n",
      "         -1.1245e-01, -3.2063e-02],\n",
      "        [-5.0944e+00, -2.8425e+01, -7.8133e-01,  2.5000e+00,  1.0000e+00,\n",
      "         -3.0244e-02, -9.1614e-03],\n",
      "        [-4.4321e+00, -3.0607e+01, -7.8121e-01,  2.5000e+00,  1.0000e+00,\n",
      "         -6.9875e-02, -1.8177e-02],\n",
      "        [-3.6590e+00, -6.4799e+01, -7.8136e-01,  2.5000e+00,  1.0000e+00,\n",
      "         -8.2007e-02, -1.0154e-02],\n",
      "        [-1.2244e+01, -6.5443e+01, -7.8319e-01,  5.5000e+00,  1.0000e+00,\n",
      "         -6.6255e-02, -1.7028e-02],\n",
      "        [-4.7150e+01, -6.9238e+01, -7.9062e-01,  1.0000e+01,  1.0000e+00,\n",
      "         -7.5137e-02, -5.7754e-02],\n",
      "        [ 2.4138e+00, -4.0016e+00, -7.7952e-01,  3.0000e+00,  1.0000e+00,\n",
      "         -7.0434e-02, -2.2011e-03],\n",
      "        [ 1.9506e+00, -8.8167e+00, -7.7966e-01, -1.0000e+00,  1.0000e+00,\n",
      "         -5.7578e-02, -5.0381e-03],\n",
      "        [ 2.4141e+00, -4.0018e+00, -7.7924e-01,  3.5000e+00,  2.0000e+00,\n",
      "         -6.8934e-02, -2.1542e-03],\n",
      "        [ 1.9440e+00, -8.6169e+00, -7.7946e-01,  0.0000e+00,  2.0000e+00,\n",
      "         -5.5540e-02, -4.9844e-03],\n",
      "        [-6.6495e+00, -1.6062e+01, -7.8246e-01,  1.5000e+00,  2.0000e+00,\n",
      "         -1.9069e-02, -1.2077e-02],\n",
      "        [ 1.4826e+00, -2.0635e+01, -7.7979e-01, -1.0000e+00,  2.0000e+00,\n",
      "         -2.1099e-02, -1.5984e-03]], dtype=torch.float64) tensor([5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 2., 5.], dtype=torch.float64)\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "data_root = './TrainingTooling_Data/nuScenes_dataset_in_AtCity_format/vjaiswalEnd2End/data/newreltime_val_keyframes/'\n",
    "frame = 'scene-0003_frame2.npy'\n",
    "frame_path = os.path.join(data_root, frame)\n",
    "data = np.load(frame_path, 'r')\n",
    "#print(data[:, 6])\n",
    "print(data.shape)\n",
    "#print(data[:2, :])\n",
    "print(data.dtype)\n",
    "frame_radar_point_data = torch.tensor(data[20:32, [0, 1, 2, 7,8, 15, 16]])\n",
    "frame_radar_point_label = torch.tensor(data[20:32, -1])\n",
    "print(frame_radar_point_data, frame_radar_point_label)\n",
    "print(frame_radar_point_label.dtype)\n",
    "#data = data.astype(int)\n",
    "#print(np.argmax(data_reshape, axis=1))\n",
    "#index_counts = np.bincount(data[:, -1])\n",
    "#print(index_counts)\n",
    "#for index, count in enumerate(index_counts):\n",
    "    #print(f\"Index {index} occurs {count} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1382, 18)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 4.\n",
      " 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4. 4.\n",
      " 4. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 5.\n",
      " 5. 5. 5. 5. 5. 5. 5. 5. 5. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 2. 2. 2.]\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "data_root = './TrainingTooling_Data/nuScenes_dataset_in_AtCity_format/vjaiswalEnd2End/data/Data_with_stationary_moving_NOSWEEPS/train_keyframes/'\n",
    "frame = 'scene-0002_frame2.npy'\n",
    "frame_path = os.path.join(data_root, frame)\n",
    "data = np.load(frame_path, 'r')\n",
    "#print(data[:, 6])\n",
    "print(data.shape)\n",
    "print(data[:200, 8])\n",
    "print(data.dtype)\n",
    "\n",
    "#inference_data = data[data[:, 8] == 0, :]\n",
    "#print(inference_data.shape, inference_data[:, 8])\n",
    "#print(data[:inference_data.shape[0]+2, 8])\n",
    "#frame_radar_point_data = torch.tensor(data[100:200, [8]])\n",
    "#frame_radar_point_label = torch.tensor(data[30:50, -1])\n",
    "#print(frame_radar_point_data, frame_radar_point_label)\n",
    "#print(frame_radar_point_label.dtype)\n",
    "#data = data.astype(int)\n",
    "#print(np.argmax(data_reshape, axis=1))\n",
    "#index_counts = np.bincount(data[:, -1])\n",
    "#print(index_counts)\n",
    "#for index, count in enumerate(index_counts):\n",
    "    #print(f\"Index {index} occurs {count} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scene-0001_frame1.npy'] 28130\n",
      "342 14294\n",
      "scene-0517_frame33.npy\n",
      "['scene-0001_frame1.npy']\n",
      "36 4516\n",
      "scene-0185_frame27.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from natsort import natsorted\n",
    "data_root = './TrainingTooling_Data/nuScenes_dataset_in_AtCity_format/vjaiswalEnd2End/data/Data_with_stationary_moving_NOSWEEPS/train_keyframes/'\n",
    "all_frames = natsorted(os.listdir(data_root))\n",
    "max = 0\n",
    "j = 0\n",
    "that_idx = 0\n",
    "print(all_frames[:1], len(all_frames))\n",
    "for i in range(len(all_frames)):\n",
    "    frame_file_path = os.path.join(data_root, all_frames[i])\n",
    "    data = np.load(frame_file_path)\n",
    "    if data.shape[0] > max:\n",
    "        max = data.shape[0]\n",
    "        that_idx = i\n",
    "    else:\n",
    "        max = max\n",
    "    j = i\n",
    "print(max, that_idx)\n",
    "print(all_frames[that_idx])\n",
    "\n",
    "\n",
    "\n",
    "min = 300\n",
    "k = 0\n",
    "min_idx = 0\n",
    "print(all_frames[:1])\n",
    "for i in range(len(all_frames)):\n",
    "    frame_file_path = os.path.join(data_root, all_frames[i])\n",
    "    data = np.load(frame_file_path)\n",
    "    if data.shape[0] < min:\n",
    "        min = data.shape[0]\n",
    "        min_idx = i\n",
    "    else:\n",
    "        min = min\n",
    "    j = i\n",
    "print(min, min_idx)\n",
    "print(all_frames[min_idx])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.14889568e-312]\n",
      " [0.00000000e+000]\n",
      " [3.18299369e-313]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [1.06099790e-312]\n",
      " [1.39736850e-076]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [3.81959242e-313]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]\n",
      " [0.00000000e+000]]\n",
      "[[9.14889568e-312 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000]\n",
      " [3.18299369e-313 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000]\n",
      " [1.06099790e-312 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000]\n",
      " [1.39736850e-076 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000]\n",
      " [3.81959242e-313 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000]\n",
      " [0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000 0.00000000e+000\n",
      "  0.00000000e+000 0.00000000e+000 0.00000000e+000]] (18, 35)\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[0.01395106, 0.01395106, 0.01395106, 0.01395106, 0.01395106,\n",
    "        0.01395106, 0.01395106, 0.01395106, 0.01395106, 0.01395106,\n",
    "        0.01395106, 0.01395106, 0.01395106, 0.01395106, 0.01395106,\n",
    "        0.01395106, 0.01395106, 0.01395106, 0.01395106, 0.01395106,\n",
    "        0.01395106, 0.01395106, 0.01395106, 0.01395106, 0.01395106,\n",
    "        0.01395106, 0.01395106, 0.01395106, 0.01395106, 0.01395106,\n",
    "        0.01395106, 0.01395106, 0.01395106, 0.08425093, 0.08425093,\n",
    "        0.08425093, 0.08425093, 0.08425093, 0.08425093, 0.08425093,\n",
    "        0.08425093, 0.08425093, 0.08425093, 0.08425093, 0.08425093,\n",
    "        0.08425093, 0.08425093, 0.08425093, 0.08425093, 0.08425093,\n",
    "        0.08425093, 0.08425093, 0.08425093, 0.08425093, 0.08425093,\n",
    "        0.08425093, 0.08425093, 0.08425093, 0.08425093, 0.08425093,\n",
    "        0.08425093, 0.08425093, 0.08425093, 0.08425093, 0.08425093,\n",
    "        0.08425093, 0.08425093, 0.08425093, 0.08425093, 0.08425093,\n",
    "        0.08425093, 0.15932608, 0.15932608, 0.15932608, 0.15932608,\n",
    "        0.15932608, 0.15932608, 0.15932608, 0.15932608, 0.15932608,\n",
    "        0.15932608, 0.15932608, 0.15932608, 0.15932608, 0.15932608,\n",
    "        0.15932608, 0.15932608, 0.15932608, 0.15932608, 0.15932608,\n",
    "        0.15932608, 0.15932608, 0.15932608, 0.15932608, 0.15932608,\n",
    "        0.15932608, 0.15932608, 0.15932608, 0.15932608, 0.15932608,\n",
    "        0.15932608, 0.15932608, 0.15932608, 0.15932608, 0.15932608,\n",
    "        0.15932608, 0.15932608, 0.15932608, 0.15932608, 0.15932608,\n",
    "        0.15932608, 0.15932608, 0.15932608, 0.2335999 , 0.2335999 ,\n",
    "        0.2335999 , 0.2335999 , 0.2335999 , 0.2335999 , 0.2335999 ,\n",
    "        0.2335999 , 0.2335999 , 0.2335999 , 0.2335999 , 0.2335999 ,\n",
    "        0.2335999 , 0.2335999 , 0.2335999 , 0.2335999 , 0.2335999 ,\n",
    "        0.2335999 , 0.2335999 , 0.2335999 , 0.2335999 , 0.2335999 ,\n",
    "        0.2335999 , 0.2335999 , 0.2335999 , 0.2335999 , 0.2335999 ,\n",
    "        0.2335999 , 0.2335999 , 0.2335999 , 0.2335999 , 0.2335999 ,\n",
    "        0.2335999 , 0.2335999 , 0.2335999 , 0.2335999 , 0.2335999 ,\n",
    "        0.2335999 , 0.2335999 , 0.2335999 , 0.2335999 , 0.2335999 ,\n",
    "        0.2335999 , 0.2335999 , 0.30367589, 0.30367589, 0.30367589,\n",
    "        0.30367589, 0.30367589, 0.30367589, 0.30367589, 0.30367589,\n",
    "        0.30367589, 0.30367589, 0.30367589, 0.30367589, 0.30367589,\n",
    "        0.30367589, 0.30367589, 0.30367589, 0.30367589, 0.30367589,\n",
    "        0.30367589, 0.30367589, 0.30367589, 0.30367589, 0.30367589,\n",
    "        0.30367589, 0.30367589, 0.30367589, 0.30367589, 0.30367589,\n",
    "        0.30367589, 0.30367589, 0.30367589, 0.30367589, 0.30367589,\n",
    "        0.30367589, 0.30367589, 0.30367589, 0.30367589, 0.30367589,\n",
    "        0.30367589, 0.30367589, 0.30367589, 0.30367589, 0.30367589,\n",
    "        0.30367589, 0.30367589, 0.37934089, 0.37934089, 0.37934089,\n",
    "        0.37934089, 0.37934089, 0.37934089, 0.37934089, 0.37934089,\n",
    "        0.37934089, 0.37934089, 0.37934089, 0.37934089, 0.37934089,\n",
    "        0.37934089, 0.37934089, 0.37934089, 0.37934089, 0.37934089,\n",
    "        0.37934089, 0.37934089, 0.37934089, 0.37934089, 0.37934089,\n",
    "        0.37934089, 0.37934089, 0.37934089, 0.37934089, 0.37934089,\n",
    "        0.37934089, 0.37934089, 0.37934089, 0.37934089, 0.37934089,\n",
    "        0.37934089, 0.37934089, 0.37934089, 0.37934089, 0.37934089,\n",
    "        0.37934089, 0.37934089, 0.37934089, 0.37934089, 0.37934089]])\n",
    "\n",
    "unique_values, counts = np.unique(a, return_counts=True)\n",
    "#print(unique_values, counts)\n",
    "# Calculate the cumulative sum of counts\n",
    "cumulative_counts = np.cumsum(counts)\n",
    "#print(cumulative_counts)\n",
    "# Create a new array based on cumulative counts\n",
    "result = np.zeros(cumulative_counts[-1], dtype=int)\n",
    "# Assign values to the new array based on unique values and their counts\n",
    "# Assign values to the new array based on cumulative counts\n",
    "for i, count in enumerate(cumulative_counts):\n",
    "    if i > 0:\n",
    "        result[cumulative_counts[i - 1]:count] = i\n",
    "\n",
    "#print(result)\n",
    "new_unique_values, new_counts = np.unique(result, return_counts=True)\n",
    "#print(new_unique_values, new_counts)\n",
    "#print(cumulative_counts[-1])\n",
    "\n",
    "x = np.zeros((18,34), dtype=int)\n",
    "#print(x.shape)\n",
    "#print(x[:, 0:20])\n",
    "data_radar = np.empty((18,1))\n",
    "print(data_radar)\n",
    "data_radar = np.concatenate((data_radar, x), axis=1)\n",
    "print(data_radar, data_radar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n",
      "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2\n",
      "  2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      "  2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      "  3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      "  4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      "  5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor i, count in enumerate(cumulative_counts):\\n    if i > 0:\\n        result[cumulative_counts[i - 1]:count] = i\\n\\n#print(result)\\nnew_unique_values, new_counts = np.unique(result, return_counts=True)\\n#print(new_unique_values, new_counts)\\n#print(cumulative_counts[-1])\\n\\nx = np.zeros((18,34), dtype=int)\\n#print(x.shape)\\n#print(x[:, 0:20])\\ndata_radar = np.empty((18,1))\\nprint(data_radar)\\ndata_radar = np.concatenate((data_radar, x), axis=1)\\nprint(data_radar, data_radar.shape)\\n\\nnum_timestamps = cumulative_counts[-1]\\nrelative_timestamp_feat = np.zeros((1, num_timestamps), dtype=int)\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[0.01395106, 0.01395106, 0.01395106, 0.01395106, 0.01395106,\n",
    "        0.01395106, 0.01395106, 0.01395106, 0.01395106, 0.01395106,\n",
    "        0.01395106, 0.01395106, 0.01395106, 0.01395106, 0.01395106,\n",
    "        0.01395106, 0.01395106, 0.01395106, 0.01395106, 0.01395106,\n",
    "        0.01395106, 0.01395106, 0.01395106, 0.01395106, 0.01395106,\n",
    "        0.01395106, 0.01395106, 0.01395106, 0.01395106, 0.01395106,\n",
    "        0.01395106, 0.01395106, 0.01395106, 0.08425093, 0.08425093,\n",
    "        0.08425093, 0.08425093, 0.08425093, 0.08425093, 0.08425093,\n",
    "        0.08425093, 0.08425093, 0.08425093, 0.08425093, 0.08425093,\n",
    "        0.08425093, 0.08425093, 0.08425093, 0.08425093, 0.08425093,\n",
    "        0.08425093, 0.08425093, 0.08425093, 0.08425093, 0.08425093,\n",
    "        0.08425093, 0.08425093, 0.08425093, 0.08425093, 0.08425093,\n",
    "        0.08425093, 0.08425093, 0.08425093, 0.08425093, 0.08425093,\n",
    "        0.08425093, 0.08425093, 0.08425093, 0.08425093, 0.08425093,\n",
    "        0.08425093, 0.15932608, 0.15932608, 0.15932608, 0.15932608,\n",
    "        0.15932608, 0.15932608, 0.15932608, 0.15932608, 0.15932608,\n",
    "        0.15932608, 0.15932608, 0.15932608, 0.15932608, 0.15932608,\n",
    "        0.15932608, 0.15932608, 0.15932608, 0.15932608, 0.15932608,\n",
    "        0.15932608, 0.15932608, 0.15932608, 0.15932608, 0.15932608,\n",
    "        0.15932608, 0.15932608, 0.15932608, 0.15932608, 0.15932608,\n",
    "        0.15932608, 0.15932608, 0.15932608, 0.15932608, 0.15932608,\n",
    "        0.15932608, 0.15932608, 0.15932608, 0.15932608, 0.15932608,\n",
    "        0.15932608, 0.15932608, 0.15932608, 0.2335999 , 0.2335999 ,\n",
    "        0.2335999 , 0.2335999 , 0.2335999 , 0.2335999 , 0.2335999 ,\n",
    "        0.2335999 , 0.2335999 , 0.2335999 , 0.2335999 , 0.2335999 ,\n",
    "        0.2335999 , 0.2335999 , 0.2335999 , 0.2335999 , 0.2335999 ,\n",
    "        0.2335999 , 0.2335999 , 0.2335999 , 0.2335999 , 0.2335999 ,\n",
    "        0.2335999 , 0.2335999 , 0.2335999 , 0.2335999 , 0.2335999 ,\n",
    "        0.2335999 , 0.2335999 , 0.2335999 , 0.2335999 , 0.2335999 ,\n",
    "        0.2335999 , 0.2335999 , 0.2335999 , 0.2335999 , 0.2335999 ,\n",
    "        0.2335999 , 0.2335999 , 0.2335999 , 0.2335999 , 0.2335999 ,\n",
    "        0.2335999 , 0.2335999 , 0.30367589, 0.30367589, 0.30367589,\n",
    "        0.30367589, 0.30367589, 0.30367589, 0.30367589, 0.30367589,\n",
    "        0.30367589, 0.30367589, 0.30367589, 0.30367589, 0.30367589,\n",
    "        0.30367589, 0.30367589, 0.30367589, 0.30367589, 0.30367589,\n",
    "        0.30367589, 0.30367589, 0.30367589, 0.30367589, 0.30367589,\n",
    "        0.30367589, 0.30367589, 0.30367589, 0.30367589, 0.30367589,\n",
    "        0.30367589, 0.30367589, 0.30367589, 0.30367589, 0.30367589,\n",
    "        0.30367589, 0.30367589, 0.30367589, 0.30367589, 0.30367589,\n",
    "        0.30367589, 0.30367589, 0.30367589, 0.30367589, 0.30367589,\n",
    "        0.30367589, 0.30367589, 0.37934089, 0.37934089, 0.37934089,\n",
    "        0.37934089, 0.37934089, 0.37934089, 0.37934089, 0.37934089,\n",
    "        0.37934089, 0.37934089, 0.37934089, 0.37934089, 0.37934089,\n",
    "        0.37934089, 0.37934089, 0.37934089, 0.37934089, 0.37934089,\n",
    "        0.37934089, 0.37934089, 0.37934089, 0.37934089, 0.37934089,\n",
    "        0.37934089, 0.37934089, 0.37934089, 0.37934089, 0.37934089,\n",
    "        0.37934089, 0.37934089, 0.37934089, 0.37934089, 0.37934089,\n",
    "        0.37934089, 0.37934089, 0.37934089, 0.37934089, 0.37934089,\n",
    "        0.37934089, 0.37934089, 0.37934089, 0.37934089, 0.37934089]])\n",
    "\n",
    "unique_values, counts = np.unique(a, return_counts=True)\n",
    "#print(unique_values, counts)\n",
    "# Calculate the cumulative sum of counts\n",
    "cumulative_counts = np.cumsum(counts)\n",
    "#print(cumulative_counts)\n",
    "# Create a new array based on cumulative counts\n",
    "result = np.zeros((1,cumulative_counts[-1]), dtype=int)\n",
    "# Assign values to the new array based on unique values and their counts\n",
    "# Assign values to the new array based on cumulative counts\n",
    "# Create an array of indices for each timestamp based on cumulative counts\n",
    "timestamp_indices = np.arange(cumulative_counts[-1])\n",
    "\n",
    "# Calculate which relative timestamp each point belongs to\n",
    "relative_timestamps = np.searchsorted(cumulative_counts, timestamp_indices, side='right')\n",
    "\n",
    "# Assign the relative timestamps to the corresponding locations in the array\n",
    "result[:, timestamp_indices] = relative_timestamps\n",
    "print(result)\n",
    "\"\"\"\n",
    "for i, count in enumerate(cumulative_counts):\n",
    "    if i > 0:\n",
    "        result[cumulative_counts[i - 1]:count] = i\n",
    "\n",
    "#print(result)\n",
    "new_unique_values, new_counts = np.unique(result, return_counts=True)\n",
    "#print(new_unique_values, new_counts)\n",
    "#print(cumulative_counts[-1])\n",
    "\n",
    "x = np.zeros((18,34), dtype=int)\n",
    "#print(x.shape)\n",
    "#print(x[:, 0:20])\n",
    "data_radar = np.empty((18,1))\n",
    "print(data_radar)\n",
    "data_radar = np.concatenate((data_radar, x), axis=1)\n",
    "print(data_radar, data_radar.shape)\n",
    "\n",
    "num_timestamps = cumulative_counts[-1]\n",
    "relative_timestamp_feat = np.zeros((1, num_timestamps), dtype=int)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  90616.,  259511.,  453208.,   32541.,   11503., 6486614.],\n",
      "       dtype=torch.float64)\n",
      "tensor([0.0124, 0.0354, 0.0618, 0.0044, 0.0016, 0.8845], dtype=torch.float64)\n",
      "tensor([ 80.9283,  28.2600,  16.1821, 225.3262, 637.1660,   1.1306],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def class_weight(data_root, train):\n",
    "    if train == True:\n",
    "        train_data_root = os.path.join(data_root, \"train_keyframes\")\n",
    "    else:\n",
    "        train_data_root = os.path.join(data_root, \"val_keyframes\")\n",
    "    all_frames = natsorted(os.listdir(train_data_root))\n",
    "    all_targets = []\n",
    "    for i in range(len(all_frames)):\n",
    "        current_frame_name = all_frames[i]\n",
    "        current_frame_file_path = os.path.join(train_data_root, current_frame_name)\n",
    "        target = np.load(current_frame_file_path, 'r')[:, -1]\n",
    "        all_targets.append(target)\n",
    "    results_targets = np.hstack(all_targets).astype(float)\n",
    "    index_counts = torch.histc(torch.tensor(results_targets), bins=6, min=0, max=5)\n",
    "\n",
    "    return index_counts\n",
    "\n",
    "\n",
    "data_root = './TrainingTooling_Data/nuScenes_dataset_in_AtCity_format/vjaiswalEnd2End/data/'\n",
    "\n",
    "class_counts = class_weight(data_root, train= False)\n",
    "print(class_counts)\n",
    "class_frequencies = class_counts / (torch.sum(class_counts))\n",
    "print(class_frequencies)\n",
    "inverse_class_frequencies = 1.0 / (class_frequencies + 1e-6)\n",
    "print(inverse_class_frequencies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      "[[0.67850841 0.24394496 0.04997865 0.91241903 0.93730592 0.62516909\n",
      "  0.34739785 0.35402426 0.39355866 0.04094832 0.52505856 0.41492457\n",
      "  0.28283608 0.08239358 0.94678283 0.5168608  0.28823134 0.76313039]\n",
      " [0.86711193 0.86019648 0.54025246 0.75559447 0.21071811 0.77898976\n",
      "  0.18341398 0.5413501  0.83238507 0.10919038 0.72458331 0.40293265\n",
      "  0.89346082 0.20008661 0.57476995 0.05664437 0.91791576 0.73537813]\n",
      " [0.25855508 0.22619698 0.6150341  0.48653924 0.21408806 0.83655874\n",
      "  0.21794372 0.75657985 0.30213868 0.46717864 0.77525019 0.87286572\n",
      "  0.53098937 0.03247781 0.5784415  0.03871108 0.1805793  0.28979083]\n",
      " [0.05189511 0.25578662 0.13916935 0.32660335 0.13995938 0.33764254\n",
      "  0.58428401 0.82524138 0.31066672 0.43496553 0.25857686 0.41138442\n",
      "  0.73953919 0.30650498 0.01759187 0.42241034 0.79766364 0.1091572 ]\n",
      " [0.50227621 0.26802609 0.14472621 0.9027622  0.80756931 0.59895201\n",
      "  0.0330912  0.69591854 0.70271183 0.16452625 0.87448878 0.07957277\n",
      "  0.51145584 0.31681    0.76208248 0.52030484 0.50979632 0.63382158]]\n",
      "\n",
      "Selected columns:\n",
      "[[0.67850841 0.24394496 0.04997865 0.35402426 0.5168608  0.28823134]\n",
      " [0.86711193 0.86019648 0.54025246 0.5413501  0.05664437 0.91791576]\n",
      " [0.25855508 0.22619698 0.6150341  0.75657985 0.03871108 0.1805793 ]\n",
      " [0.05189511 0.25578662 0.13916935 0.82524138 0.42241034 0.79766364]\n",
      " [0.50227621 0.26802609 0.14472621 0.69591854 0.52030484 0.50979632]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a sample array of shape N,18\n",
    "data = np.random.rand(5, 18)\n",
    "\n",
    "# Extract columns 0, 1, 2, 7, 15, and 16\n",
    "selected_columns = data[:, [0, 1, 2, 7, 15, 16]]\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(data)\n",
    "print(\"\\nSelected columns:\")\n",
    "print(selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Precision recall curve Script which Later I changed because the confusion matrix was calculated on 45 thresholds which was not need for semantic segmentation case\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "import json\n",
    "from natsort import natsorted\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add the root folder to the Python path\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "ROOT_DIR = BASE_DIR\n",
    "sys.path.insert(0, ROOT_DIR)\n",
    "\n",
    "def Precision_Recall(cm):\n",
    "    \"\"\"\n",
    "    This function returns class-wise TPR, FNR, FPR & TNR\n",
    "    [[cm]]: a 2-D array of a multiclass confusion matrix\n",
    "            where horizontal axes represent actual classes\n",
    "            and vertical axes represent predicted classes\n",
    "    {output}: a dictionary of class-wise accuracy parameters\n",
    "    \"\"\"\n",
    "    dict_metric = dict()\n",
    "    num_classes = len(cm[0])\n",
    "    row_sums = cm.sum(axis=1)          # gives total positives samples\n",
    "    col_sums = cm.sum(axis=0)          # gives total predicted samples\n",
    "    array_sum = sum(sum(cm))\n",
    "    #initialize a blank nested dictionary\n",
    "    for i in range(1, num_classes+1):\n",
    "        keys = str(i)\n",
    "        dict_metric[keys] = {\"Precision\":0, \"Recall\":0}\n",
    "    # calculate and store class-wise Precision and Recall\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            if i == j:\n",
    "                keys = str(i+1)\n",
    "                tp = cm[i, j]\n",
    "                \n",
    "                dict_metric[keys][\"Precision\"] = tp / col_sums[i]\n",
    "                dict_metric[keys][\"Recall\"] = tp / row_sums[i]\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    machine_config_json_path = os.path.join(\"configs\", \"machine_configs\", \"machine_config_local.json\")\n",
    "    with open(machine_config_json_path, \"r\") as local_config:\n",
    "        local_config_data = json.load(local_config)\n",
    "\n",
    "    logs_dir = local_config_data.get(\"model_path\")\n",
    "    logs_dir = os.path.join(ROOT_DIR, logs_dir)\n",
    "    logs_dir = Path(logs_dir)\n",
    "    conf_matrix_dir = logs_dir.joinpath('PointNet++_original_hyperparameters')\n",
    "    conf_matrix_dir = conf_matrix_dir.joinpath('baseline_with_focalloss_weight_on_whole_data')\n",
    "    conf_matrix_dir = conf_matrix_dir.joinpath('checkpoint_epoch95_h5_confusion_matrix')\n",
    "    conf_matrices = natsorted(os.listdir(conf_matrix_dir))\n",
    "    score_thresholds = [0.1, 0.12, 0.14, 0.16, 0.18, 0.2, 0.22, 0.24, 0.26, 0.28, 0.3, 0.32, 0.34, 0.36, 0.38, 0.4, 0.42, 0.44, 0.46, 0.48, 0.5, 0.52,\n",
    "                    0.54, 0.56, 0.58, 0.6, 0.62, 0.64, 0.66, 0.68, 0.7, 0.72, 0.74, 0.76, 0.78, 0.8, 0.82, 0.84, 0.86, 0.88, 0.9, 0.92, 0.94, 0.96, 0.98]\n",
    "    \n",
    "    sum_conf_matrices = np.zeros((45,6,6), dtype=np.int32)\n",
    "    for matrix in conf_matrices:\n",
    "        each_matrix_path = os.path.join(conf_matrix_dir, matrix)\n",
    "        with h5py.File(each_matrix_path, 'r') as matrix_file:\n",
    "            nocare_false = matrix_file['nocare_false']   \n",
    "            matrix_data = nocare_false['confus']  # order is true, pred, num_thresholds\n",
    "            #creating an empty confusion matrix of shape 6*6 initialized with zeros\n",
    "            matrix_data = np.array(matrix_data).transpose(2, 0, 1)           # order changed to num_thresholds, true, pred\n",
    "            sum_conf_matrices += matrix_data\n",
    "\n",
    "    class_colors = ['red', 'blue', 'orange', 'green', 'yellow', 'black']\n",
    "    line_styles = ['-', '--', '-.', ':', 'solid', 'dashed']\n",
    "    Classes = {'0': 'Background','1': 'Vehile Large','2': 'Vehicle','3': 'Pedestrian','4': 'Bike','5': 'Other'}\n",
    "\n",
    "    plt.figure()\n",
    "    for class_idx in range(6):\n",
    "        precision_list = []\n",
    "        recall_list = []\n",
    "        for i in range(len(score_thresholds)):\n",
    "            dict_metric = Precision_Recall(sum_conf_matrices[i, :, :])   \n",
    "            precision = dict_metric[str(class_idx + 1)]['Precision']\n",
    "            if math.isnan(precision):\n",
    "                continue\n",
    "            else:\n",
    "                precision_list.append(precision)\n",
    "                recall= dict_metric[str(class_idx + 1)]['Recall']\n",
    "                recall_list.append(recall)\n",
    "\n",
    "        plt.plot(precision_list, recall_list, label=f'Class {Classes[str(class_idx)]}', color=class_colors[class_idx], linestyle=line_styles[class_idx])\n",
    "\n",
    "    plt.title('Precision Recall curve')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "main()\n",
    "\n",
    "\"\"\"\n",
    "sample = np.array([[15, 2, 1],\n",
    "                   [3, 15, 1],\n",
    "                   [1, 4, 15]])\n",
    "dict_metric = Precision_Recall(sample)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Data Loader when it was padded with 0 for data and -1 for labels\"\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from build_dataset.dataset import Nuscenes_RadarPC_Dataset\n",
    "import os\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    # Pad sequences to make them of the same length\n",
    "    data_batch = [item[0] for item in batch]\n",
    "    labels_batch = [item[1] for item in batch]\n",
    "    frame_name = [item[2] for item in batch]\n",
    "    padding_label_value = -1\n",
    "\n",
    "    #max_sequence_length for padding\n",
    "    max_length = max(data.shape[0] for data in data_batch)\n",
    "\n",
    "    # Pad data sequences\n",
    "    data_padded = pad_sequence(data_batch, batch_first=True, padding_value=0)\n",
    "    # Pad labels with -1 to distinguish from actual labels (0 to 5)\n",
    "    labels_padded = [torch.cat([labels, torch.full((max_length - labels.shape[0],), padding_label_value, dtype=torch.long)]) for labels in labels_batch]\n",
    "    labels_padded = torch.stack(labels_padded)\n",
    "    # Create a mask to ignore padded labels during loss computation\n",
    "    mask = (labels_padded != padding_label_value)\n",
    "    return data_padded, labels_padded, mask, frame_name\n",
    "\n",
    "\n",
    "#  to wrap an iterable around Dataset enabling easy access to the samples\n",
    "def data_loader(data_root, batch_size, train, num_workers, pin_memory):\n",
    "    \n",
    "    if train == True:\n",
    "        train_data_path = os.path.join(data_root, \"train_keyframes\", \"\")\n",
    "        train_dataset = Nuscenes_RadarPC_Dataset(data_root=train_data_path)\n",
    "        dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory, collate_fn=custom_collate_fn, drop_last=True)\n",
    "\n",
    "    else:\n",
    "        val_data_path = os.path.join(data_root, \"val_keyframes\", \"\")\n",
    "        val_dataset = Nuscenes_RadarPC_Dataset(data_root=val_data_path)\n",
    "        dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    batch_size = 4\n",
    "    root = './TrainingTooling_Data/nuScenes_dataset_in_AtCity_format/vjaiswalEnd2End/data/'\n",
    "    dataloader = data_loader(data_root=root, batch_size=batch_size, train=True, num_workers=2, pin_memory=0)\n",
    "    tb = 0\n",
    "    for i, (data, labels, mask, frame_name, class_frequency) in enumerate(dataloader):\n",
    "        print(data.shape, labels.shape, mask.shape)\n",
    "        tb += 1\n",
    "    print(tb)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of .npy files with pickled data issue: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "data_root = './TrainingTooling_Data/nuScenes_dataset_in_AtCity_format/vjaiswalEnd2End/data/'\n",
    "folder_path = os.path.join(data_root, \"train_keyframes\")  # Replace with the actual folder path\n",
    "correct_npy_files = []\n",
    "\n",
    "# Iterate through the files in the folder\n",
    "for idx, file_name in enumerate(os.listdir(folder_path)):\n",
    "    if file_name.endswith(\".npy\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        try:\n",
    "            # Attempt to load the file to check for pickled data\n",
    "            data = np.load(file_path, allow_pickle=False)\n",
    "            correct_npy_files.append((file_name))\n",
    "        except ValueError as e:\n",
    "            if \"Cannot load file containing pickled data\" in str(e):\n",
    "                print(f\"Skipping file {file_name} due to pickled data issue\")\n",
    "                continue  # Skip to the next iteration\n",
    "                #npy_files_with_pickled_issue.append((idx, file_name))\n",
    "\n",
    "print(\"Number of .npy files with pickled data issue:\", len(npy_files_with_pickled_issue))\n",
    "\n",
    "# Print index and file name for each problematic file\n",
    "for idx, file_name in npy_files_with_pickled_issue:\n",
    "    print(f\"Index: {idx}, File: {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.13746317 0.14271352 0.13599609 0.07946634 0.13314667 0.37121427]\n",
      "  [0.2435165  0.09190841 0.16223449 0.12078015 0.21179003 0.16977055]\n",
      "  [0.22234337 0.1058269  0.15778023 0.11311816 0.18265638 0.21827495]\n",
      "  [0.18879935 0.13617933 0.13309108 0.06782927 0.12021647 0.35388452]\n",
      "  [0.20239516 0.14665665 0.1284584  0.05656371 0.09103625 0.37488988]\n",
      "  [0.18011338 0.14223598 0.12402354 0.05978188 0.10116    0.39268517]\n",
      "  [0.14907166 0.14342202 0.12553288 0.07436088 0.11247776 0.39513475]\n",
      "  [0.17608753 0.15841405 0.133594   0.06870616 0.14945178 0.31374648]\n",
      "  [0.1417522  0.14272805 0.14013737 0.07374226 0.14360261 0.35803756]\n",
      "  [0.13132027 0.2606394  0.09261864 0.04673263 0.06791212 0.40077698]\n",
      "  [0.13034964 0.14532945 0.13473438 0.08330745 0.13030913 0.37597   ]\n",
      "  [0.13919015 0.14361583 0.14055565 0.07575537 0.1442205  0.3566625 ]\n",
      "  [0.18046436 0.13934052 0.14985721 0.04464339 0.11779872 0.3678958 ]\n",
      "  [0.13746317 0.14271352 0.13599609 0.07946634 0.13314667 0.37121427]\n",
      "  [0.24436854 0.09107042 0.16206014 0.12176648 0.21122617 0.1695082 ]\n",
      "  [0.266379   0.07628171 0.15929373 0.14470501 0.24888577 0.10445473]\n",
      "  [0.16308174 0.18674903 0.11468435 0.03942204 0.07211532 0.42394757]\n",
      "  [0.13519628 0.27630058 0.10565405 0.05426993 0.10308346 0.32549566]\n",
      "  [0.01671539 0.19709581 0.07313889 0.4633427  0.06326436 0.18644294]\n",
      "  [0.14436635 0.13896371 0.1326824  0.07705623 0.12525228 0.381679  ]\n",
      "  [0.20540902 0.12261534 0.14415099 0.10106222 0.14799243 0.27876997]\n",
      "  [0.19865634 0.12941779 0.14017713 0.08065001 0.13505322 0.31604546]\n",
      "  [0.16764233 0.13662721 0.14902882 0.05062354 0.13190222 0.36417583]\n",
      "  [0.20761386 0.13450077 0.13169827 0.05387968 0.09852964 0.37377778]\n",
      "  [0.21554135 0.14360777 0.13226064 0.05674052 0.08782195 0.36402777]\n",
      "  [0.1524195  0.1302301  0.12669785 0.16495447 0.15487798 0.27082008]\n",
      "  [0.14858158 0.15830784 0.13056788 0.07223391 0.13892916 0.3513797 ]\n",
      "  [0.18043987 0.1394645  0.1486701  0.04452996 0.11612675 0.3707688 ]\n",
      "  [0.16902548 0.17626624 0.12765911 0.09945111 0.1579396  0.26965848]\n",
      "  [0.1729497  0.18711373 0.14325574 0.04635078 0.14772432 0.3026058 ]\n",
      "  [0.16278231 0.17587683 0.17232227 0.0334758  0.07258487 0.3829579 ]\n",
      "  [0.17020974 0.13130341 0.1377007  0.08149286 0.13553107 0.34376222]\n",
      "  [0.12674238 0.146368   0.12503624 0.09634193 0.11638221 0.38912928]\n",
      "  [0.17347234 0.1422678  0.1815586  0.04378937 0.09157994 0.367332  ]\n",
      "  [0.18978551 0.13321051 0.16612196 0.04173864 0.09314257 0.3760008 ]\n",
      "  [0.14592989 0.1381839  0.13272278 0.07574116 0.12480135 0.38262093]\n",
      "  [0.13814381 0.21875973 0.1654533  0.02753488 0.05436079 0.39574745]\n",
      "  [0.10112759 0.27745453 0.07892416 0.04329417 0.04899687 0.45020276]\n",
      "  [0.12743343 0.25304624 0.09270543 0.04287443 0.05781196 0.42612854]\n",
      "  [0.1575598  0.1990754  0.11266767 0.03636083 0.06286371 0.43147257]\n",
      "  [0.16716838 0.15727495 0.17935388 0.03923548 0.08323862 0.3737287 ]\n",
      "  [0.07339512 0.1602799  0.10944647 0.18178704 0.10555994 0.3695315 ]\n",
      "  [0.06138061 0.17144887 0.09452767 0.2244745  0.08787453 0.36029392]\n",
      "  [0.19520643 0.13572028 0.15386823 0.0396296  0.10088601 0.3746894 ]\n",
      "  [0.16975664 0.16474628 0.14767593 0.04104845 0.09759697 0.37917566]\n",
      "  [0.06285    0.16761544 0.11196078 0.18933402 0.11029152 0.35794824]\n",
      "  [0.1458455  0.1944524  0.12328022 0.04965422 0.08731116 0.3994565 ]\n",
      "  [0.1742488  0.14231913 0.17436548 0.04406203 0.09177236 0.37323216]\n",
      "  [0.07600927 0.14092195 0.10568026 0.2551913  0.11308045 0.30911678]\n",
      "  [0.15537237 0.13509561 0.15047526 0.05861422 0.14669925 0.35374326]\n",
      "  [0.14973308 0.13827322 0.1450777  0.06525446 0.14837287 0.35328865]\n",
      "  [0.19399714 0.13017878 0.16723593 0.0410398  0.0958994  0.37164894]\n",
      "  [0.19951022 0.12942606 0.16883458 0.03915316 0.09627998 0.36679602]\n",
      "  [0.18845585 0.12987156 0.17416699 0.04275656 0.09633506 0.36841398]\n",
      "  [0.15045503 0.20301414 0.1633876  0.02889743 0.05939496 0.39485085]\n",
      "  [0.09559803 0.28320184 0.15875392 0.02856633 0.04615681 0.38772306]\n",
      "  [0.12301794 0.24389835 0.11272894 0.03379765 0.05125125 0.43530592]\n",
      "  [0.09867104 0.2969579  0.10035273 0.03865274 0.04968264 0.41568294]\n",
      "  [0.16087545 0.18811932 0.15897612 0.0307513  0.06428192 0.39699593]\n",
      "  [0.20279811 0.12918791 0.1684698  0.03851175 0.09627208 0.36476037]\n",
      "  [0.12900575 0.14547862 0.12991671 0.08609824 0.12112246 0.38837823]\n",
      "  [0.11538676 0.15637754 0.12576672 0.09768435 0.11958846 0.38519612]\n",
      "  [0.16876107 0.13170351 0.13673276 0.08320179 0.1340855  0.34551534]\n",
      "  [0.14825188 0.13757552 0.13102067 0.07946534 0.12305804 0.38062856]\n",
      "  [0.04281828 0.13391009 0.0953539  0.36137643 0.10572297 0.26081836]\n",
      "  [0.20861977 0.11395327 0.1521669  0.1124476  0.17139831 0.24141419]\n",
      "  [0.14176784 0.14107159 0.1411811  0.07388855 0.14407976 0.35801122]\n",
      "  [0.17821409 0.13582116 0.17081091 0.04588668 0.09875997 0.3705072 ]\n",
      "  [0.16104415 0.18313344 0.16990101 0.04696484 0.16192752 0.27702913]\n",
      "  [0.17068078 0.13177322 0.17439076 0.0551199  0.18471669 0.28331867]\n",
      "  [0.18727407 0.14006948 0.14183512 0.04365694 0.10923403 0.3779303 ]\n",
      "  [0.18101762 0.13936937 0.14972985 0.04439276 0.11692756 0.3685628 ]\n",
      "  [0.16354772 0.18335429 0.11856856 0.04008218 0.07634933 0.41809788]\n",
      "  [0.16268958 0.14875188 0.20132247 0.04549259 0.09897699 0.34276655]\n",
      "  [0.10571837 0.2751599  0.066463   0.04922606 0.05278685 0.45064586]\n",
      "  [0.08350746 0.31459987 0.11689743 0.03201175 0.04371478 0.4092687 ]\n",
      "  [0.14233045 0.21960688 0.09771497 0.05011858 0.0700816  0.42014754]\n",
      "  [0.15102021 0.20110026 0.15978558 0.03014235 0.06136699 0.3965846 ]\n",
      "  [0.14306402 0.21468282 0.12103652 0.03350319 0.05905166 0.4286617 ]\n",
      "  [0.18726316 0.12963969 0.17427945 0.04352701 0.09664311 0.36864755]\n",
      "  [0.17118725 0.16264951 0.13172755 0.04521795 0.11995796 0.3692598 ]\n",
      "  [0.15528207 0.21942711 0.14874709 0.0427676  0.1194551  0.314321  ]\n",
      "  [0.19632392 0.15025048 0.12545548 0.03575301 0.07426505 0.41795203]\n",
      "  [0.05867011 0.3470133  0.15481132 0.02477586 0.03938326 0.37534606]\n",
      "  [0.15892196 0.18645127 0.1934348  0.03206474 0.08141739 0.3477098 ]\n",
      "  [0.18816414 0.14839548 0.13296491 0.04004943 0.08998447 0.4004416 ]\n",
      "  [0.16891858 0.16148917 0.166104   0.03712664 0.07952023 0.38684133]\n",
      "  [0.1679818  0.18278208 0.10652193 0.04321235 0.07515761 0.4243442 ]\n",
      "  [0.1556231  0.21245384 0.09106537 0.05249839 0.07060918 0.41775012]\n",
      "  [0.13026269 0.23255682 0.17892167 0.02685597 0.05465793 0.37674496]\n",
      "  [0.15967716 0.1945382  0.10044832 0.04299418 0.06650682 0.43583533]\n",
      "  [0.17148258 0.17872357 0.11475409 0.0401989  0.08022804 0.41461286]\n",
      "  [0.19882879 0.13518527 0.15578997 0.0391608  0.10113342 0.36990178]\n",
      "  [0.14623515 0.20835797 0.16274607 0.02879846 0.05824997 0.3956124 ]\n",
      "  [0.15282199 0.13662834 0.1480343  0.06146844 0.14853984 0.35250708]\n",
      "  [0.19140534 0.13604018 0.1530953  0.04124876 0.10642996 0.37178046]\n",
      "  [0.23893046 0.09717254 0.15740493 0.1283544  0.19710857 0.18102902]\n",
      "  [0.15259466 0.13669218 0.12684408 0.12354705 0.1331656  0.32715645]\n",
      "  [0.16407932 0.19151795 0.11104143 0.04910852 0.09865338 0.38559932]\n",
      "  [0.13113263 0.23866859 0.13620332 0.03944293 0.07195452 0.38259804]\n",
      "  [0.09379591 0.2869995  0.15808374 0.02886235 0.04629415 0.38596436]\n",
      "  [0.17516106 0.13865398 0.17465326 0.04617451 0.10051774 0.36483943]\n",
      "  [0.18633774 0.13772047 0.14960843 0.04259878 0.10982112 0.3739135 ]\n",
      "  [0.23684683 0.09765604 0.16013446 0.12277684 0.19572707 0.18685879]\n",
      "  [0.17172278 0.12948927 0.18036048 0.05469795 0.18617555 0.27755395]\n",
      "  [0.16872773 0.15737054 0.1777546  0.03863423 0.08362982 0.37388307]\n",
      "  [0.15283543 0.13198107 0.15206969 0.06345841 0.15146258 0.34819284]\n",
      "  [0.17164901 0.13885647 0.14605756 0.04833619 0.12461607 0.3704847 ]\n",
      "  [0.14883506 0.15265626 0.1318455  0.0726224  0.13663946 0.35740125]\n",
      "  [0.16408521 0.19987115 0.11191981 0.04027365 0.08488008 0.3989701 ]\n",
      "  [0.16102242 0.15197894 0.20120409 0.0449656  0.09958215 0.3412468 ]\n",
      "  [0.16469252 0.18564618 0.11327788 0.05409123 0.11018413 0.37210807]\n",
      "  [0.17226353 0.16737925 0.13858026 0.04532545 0.13134784 0.34510374]\n",
      "  [0.00325925 0.44329226 0.34822494 0.00388937 0.0387412  0.16259299]\n",
      "  [0.16223936 0.16739057 0.15548225 0.04116236 0.08633108 0.38739443]\n",
      "  [0.19137615 0.14701349 0.13253833 0.0393886  0.08627056 0.40341297]\n",
      "  [0.16443552 0.16198988 0.13027391 0.05552416 0.14057203 0.3472045 ]\n",
      "  [0.06175397 0.33428285 0.07894454 0.03756548 0.03648189 0.45097128]\n",
      "  [0.06631847 0.3331568  0.09082472 0.03448276 0.03844619 0.43677118]\n",
      "  [0.15230477 0.15625013 0.13540718 0.06298128 0.15024377 0.3428129 ]\n",
      "  [0.15275526 0.22057529 0.15461001 0.04477892 0.13169302 0.29558748]\n",
      "  [0.14081769 0.21403012 0.09551837 0.10290442 0.09436913 0.3523603 ]\n",
      "  [0.14982475 0.14248604 0.13459072 0.07182705 0.13390134 0.36737013]\n",
      "  [0.10309049 0.27044046 0.1595508  0.0284999  0.04742998 0.39098835]\n",
      "  [0.20360921 0.13061868 0.16603827 0.03803558 0.09640902 0.36528924]\n",
      "  [0.14790757 0.23881647 0.1008469  0.05287985 0.10095149 0.35859773]\n",
      "  [0.18178704 0.16342458 0.14622082 0.04006421 0.10687023 0.36163318]\n",
      "  [0.167195   0.18945163 0.1170262  0.04330122 0.10024779 0.3827781 ]\n",
      "  [0.17194399 0.17850448 0.10929284 0.04206811 0.07861276 0.41957793]\n",
      "  [0.21081339 0.12394558 0.18718658 0.03878148 0.10460508 0.33466786]\n",
      "  [0.13192232 0.25930053 0.09154346 0.0553137  0.06901409 0.39290592]\n",
      "  [0.17714909 0.15829599 0.12593913 0.044148   0.1039332  0.39053467]\n",
      "  [0.12566872 0.23653084 0.17328046 0.02810926 0.05539585 0.38101488]\n",
      "  [0.15988009 0.18064342 0.16432598 0.03328617 0.06977008 0.39209422]\n",
      "  [0.13951114 0.22784661 0.11386368 0.04195008 0.06672045 0.41010806]\n",
      "  [0.14845599 0.19901489 0.12873243 0.04314978 0.08755473 0.39309216]\n",
      "  [0.13168891 0.23996894 0.09697432 0.0471386  0.06136049 0.4228688 ]\n",
      "  [0.17092603 0.13792965 0.14765714 0.04889728 0.1279195  0.3666705 ]\n",
      "  [0.15847369 0.18984562 0.13079678 0.0360429  0.07248464 0.41235638]\n",
      "  [0.17173864 0.16636145 0.12727779 0.04094683 0.08495373 0.40872154]\n",
      "  [0.17935327 0.16298729 0.13005927 0.04258452 0.10949398 0.3755217 ]\n",
      "  [0.15678033 0.19454356 0.1179307  0.04162148 0.08635449 0.40276945]\n",
      "  [0.16619363 0.17611422 0.12912436 0.05072979 0.13814329 0.33969474]\n",
      "  [0.16572048 0.17257772 0.12013847 0.04491386 0.09231005 0.40433943]\n",
      "  [0.18034454 0.16426636 0.13677345 0.04190893 0.11121363 0.3654931 ]\n",
      "  [0.16748665 0.13410325 0.17382094 0.05730927 0.18793702 0.27934283]\n",
      "  [0.00327789 0.44791353 0.34444055 0.00406032 0.03784307 0.16246471]\n",
      "  [0.1679945  0.15879388 0.17304471 0.03902904 0.08550876 0.37562913]\n",
      "  [0.19845873 0.13504    0.15612969 0.03921283 0.10128625 0.3698725 ]\n",
      "  [0.15488866 0.18706636 0.20040227 0.0334154  0.08596154 0.33826578]\n",
      "  [0.20971557 0.1267469  0.18347761 0.03842317 0.10543579 0.33620095]\n",
      "  [0.17613542 0.1525921  0.13208513 0.04470265 0.10753205 0.3869526 ]\n",
      "  [0.20240073 0.13014828 0.1663054  0.03845674 0.09610938 0.36657953]\n",
      "  [0.16853458 0.15363407 0.17787442 0.04066856 0.0859975  0.3732909 ]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "data_root = './logs/PointNet++_original_hyperparameters/6_PaperArchitecture_Pad0-1_16_NumWo6_50_1e-3_20_0.5/checkpoint_epoch25_npy_predictions/'\n",
    "frame = 'scene-0003_frame1.npy'\n",
    "frame_path = os.path.join(data_root, frame)\n",
    "data = np.load(frame_path, 'r')\n",
    "print(data)\n",
    "#print(data[:, 6])\n",
    "#print(data.shape)\n",
    "#print(data[:2, :])\n",
    "#print(data.dtype)\n",
    "#frame_radar_point_data = torch.tensor(data[:2, [0, 1, 2, 7, 15, 16]])\n",
    "#frame_radar_point_label = torch.tensor(data[:2, -1])\n",
    "#print(frame_radar_point_data, frame_radar_point_label)\n",
    "#print(frame_radar_point_label.dtype)\n",
    "#data = data.astype(int)\n",
    "#print(np.argmax(data_reshape, axis=1))\n",
    "#index_counts = np.bincount(data[:, -1])\n",
    "#print(index_counts)\n",
    "#for index, count in enumerate(index_counts):\n",
    "    #print(f\"Index {index} occurs {count} times\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyThesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
