{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"###################################################  data_provider.py  ##############################################################\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import collections\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "import cv2\n",
    "import copy\n",
    "import h5py\n",
    "from types import TracebackType\n",
    "import os\n",
    "import sys\n",
    "from typing import Any, Optional, List, Dict, Type\n",
    "\n",
    "\n",
    "from .label import Label\n",
    "from ..sensors.lidar import Lidar\n",
    "from ..sensors.radar import Radar\n",
    "from ..sensors.camera import Camera\n",
    "from ..calibrations.lidar_calib import LidarCalib\n",
    "from ..calibrations.radar_calib import RadarCalib\n",
    "from ..calibrations.camera_calib import CameraCalib\n",
    "from .cbor_handler import CBOR_Handler\n",
    "from end2end_utilities.class_mapper.class_mapper import ClassMapper\n",
    "\n",
    "\n",
    "class DataProviderDataSpec:\n",
    "    \"\"\"\n",
    "    structure for DataProviderGenerator input\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, folder: str, type: str, sequence_list: List[str], label_source: str, mapping_file: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        inits DataProviderGenerator\n",
    "\n",
    "        Args:\n",
    "            folder:         data root folder\n",
    "            type:           data format out of ['trainingTool2', 'cbor']\n",
    "            sequence_list:  name of logs to load\n",
    "            label_source:   source of the labeling (e.g. 'XC90 Pandora', 'Understand AI', 'Autolabeling-EndReview-FixedLeGO-LOAM')\n",
    "                            or None if no labels shall be loaded at all\n",
    "            mapping_file:   path of class mapping file\n",
    "        \"\"\"\n",
    "        if type not in [\"trainingTool2\", \"cbor\"]:\n",
    "            raise ValueError(\"Data type %s not supported, in DataProviderDataSpec.__init__\" % type)\n",
    "\n",
    "        # store input\n",
    "        self.folder = folder\n",
    "        self.type = type\n",
    "        self.sequence_list = sequence_list\n",
    "        self.label_source = label_source\n",
    "        self.mapping_file = mapping_file\n",
    "\n",
    "        # expand sequence list if all sequences are required\n",
    "        if not self.sequence_list:\n",
    "            if self.type == \"trainingTool2\":\n",
    "                seq_in_folder_dict = {os.path.splitext(os.path.basename(ff))[0]: ff for ff in glob.glob(os.path.join(self.folder, \"Meta\", \"*.h5\"))}\n",
    "                self.sequence_list = seq_in_folder_dict.keys()\n",
    "            elif self.type == \"cbor\":\n",
    "                seq_in_folder_dict = {os.path.basename(ff).split(\".\")[0].split(\"_\")[1]: ff for ff in glob.glob(os.path.join(self.folder, \"config_*.yml\"))}\n",
    "                self.sequence_list = seq_in_folder_dict.keys()\n",
    "\n",
    "        # get data versions\n",
    "        if self.type == \"cbor\":\n",
    "            # NOTE: no versioning for cbor data available yet\n",
    "            self.data_version = [None for log in self.sequence_list]\n",
    "        elif self.type == \"trainingTool2\":\n",
    "            self.data_version = []\n",
    "            for log in self.sequence_list:\n",
    "                meta_file = h5py.File(os.path.join(self.folder, \"Meta\", log + \".h5\"), \"r\")\n",
    "                self.data_version.append(meta_file[\"info\"].attrs[\"version\"])\n",
    "                meta_file.close()\n",
    "\n",
    "    def get_log_specs(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        get evaluated log list defined by data_spec\n",
    "\n",
    "        Returns:\n",
    "            List of dicts holding {log_type: <sequence data type (e.g.: 'cbor')>,\n",
    "                                   log_folder: <data folder>,\n",
    "                                   log_name: <list of sequence names>,\n",
    "                                   data_version: <data version>\n",
    "                                   label_source: <GT label source>,\n",
    "                                   mapping_file: <mapping file>\n",
    "        \"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"log_type\": self.type,\n",
    "                \"log_folder\": self.folder,\n",
    "                \"log_name\": log,\n",
    "                \"data_version\": version,\n",
    "                \"label_source\": self.label_source,\n",
    "                \"mapping_file\": self.mapping_file,\n",
    "            }\n",
    "            for log, version in zip(self.sequence_list, self.data_version)\n",
    "        ]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "structure for file meta data:\n",
    "    sensor_type:        type of sensor\n",
    "    sensor_id:          id of sensor\n",
    "    sensor_name:        name of sensor\n",
    "    source_type:        type of source (e.g. trainingTool2, etc.)\n",
    "    folder:             superfolder containing data\n",
    "    sequence_name:      subfolder containing sequence the file belongs to\n",
    "    sequence_number:    order position of sequence in complete loaded sequence list\n",
    "    aligned_sensor_names:  name of aligned sensors per [sensor][sensor id]\n",
    "    aligned_fids:       frame id within file per [sensor][sensor id] (can alternatively be one number for all)\n",
    "    aligned_timestamps: timestamp per [sensor][sensor id] and [label] (can alternatively be one number for all)\n",
    "    aligned_label_ids:  label file idx per [sensor][sensor id] (can alternatively be one number for all)\n",
    "    aligned_sids:       aligned scan ids from sensor hardware\n",
    "    aligned_subsensors: subsensor information for artificial fused sensors\n",
    "    framenumber:        order position of file within sequence\n",
    "    data_handlers:      data handling structure if needed for data type\n",
    "    flags:              dict to be used to pass additional information in inherited DataProviders\n",
    "\"\"\"\n",
    "DataProviderMetaData = collections.namedtuple(\n",
    "    \"DataProviderMetaData\",\n",
    "    \"sensor_type \"\n",
    "    \"sensor_id \"\n",
    "    \"sensor_name \"\n",
    "    \"source_type \"\n",
    "    \"folder \"\n",
    "    \"sequence_name \"\n",
    "    \"sequence_number \"\n",
    "    \"aligned_sensor_names \"\n",
    "    \"aligned_fids \"\n",
    "    \"aligned_timestamps \"\n",
    "    \"aligned_sids \"\n",
    "    \"aligned_subsensors \"\n",
    "    \"framenumber \"\n",
    "    \"data_handlers \"\n",
    "    \"flags\",\n",
    ")\n",
    "\n",
    "\n",
    "class DataManipulator:\n",
    "    \"\"\"\n",
    "    Basic data manipulator object\n",
    "    \"\"\"\n",
    "\n",
    "    # data_package and data_config need always to be first arguments after self\n",
    "    def __init__(self, data_package: \"DataProvider\", data_config: dict):\n",
    "        \"\"\"\n",
    "        dummy for data manipulation function which can be added in derived classes, to change the inserted DataProvider\n",
    "        Note: this function will not be called again when DataProvider is taken from scratch\n",
    "\n",
    "        Args:\n",
    "            data_package:   DataProvider to be manipulated\n",
    "            data_config:    Additional information from DataProvider generation that might be used at manipulation step\n",
    "        \"\"\"\n",
    "        pass\n",
    "        # print('Manipulating %s' % data_package.tag)\n",
    "\n",
    "    def update_cache_independent(self, data_package: \"DataProvider\", data_config: dict, **args: Any):\n",
    "        \"\"\"\n",
    "        dummy for data manipulations that shall be performed always, even if the DataProvider was taken from cache\n",
    "\n",
    "        Args:\n",
    "            data_package:   DataProvider to be manipulated\n",
    "            data_config:    Additional information from DataProvider generation that might be used at manipulation step\n",
    "            **args:         dummy for additional arguments, replace by the arguments you use in __init__ function (IMPORTANT)\n",
    "        \"\"\"\n",
    "        pass\n",
    "        # print('Updating %s' % data_package.tag)\n",
    "\n",
    "    def draw_2d_overlay(self, data_package: \"DataProvider\", cam: int, canvas: \"vtk.vtkImageCanvasSource2D\"):\n",
    "        \"\"\"\n",
    "        dummy for plot functions which can be added in derived classes, to plot class specific overlays on camera images\n",
    "\n",
    "        Args:\n",
    "            data_package:   DataProvider holding data to plot\n",
    "            cam:            holds the camera list id\n",
    "            canvas:         holds the viewer 2d vtk rendering struct for this camera (see vtk.vtkImageCanvasSource2D)\n",
    "        \"\"\"\n",
    "        pass\n",
    "        # do not plot anything with this function in base class, use a derived class with overloaded function\n",
    "        # e.g. plot a red box on image canvas (vtk.vtkImageCanvasSource2D):\n",
    "        # canvas.SetDrawColor(255.0, 0.0, 0.0, 255.0)\n",
    "        # canvas.FillBox(10, 100, 10, 100)\n",
    "\n",
    "    def add_2d_actors(self, data_package: \"DataProvider\", cam: int, actor_data_list: list, xc: float, yc: float):\n",
    "        \"\"\"\n",
    "        dummy for plot functions which can be added in derived classes, to add class specific actors in 2d view,\n",
    "        image plane is located between X(right) and Y(down) axes, with top left edge in origin\n",
    "        (i.e. center at x=width/2, y=height/2, z=0), camera recording this plane is located at x=width/2, y=height/2, z=-height/2,\n",
    "        thus, horizontal aperture angle=90deg\n",
    "\n",
    "        Args:\n",
    "            data_package:       DataProvider holding data to plot\n",
    "            cam:                holds the camera list id\n",
    "            actor_data_list:    list to add your plot data to, as tuple (vtk object actors, list with data to be kept in memory as long as data is plotted)\n",
    "            xc:                 hold image width/2 as center of image plane position\n",
    "            yc:                 hold image height/2 as center of image plane position\n",
    "        \"\"\"\n",
    "\n",
    "        pass\n",
    "        # do not plot anything with this function in base class, use a derived class with overloaded function\n",
    "        # e.g. plot a red cube at origin, so that half of it will be in front of image plane while half will be occluded:\n",
    "        # import vtk\n",
    "        # # create cube structure\n",
    "        # cube = vtk.vtkCubeSource()\n",
    "        # cube.SetCenter(xc, yc, 0)\n",
    "        # cube.SetXLength(200)\n",
    "        # cube.SetYLength(200)\n",
    "        # cube.SetZLength(200)\n",
    "        # # create actor\n",
    "        # cubeActor = vtk.vtkActor()\n",
    "        # cubeActor.GetProperty().SetRepresentationToWireframe()\n",
    "        # cubeActor.GetProperty().SetColor(1, 0, 0)\n",
    "        # cubeActor.GetProperty().LightingOff()\n",
    "        # # create and connect mapper\n",
    "        # cubeMapper = vtk.vtkPolyDataMapper()\n",
    "        # cubeMapper.SetInputConnection(cube.GetOutputPort())\n",
    "        # cubeActor.SetMapper(cubeMapper)\n",
    "        # # add actor to list (no data to be kept in memory explicitely here, as cube is kept by cubeMapper and cubeMapper is kept by cubeActor)\n",
    "        # actor_data_list.append((cubeActor, []))\n",
    "\n",
    "    def add_3d_actors(self, data_package: \"DataProvider\", actor_data_list: list):\n",
    "        \"\"\"\n",
    "        dummy for plot functions which can be added in derived classes, to add class specific actors in 3d view\n",
    "\n",
    "        Args:\n",
    "            data_package:       DataProvider holding data to plot\n",
    "            actor_data_list:    list to add your plot data to, as tuple (vtk object actors, list with data to be kept in memory as long as data is plotted)\n",
    "        \"\"\"\n",
    "        pass\n",
    "        # do not plot anything with this function in base class, use a derived class with overloaded function\n",
    "        # e.g. plot a red cube at origin:\n",
    "        # import vtk\n",
    "        # # create cube structure\n",
    "        # cube = vtk.vtkCubeSource()\n",
    "        # cube.SetCenter(0, 0, 0)\n",
    "        # cube.SetXLength(4)\n",
    "        # cube.SetYLength(2)\n",
    "        # cube.SetZLength(1.5)\n",
    "        # # create actor\n",
    "        # cubeActor = vtk.vtkActor()\n",
    "        # cubeActor.GetProperty().SetRepresentationToWireframe()\n",
    "        # cubeActor.GetProperty().SetColor(1, 0, 0)\n",
    "        # cubeActor.GetProperty().LightingOff()\n",
    "        # # create and connect mapper\n",
    "        # cubeMapper = vtk.vtkPolyDataMapper()\n",
    "        # cubeMapper.SetInputConnection(cube.GetOutputPort())\n",
    "        # cubeActor.SetMapper(cubeMapper)\n",
    "        # # add actor to list (no data to be kept in memory explicitely here, as cube is kept by cubeMapper and cubeMapper is kept by cubeActor)\n",
    "        # actor_data_list.append((cubeActor, []))\n",
    "\n",
    "\n",
    "class DataProvider(object):\n",
    "    \"\"\"\n",
    "    Data provider object\n",
    "    \"\"\"\n",
    "\n",
    "    # static variables\n",
    "    callback_dmy_cnt = 0\n",
    "\n",
    "    # define available named sensortypes\n",
    "    named_sensor_types = [\"LIDAR\", \"RADAR\", \"FUSEDRADAR\", \"CAMERA\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dp_meta_data: Optional[DataProviderMetaData] = None,\n",
    "        tag: str = \"\",\n",
    "        data2load: dict = {\"LIDAR\": [], \"RADAR\": [], \"FUSEDRADAR\": [], \"CAMERA\": [], \"LABEL\": []},\n",
    "        label_converter: Optional[ClassMapper] = None,\n",
    "        data_cache: Optional[dict] = None,\n",
    "        version2load: dict = {},\n",
    "        data_manipulators: list = [],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        inits data provider object\n",
    "\n",
    "        Args:\n",
    "            dp_meta_data:       meta data structure\n",
    "            tag:                optional tag for data package\n",
    "            data2load:          which data schall be loaded\n",
    "            label_converter:    class mapper\n",
    "            data_cache:         cached data\n",
    "            version2load:       optional specifications of data versions to load\n",
    "            data_manipulators:  data manipulators assigned to data package\n",
    "        \"\"\"\n",
    "\n",
    "        # data structures\n",
    "        self.META = dict()\n",
    "        self.LIDARS = dict()\n",
    "        self.RADARS = dict()\n",
    "        self.FUSEDRADARS = dict()\n",
    "        self.CAMERAS = dict()\n",
    "        self.LABEL = dict()\n",
    "\n",
    "        # store source type\n",
    "        if dp_meta_data is None:\n",
    "            self.source_type = None\n",
    "        else:\n",
    "            self.source_type = dp_meta_data.source_type\n",
    "\n",
    "        # tag as identifier code for data sample, create one if not provided\n",
    "        self.tag = tag\n",
    "        if self.tag == \"\":\n",
    "            self.tag = \"\".join(random.choice(string.ascii_uppercase + string.digits) for _ in range(8))\n",
    "\n",
    "        # read data if provided\n",
    "        if dp_meta_data is not None:\n",
    "            self.read_data(dp_meta_data, data2load, version2load, data_cache)\n",
    "\n",
    "        # convert label if entered in converter\n",
    "        if label_converter is not None:\n",
    "            for layer in self.LABEL:\n",
    "                for lab in self.LABEL[layer]:\n",
    "                    if lab.breed == \"annotation\":\n",
    "                        speed = np.linalg.norm(lab.vel_3d_filtered)\n",
    "                    else:\n",
    "                        speed = np.linalg.norm(lab.vel_3d)\n",
    "                    mapped_type, _, _ = label_converter.map(class_in=lab.type, speed=speed)\n",
    "                    if mapped_type is not None:\n",
    "                        lab.type = mapped_type\n",
    "\n",
    "        # add data handlers\n",
    "        if dp_meta_data is not None:\n",
    "            data_handlers = dp_meta_data.data_handlers\n",
    "        else:\n",
    "            data_handlers = None\n",
    "\n",
    "        # apply data manipulators\n",
    "        self.data_manipulators = []\n",
    "        if len(data_manipulators) > 0:\n",
    "            self.apply_data_manipulators(data_cache, data_manipulators, data2load, version2load, label_converter, data_handlers)\n",
    "\n",
    "    def get_sensor_data(self, sensor_type: str, sensor_id: int) -> dict:\n",
    "        \"\"\"\n",
    "        getter for data of specific sensor\n",
    "\n",
    "        Args:\n",
    "            sensor_type:    query sensor type\n",
    "            sensor_id:      query sensor id\n",
    "\n",
    "        Returns:\n",
    "            query result dict\n",
    "        \"\"\"\n",
    "\n",
    "        if sensor_type in self.named_sensor_types:\n",
    "            out = None\n",
    "            if sensor_type == \"LIDAR\":\n",
    "                out = self.LIDARS.get(sensor_id, None)\n",
    "            elif sensor_type == \"RADAR\":\n",
    "                out = self.RADARS.get(sensor_id, None)\n",
    "            elif sensor_type == \"CAMERA\":\n",
    "                out = self.CAMERAS.get(sensor_id, None)\n",
    "            elif sensor_type == \"FUSEDRADAR\":\n",
    "                out = self.FUSEDRADARS.get(sensor_id, None)\n",
    "            if out is None:\n",
    "                print(\"WARNING: there is no sensor id %s for sensor type %s\" % (sensor_id, sensor_type))\n",
    "            return out\n",
    "        else:\n",
    "            print(\"WARNING: sensor type %s not supported\" % sensor_type)\n",
    "            return None\n",
    "\n",
    "    def apply_data_manipulators(\n",
    "        self,\n",
    "        data_cache: dict,\n",
    "        registered_data_manipulators: list,\n",
    "        data2load: dict,\n",
    "        version2load: dict,\n",
    "        label_converter: ClassMapper,\n",
    "        data_handlers: Optional[list],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        apply data manipulators\n",
    "\n",
    "        Args:\n",
    "            data_cache:                     cached data\n",
    "            registered_data_manipulators:   data manipulators assigned to data package\n",
    "            data2load:                      which data schall be loaded\n",
    "            version2load:                   optional specifications of data versions to load\n",
    "            label_converter:                class mapper\n",
    "            data_handlers:                  data handling structure\n",
    "        \"\"\"\n",
    "\n",
    "        # add and perform data manipulators\n",
    "        for rdm in registered_data_manipulators:\n",
    "            # check type\n",
    "            if not type(rdm) is tuple and len(rdm) == 2 and issubclass(rdm[0], DataManipulator) and type(rdm[1]) is dict:\n",
    "                raise ValueError(\n",
    "                    \"Each element of data_manipulators must be tuple of (class derived from DataManipulator, argument dict) in DataProvider.__init__\"\n",
    "                )\n",
    "            # create manipulator and perform data manipulation\n",
    "            if data_cache is None:\n",
    "                dc = None\n",
    "            else:\n",
    "                dc = data_cache[0]\n",
    "            data_config = {\n",
    "                \"data2load\": data2load,\n",
    "                \"version2load\": version2load,\n",
    "                \"label_converter\": label_converter,\n",
    "                \"data_handlers\": data_handlers,\n",
    "                \"data_cache\": dc,\n",
    "            }\n",
    "            dm_instance = rdm[0](self, data_config, **rdm[1])\n",
    "            # add manipulator link\n",
    "            self.data_manipulators.append(dm_instance)\n",
    "\n",
    "        # perform cache independent updates as well\n",
    "        self.update_data_manipulators_cache_independent(data_cache, registered_data_manipulators, data2load, version2load, label_converter, data_handlers)\n",
    "\n",
    "    def update_data_manipulators_cache_independent(\n",
    "        self,\n",
    "        data_cache: dict,\n",
    "        registered_data_manipulators: list,\n",
    "        data2load: dict,\n",
    "        version2load: dict,\n",
    "        label_converter: ClassMapper,\n",
    "        data_handlers: Optional[list],\n",
    "    ):\n",
    "        \"\"\"\n",
    "        update data manipulators cache independent, this functionality is called even if generator takes DataProvider from cache\n",
    "\n",
    "        Args:\n",
    "            data_cache:                     cached data\n",
    "            registered_data_manipulators:   data manipulators assigned to data package\n",
    "            data2load:                      which data shall be loaded\n",
    "            version2load:                   optional specifications of data versions to load\n",
    "            label_converter:                class mapper\n",
    "            data_handlers:                  data handling structure\n",
    "        \"\"\"\n",
    "        # get lookup of data manipulator type to manipulator input\n",
    "        manipulator_input_lookup = {rdm[0]: rdm[1] for rdm in registered_data_manipulators}\n",
    "        # cycle data_manipulator instances of this DataProvider and update with registered input\n",
    "        for manipulator_instance in self.data_manipulators:\n",
    "            # get input from lookup\n",
    "            manipulator_input = manipulator_input_lookup[type(manipulator_instance)]\n",
    "            # perform update\n",
    "            if data_cache is None:\n",
    "                dc = None\n",
    "            else:\n",
    "                dc = data_cache[0]\n",
    "            data_config = {\n",
    "                \"data2load\": data2load,\n",
    "                \"version2load\": version2load,\n",
    "                \"label_converter\": label_converter,\n",
    "                \"data_handlers\": data_handlers,\n",
    "                \"data_cache\": dc,\n",
    "            }\n",
    "            manipulator_instance.update_cache_independent(self, data_config, **manipulator_input)\n",
    "\n",
    "    @staticmethod\n",
    "    def callback_dmy() -> bool:\n",
    "        \"\"\"\n",
    "        dummy for a callback function that could be connected to a viewer with\n",
    "        Viewer.register_callback_function('dmy_name', DataProvider.callback_dmy)\n",
    "\n",
    "        Returns:\n",
    "            requires repaint\n",
    "        \"\"\"\n",
    "        print(\"callback_dmy_cnt=%d\" % DataProvider.callback_dmy_cnt)\n",
    "        print(\"Increase callback_dmy_cnt\")\n",
    "        DataProvider.callback_dmy_cnt += 1\n",
    "        # return True if repaint is required after callback\n",
    "        return False\n",
    "\n",
    "    def get_byte_size(self) -> int:\n",
    "        \"\"\"\n",
    "        get approximate size of DataProvider object in byte\n",
    "\n",
    "        Returns:\n",
    "            approximate size of DataProvider object in byte\n",
    "        \"\"\"\n",
    "        sz = 0\n",
    "        for val in self.LIDARS.values():\n",
    "            sz += sys.getsizeof(val.pointcloud)\n",
    "        for val in self.RADARS.values():\n",
    "            sz += sys.getsizeof(val.pointcloud)\n",
    "        for val in self.FUSEDRADARS.values():\n",
    "            sz += sys.getsizeof(val.pointcloud)\n",
    "        for val in self.CAMERAS.values():\n",
    "            sz += sys.getsizeof(val.image)\n",
    "        return sz\n",
    "\n",
    "    def read_data(self, dp_meta_data: DataProviderMetaData, data2load: dict, version2load: dict, data_cache: dict):\n",
    "        \"\"\"\n",
    "        read data and fill it into DataProvider structures\n",
    "\n",
    "        Args:\n",
    "            dp_meta_data:   meta data structure\n",
    "            data2load:      which data schall be loaded\n",
    "            version2load:   optional specifications of data versions to load\n",
    "            data_cache:     cached data\n",
    "        \"\"\"\n",
    "        # read data\n",
    "        data = DataProvider.read(dp_meta_data, data2load, version2load, data_cache)\n",
    "        # create data package\n",
    "        self.META = data[\"META\"]\n",
    "        self.LIDARS = data[\"LIDARS\"]\n",
    "        self.RADARS = data[\"RADARS\"]\n",
    "        self.FUSEDRADARS = data[\"FUSEDRADARS\"]\n",
    "        self.CAMERAS = data[\"CAMERAS\"]\n",
    "        self.LABEL = data[\"LABEL\"]\n",
    "        # create sensor name to index lookups\n",
    "        self.META[\"sensor_name_to_idx_lookup\"] = dict()\n",
    "        for stype in DataProvider.named_sensor_types:\n",
    "            self.META[\"sensor_name_to_idx_lookup\"][stype] = dict()\n",
    "            for sid, sdat in data[stype + \"S\"].items():\n",
    "                self.META[\"sensor_name_to_idx_lookup\"][stype][sdat.sensor_name] = sid\n",
    "\n",
    "    @staticmethod\n",
    "    def read(dp_meta_data: DataProviderMetaData, data2load: dict, version2load: dict, data_cache: dict) -> dict:\n",
    "        \"\"\"\n",
    "        the read function which is called by the DataProvider\n",
    "\n",
    "        Args:\n",
    "            dp_meta_data:   meta data structure\n",
    "            data2load:      which data schall be loaded\n",
    "            version2load:   optional specifications of data versions to load\n",
    "            data_cache:     cached data\n",
    "\n",
    "        Returns:\n",
    "            data dictionary\n",
    "        \"\"\"\n",
    "        source_typs = {\"trainingTool2\": DataProvider.read_training_tooling_2, \"cbor\": DataProvider.read_cbor}\n",
    "\n",
    "        load_function = source_typs.get(dp_meta_data.source_type, lambda dp_meta_data, data2load: print(\"Unknown load mode: \" + dp_meta_data.source_type))\n",
    "        return load_function(dp_meta_data, data2load, version2load, data_cache)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_training_tooling_2(dp_meta_data: DataProviderMetaData, data2load: dict, version2load: dict = {}, data_cache: Optional[dict] = None) -> dict:\n",
    "        \"\"\"\n",
    "        The training_tooling_2 read function:\n",
    "        Every read function should return the data in the structure the DataProvider expects. This means a dictionary with\n",
    "        the following members:\n",
    "          LIDARS: Dict of lidar objects, each containing a pointcloud and a lidar calibration object\n",
    "          RADARS: Dict of radar objects, each containing a pointcloud and a radar calibration object\n",
    "          FUSEDRADARS: Dict of virtual radar sensors combining the data of a list of real sensors\n",
    "          CAMERAS: Dict of camera objects, each containing a pointcloud and a camera calibration object\n",
    "          LABEL: Dict of Label layers, each containing a label list\n",
    "\n",
    "        Args:\n",
    "            dp_meta_data:   meta data structure\n",
    "            data2load:      which data schall be loaded\n",
    "            version2load:   optional specifications of data versions to load\n",
    "            data_cache:     cached data\n",
    "\n",
    "        Returns:\n",
    "            data dictionary\n",
    "        \"\"\"\n",
    "\n",
    "        # filenames\n",
    "        meta_file_name = os.path.join(dp_meta_data.folder, \"Meta\", \"%s.h5\" % dp_meta_data.sequence_name)\n",
    "        lidar_file_name = os.path.join(dp_meta_data.folder, \"Lidar\", \"%s.h5\" % dp_meta_data.sequence_name)\n",
    "        camera_file_name = os.path.join(dp_meta_data.folder, \"Camera\", \"%s.h5\" % dp_meta_data.sequence_name)\n",
    "        radar_detections_file_name = os.path.join(dp_meta_data.folder, \"Radar_Detections\", \"%s.h5\" % dp_meta_data.sequence_name)\n",
    "\n",
    "        # check if data cache contains relevant information\n",
    "        data_cache_relevant = (\n",
    "            data_cache is not None and data_cache[1].folder == dp_meta_data.folder and data_cache[1].sequence_name == dp_meta_data.sequence_name\n",
    "        )\n",
    "\n",
    "        # load radar\n",
    "        radars = dict()\n",
    "        if \"RADAR\" in data2load.keys() and os.path.exists(radar_detections_file_name):\n",
    "            radar_file = h5py.File(radar_detections_file_name, \"r\")\n",
    "            meta_file = h5py.File(meta_file_name, \"r\")\n",
    "            for rad_id in dp_meta_data.aligned_fids[\"RADAR\"]:\n",
    "                if dp_meta_data.aligned_fids[\"RADAR\"][rad_id] is not None:\n",
    "                    if data_cache_relevant and data_cache[1].aligned_fids[\"RADAR\"][rad_id] == dp_meta_data.aligned_fids[\"RADAR\"][rad_id]:\n",
    "                        radars[rad_id] = data_cache[0].RADARS[rad_id]\n",
    "                    else:\n",
    "                        rad_name = radar_file[\"radar_idx_to_name\"][rad_id]\n",
    "                        rad_gp = radar_file[\"sensors\"][rad_name]\n",
    "                        # get current frame\n",
    "                        frame = dp_meta_data.aligned_fids[\"RADAR\"][rad_id]\n",
    "                        ## get pointcloud\n",
    "                        radar_version = version2load.get(\"RADAR\", \"v0\")  # deactivate pointcloud if version is None\n",
    "                        # get slice\n",
    "                        slice = get_slice(rad_gp, frame)\n",
    "                        if radar_version is not None and slice[1] - slice[0] > 0:\n",
    "                            # get pointcloud\n",
    "                            radar_pc = np.zeros([16, slice[1] - slice[0]], dtype=np.float32)\n",
    "                            # get data\n",
    "                            cols = rad_gp[\"data\"].attrs[\"columns\"].split(\";\")\n",
    "                            lookup = dict(zip(cols, range(len(cols))))\n",
    "                            cols2slice = [lookup[kk] for kk in [\"x\", \"y\", \"z\", \"rcs\", \"rangeRate\"]]\n",
    "                            radar_pc[0:5, :] = rad_gp[\"data\"][slice[0] : slice[1], :][:, cols2slice].T\n",
    "                            # radar_pc[5:10, :] = ['doppl_propasal_0', 'doppl_propasal_1', 'doppl_propasal_2', 'doppl_propasal_3', 'doppl_propasal_4']\n",
    "                            radar_pc[10, :] = rad_gp[\"looktype\"][frame, 0]\n",
    "                            # radar_pc[11, :] ['isSingleTarget']\n",
    "                            radar_pc[12, :] = rad_id\n",
    "                            radar_pc[13, :] = rad_gp[\"timestamps\"][frame, 0]\n",
    "                            radar_pc[14, :] = frame\n",
    "                            radar_pc[15, :] = rad_gp[\"data\"][slice[0]:slice[1], -1].T\n",
    "                        else:\n",
    "                            radar_pc = None\n",
    "                        # create radar calib\n",
    "                        radar_calib = RadarCalib()\n",
    "                        radar_calib.T_V2R = rad_gp[\"calib\"][\"extrinsic\"][()].T\n",
    "                        radar_calib.T_R2V = np.linalg.inv(radar_calib.T_V2R)\n",
    "                        radar_calib.aperture_angle = meta_file[\"radar_aperture_angle\"][rad_id, :]\n",
    "                        # create radar and fill meta information\n",
    "                        radars[rad_id] = Radar(\n",
    "                            pointcloud=radar_pc,\n",
    "                            radar_calib=radar_calib,\n",
    "                            timestamp=dp_meta_data.aligned_timestamps[\"RADAR\"][rad_id],\n",
    "                            frame_id=dp_meta_data.aligned_fids[\"RADAR\"][rad_id],\n",
    "                            scan_id=dp_meta_data.aligned_sids[\"RADAR\"][rad_id],\n",
    "                            sensor_name=dp_meta_data.aligned_sensor_names[\"RADAR\"][rad_id],\n",
    "                            version=None,\n",
    "                            subsensors=None,\n",
    "                        )\n",
    "                        radars[rad_id].is_fused = False\n",
    "\n",
    "                        # load egomotion file\n",
    "                        ego_gp = rad_gp[\"ego_motion\"]\n",
    "                        radars[rad_id].ego_transform = ego_gp[\"T0\"][frame].T\n",
    "                        radars[rad_id].ego_transform_to_previous = ego_gp[\"T\"][frame].T\n",
    "                        radars[rad_id].v_lon, radars[rad_id].v_lat, radars[rad_id].yawrate = ego_gp[\"ego_scalars\"][frame]\n",
    "\n",
    "            # close file handle\n",
    "            radar_file.close()\n",
    "            meta_file.close()\n",
    "\n",
    "        # load fused radar\n",
    "        fusedradars = dict()\n",
    "        if \"FUSEDRADAR\" in data2load.keys():\n",
    "            if dp_meta_data.aligned_fids[\"FUSEDRADAR\"][0] is not None:\n",
    "                if data_cache_relevant and data_cache[1].aligned_fids[\"FUSEDRADAR\"][0] == dp_meta_data.aligned_fids[\"FUSEDRADAR\"][0]:\n",
    "                    fusedradars[0] = data_cache[0].FUSEDRADARS[0]\n",
    "                else:\n",
    "                    if os.path.exists(meta_file_name):\n",
    "                        meta_file = h5py.File(meta_file_name, \"r\")\n",
    "                        frame = dp_meta_data.aligned_fids[\"FUSEDRADAR\"][0]\n",
    "                        ## get pointcloud and calib\n",
    "                        radar_pc = None\n",
    "                        radar_version = version2load.get(\"FUSEDRADAR\", \"v0\")  # deactivate pointcloud if version is None\n",
    "                        radar_calib = dict()\n",
    "                        T_V2R = meta_file[\"radar_calib\"][()].transpose(0, 2, 1)  # TODO: Check me!!!\n",
    "                        for rad_id, rad in dp_meta_data.aligned_subsensors[\"FUSEDRADAR\"][0].items():\n",
    "                            radar_calib[rad_id] = RadarCalib()\n",
    "                            radar_calib[rad_id].T_V2R = T_V2R[rad_id]\n",
    "                            radar_calib[rad_id].T_R2V = np.linalg.inv(T_V2R[rad_id])\n",
    "                            radar_calib[rad_id].aperture_angle = meta_file[\"radar_aperture_angle\"][rad_id, :]\n",
    "                        if os.path.exists(radar_detections_file_name):\n",
    "                            radar_file = h5py.File(radar_detections_file_name, \"r\")\n",
    "                            # load batch to radar frame mapping\n",
    "                            batch2radar_map = radar_file[\"batch_to_radar_frame_map\"]\n",
    "                            # fill local pointcloud\n",
    "                            pointclouds = []\n",
    "                            for rad_id, rad in dp_meta_data.aligned_subsensors[\"FUSEDRADAR\"][0].items():\n",
    "                                if rad is not None:\n",
    "                                    rad_gp = radar_file[\"sensors\"][rad[\"sensor_name\"]]\n",
    "                                    # get slice\n",
    "                                    radar_frm = batch2radar_map[frame, rad_id]\n",
    "                                    if radar_frm >= 0:\n",
    "                                        slice = get_slice(rad_gp, radar_frm)\n",
    "                                        if radar_version is not None and slice[1] - slice[0] > 0:\n",
    "                                            # get pointcloud\n",
    "                                            radar_pc_loc = np.zeros([16, slice[1] - slice[0]], dtype=np.float32)\n",
    "                                            # get data\n",
    "                                            cols = rad_gp[\"data\"].attrs[\"columns\"].split(\";\")\n",
    "                                            lookup = dict(zip(cols, range(len(cols))))\n",
    "                                            cols2slice = [lookup[kk] for kk in [\"x\", \"y\", \"z\", \"rcs\", \"rangeRate\"]]\n",
    "                                            radar_pc_loc[0:5, :] = rad_gp[\"data\"][slice[0] : slice[1], :][:, cols2slice].T\n",
    "                                            # radar_pc[5:10, :] = ['doppl_propasal_0', 'doppl_propasal_1', 'doppl_propasal_2', 'doppl_propasal_3', 'doppl_propasal_4']\n",
    "                                            radar_pc_loc[10, :] = rad_gp[\"looktype\"][radar_frm, 0]\n",
    "                                            # radar_pc[11, :] ['isSingleTarget']\n",
    "                                            radar_pc_loc[12, :] = rad_id\n",
    "                                            radar_pc_loc[13, :] = rad_gp[\"timestamps\"][radar_frm, 0]\n",
    "                                            radar_pc_loc[14, :] = radar_frm\n",
    "                                            radar_pc_loc[15, :] = rad_gp[\"data\"][slice[0]:slice[1], -1].T\n",
    "                                            pointclouds.append(radar_pc_loc)\n",
    "                            # concatenate pointclouds\n",
    "                            if len(pointclouds) > 0:\n",
    "                                radar_pc = np.hstack(pointclouds)\n",
    "                            # close file handle\n",
    "                            radar_file.close()\n",
    "                        # create radar and fill meta information\n",
    "                        fusedradars[0] = Radar(\n",
    "                            pointcloud=radar_pc,\n",
    "                            radar_calib=radar_calib,\n",
    "                            timestamp=dp_meta_data.aligned_timestamps[\"FUSEDRADAR\"][0],\n",
    "                            frame_id=dp_meta_data.aligned_fids[\"FUSEDRADAR\"][0],\n",
    "                            scan_id=None,\n",
    "                            sensor_name=dp_meta_data.aligned_sensor_names[\"FUSEDRADAR\"][0],\n",
    "                            version=None,\n",
    "                            subsensors=dp_meta_data.aligned_subsensors[\"FUSEDRADAR\"][0],\n",
    "                        )\n",
    "                        fusedradars[0].is_fused = True\n",
    "\n",
    "                        # load egomotion file\n",
    "                        fusedradars[0].ego_transform = meta_file[\"T0\"][frame].T\n",
    "                        fusedradars[0].ego_transform_to_previous = meta_file[\"T\"][frame].T\n",
    "                        fusedradars[0].v_lon, fusedradars[0].v_lat, fusedradars[0].yawrate = meta_file[\"ego_scalars\"][frame]\n",
    "\n",
    "                        # close file handle\n",
    "                        meta_file.close()\n",
    "\n",
    "        # load lidar\n",
    "        lidars = dict()\n",
    "        if \"LIDAR\" in data2load.keys() and os.path.exists(lidar_file_name):\n",
    "            lidar_file = h5py.File(lidar_file_name, \"r\")\n",
    "            for lid_id in dp_meta_data.aligned_fids[\"LIDAR\"]:\n",
    "                if dp_meta_data.aligned_fids[\"LIDAR\"][lid_id] is not None:\n",
    "                    if data_cache_relevant and data_cache[1].aligned_fids[\"LIDAR\"][lid_id] == dp_meta_data.aligned_fids[\"LIDAR\"][lid_id]:\n",
    "                        lidars[lid_id] = data_cache[0].LIDARS[lid_id]\n",
    "                    else:\n",
    "                        lid_name = lidar_file[\"lidar_idx_to_name\"][lid_id]\n",
    "                        lid_gp = lidar_file[\"sensors\"][lid_name]\n",
    "                        # get current frame\n",
    "                        frame = dp_meta_data.aligned_fids[\"LIDAR\"][lid_id]\n",
    "                        # slice\n",
    "                        slice = get_slice(lid_gp, frame)\n",
    "                        # reshape\n",
    "                        lidar_pc = np.zeros([9, slice[1] - slice[0]], dtype=np.float64)\n",
    "                        lidar_pc[0:4, :] = lid_gp[\"data\"][slice[0] : slice[1], :].T\n",
    "                        # normalize\n",
    "                        lidar_pc[3, :] /= 255\n",
    "                        # set timestamp\n",
    "                        lidar_pc[4, :] = (lid_gp[\"timestamps\"][frame] + lid_gp[\"data_auxiliary\"][slice[0] : slice[1], 0]) * 1000\n",
    "                        # set file id\n",
    "                        lidar_pc[5, :] = frame\n",
    "                        # prepare label, instance_id, stationary_flag\n",
    "                        lidar_pc[6:9, :] = -1\n",
    "                        # create calib\n",
    "                        lidar_calib = LidarCalib()\n",
    "                        lidar_calib.T_V2L = lid_gp[\"calib\"][\"extrinsic\"][()].T\n",
    "                        lidar_calib.T_L2V = np.linalg.inv(lidar_calib.T_V2L)\n",
    "                        lidars[lid_id] = Lidar(\n",
    "                            pointcloud=lidar_pc,\n",
    "                            lidar_calib=lidar_calib,\n",
    "                            timestamp=dp_meta_data.aligned_timestamps[\"LIDAR\"][lid_id],\n",
    "                            frame_id=dp_meta_data.aligned_fids[\"LIDAR\"][lid_id],\n",
    "                            scan_id=dp_meta_data.aligned_sids[\"LIDAR\"][lid_id],\n",
    "                            sensor_name=dp_meta_data.aligned_sensor_names[\"LIDAR\"][lid_id],\n",
    "                        )\n",
    "\n",
    "                        # load egomotion\n",
    "                        ego_gr = lid_gp[\"ego_motion\"]\n",
    "                        lidars[lid_id].ego_transform = ego_gr[\"T0\"][frame].T\n",
    "                        lidars[lid_id].ego_transform_to_previous = ego_gr[\"T\"][frame].T\n",
    "                        lidars[lid_id].v_lon, lidars[lid_id].v_lat, lidars[lid_id].yawrate = ego_gr[\"ego_scalars\"][frame]\n",
    "\n",
    "            # close handle\n",
    "            lidar_file.close()\n",
    "\n",
    "        # load labels\n",
    "        labels = {\"GT\": []}\n",
    "        label_GT_src = None\n",
    "        if \"LABEL\" in data2load.keys():\n",
    "            if data_cache_relevant and data_cache[1].aligned_fids[\"LABEL\"] == dp_meta_data.aligned_fids[\"LABEL\"]:\n",
    "                labels = data_cache[0].LABEL\n",
    "            else:\n",
    "                if dp_meta_data.aligned_fids[\"LABEL\"] is not None:\n",
    "                    labels_filename = dp_meta_data.aligned_fids[\"LABEL\"][\"file\"]\n",
    "                    if os.path.exists(labels_filename):\n",
    "                        label_file = h5py.File(labels_filename, \"r\")\n",
    "                        # create h5 handles for all sensors to load\n",
    "                        if \"sensors\" in label_file:\n",
    "                            # for Lidar and single radars we use sensorwise label file\n",
    "                            label_sensor_gp = label_file[\"sensors\"][dp_meta_data.aligned_fids[\"LABEL\"][\"sensor\"]]\n",
    "                        else:\n",
    "                            # for everything else use the FUSEDRADAR label group\n",
    "                            label_sensor_gp = label_file\n",
    "                        # get unique list combining ground truth source from dataspec and additional sources from data2load\n",
    "                        GT_src = dp_meta_data.aligned_fids[\"LABEL\"][\"source\"]\n",
    "                        sources2load = list(set([GT_src] + data2load[\"LABEL\"]))\n",
    "                        # cycle sources\n",
    "                        for lab_src in sources2load:\n",
    "                            # skip label source None\n",
    "                            if lab_src is None:\n",
    "                                continue\n",
    "                            labels[lab_src] = []\n",
    "                            if lab_src.encode(\"utf-8\") in label_sensor_gp[\"label_idx_to_name\"]:\n",
    "                                label_gp = label_sensor_gp[\"label_sources\"][lab_src]\n",
    "                                frame_range = label_gp[\"frame_range\"]\n",
    "                                # capture trivial case were no labels are available at all\n",
    "                                if label_gp[\"data\"].shape[0] == 0:\n",
    "                                    continue\n",
    "                                data_UUID_mapping = [uuid.decode(\"utf-8\") for uuid in label_gp[\"data_UUID_mapping\"]]\n",
    "                                # get frame\n",
    "                                frame = dp_meta_data.aligned_fids[\"LABEL\"][\"frame\"]\n",
    "                                if frame_range[0] <= frame <= frame_range[1]:\n",
    "                                    # slice\n",
    "                                    slice = get_slice(label_gp, frame)\n",
    "                                    data = label_gp[\"data\"][slice[0] : slice[1], :]\n",
    "                                    cols = label_gp[\"data\"].attrs[\"columns\"].split(\";\")\n",
    "                                    lookup = dict(zip(cols, range(len(cols))))\n",
    "                                    for i in range(data.shape[0]):\n",
    "                                        label = Label()\n",
    "                                        # class\n",
    "                                        if \"labels_mapping\" in label_gp:\n",
    "                                            label.type = label_gp[\"labels_mapping\"][data[i][lookup[\"Label\"]].astype(int)].decode(\"utf-8\")\n",
    "                                        else:\n",
    "                                            label.type = label_gp[\"data_Label_mapping\"][data[i][lookup[\"Label\"]].astype(int)].decode(\"utf-8\")\n",
    "                                        # position\n",
    "                                        label.loc_3d[0] = data[i][lookup[\"XPos\"]]\n",
    "                                        label.loc_3d[1] = data[i][lookup[\"YPos\"]]\n",
    "                                        label.loc_3d[2] = data[i][lookup[\"ZPos\"]]\n",
    "                                        # dimension (length, width, height)\n",
    "                                        label.dim_3d[0] = data[i][lookup[\"XDim\"]]\n",
    "                                        label.dim_3d[1] = data[i][lookup[\"YDim\"]]\n",
    "                                        label.dim_3d[2] = data[i][lookup[\"ZDim\"]]\n",
    "                                        # rotation as 3d matrix vectorized along columns\n",
    "                                        # cx = math.cos(data['Roll'][i])\n",
    "                                        # sx = math.sin(data['Roll'][i])\n",
    "                                        # cy = math.cos(data['Pitch'][i])\n",
    "                                        # sy = math.sin(data['Pitch'][i])\n",
    "                                        yaw = data[i][lookup[\"Yaw\"]]\n",
    "                                        cz = math.cos(yaw)\n",
    "                                        sz = math.sin(yaw)\n",
    "                                        # label.rot_3d[0, 0] = cy * cz\n",
    "                                        # label.rot_3d[0, 1] = sx * sy * cz - cx * sz\n",
    "                                        # label.rot_3d[0, 2] = cx * sy * cz + sx * sz\n",
    "                                        # label.rot_3d[1, 0] = cy * sz\n",
    "                                        # label.rot_3d[1, 1] = sx * sy * sz + cx * cz\n",
    "                                        # label.rot_3d[1, 2] = cx * sy * sz - sx * cz\n",
    "                                        # label.rot_3d[2, 0] = -sy\n",
    "                                        # label.rot_3d[2, 1] = sx * cy\n",
    "                                        # label.rot_3d[2, 2] = cx * cy\n",
    "                                        label.rot_3d[0, 0] = cz\n",
    "                                        label.rot_3d[0, 1] = -sz\n",
    "                                        label.rot_3d[0, 2] = 0.0\n",
    "                                        label.rot_3d[1, 0] = sz\n",
    "                                        label.rot_3d[1, 1] = cz\n",
    "                                        label.rot_3d[1, 2] = 0.0\n",
    "                                        label.rot_3d[2, 0] = 0.0\n",
    "                                        label.rot_3d[2, 1] = 0.0\n",
    "                                        label.rot_3d[2, 2] = 1.0\n",
    "                                        # velocity and acceleration\n",
    "                                        label.vel_3d_filtered[0] = data[i][lookup[\"XVelFiltered\"]]\n",
    "                                        label.vel_3d_filtered[1] = data[i][lookup[\"YVelFiltered\"]]\n",
    "                                        label.vel_3d_filtered[2] = 0\n",
    "                                        label.acc_3d_filtered[0] = data[i][lookup[\"XAccFiltered\"]]\n",
    "                                        label.acc_3d_filtered[1] = data[i][lookup[\"YAccFiltered\"]]\n",
    "                                        label.acc_3d_filtered[2] = 0\n",
    "                                        if \"XVel\" in lookup:\n",
    "                                            label.vel_3d[0] = data[i][lookup[\"XVel\"]]\n",
    "                                            label.vel_3d[1] = data[i][lookup[\"YVel\"]]\n",
    "                                            label.vel_3d[2] = data[i][lookup[\"ZVel\"]]\n",
    "                                        else:\n",
    "                                            label.vel_3d = label.vel_3d_filtered.copy()\n",
    "                                        if \"XAcc\" in lookup:\n",
    "                                            label.acc_3d[0] = data[i][lookup[\"XAcc\"]]\n",
    "                                            label.acc_3d[1] = data[i][lookup[\"YAcc\"]]\n",
    "                                            label.acc_3d[2] = data[i][lookup[\"ZAcc\"]]\n",
    "                                        else:\n",
    "                                            label.acc_3d = label.acc_3d_filtered.copy()\n",
    "                                        # track id\n",
    "                                        label.track_id = data_UUID_mapping[int(data[i][lookup[\"UUID\"]])]  # int(data[i][lookup['UUID']])\n",
    "                                        # no care flag\n",
    "                                        if \"NoCare\" in lookup:\n",
    "                                            label.no_care = data[i][lookup[\"NoCare\"]]\n",
    "                                        else:\n",
    "                                            label.no_care = False\n",
    "                                        if \"geom_NoCare\" in lookup:\n",
    "                                            label.geom_no_care = data[i][lookup[\"geom_NoCare\"]]\n",
    "                                        else:\n",
    "                                            label.geom_no_care = False\n",
    "                                        # visibility\n",
    "                                        vis_entries = [vv for vv in lookup if \"Visibility\" in vv]\n",
    "                                        if len(vis_entries) > 0:\n",
    "                                            label.visibility = {int(ve.split(\"_\")[1]): data[i][lookup[ve]] for ve in vis_entries}\n",
    "                                        # mark as annotation\n",
    "                                        label.breed = \"annotation\"\n",
    "\n",
    "                                        labels[lab_src].append(label)\n",
    "                        # rename round truth source to 'GT'\n",
    "                        if GT_src in labels:\n",
    "                            label_GT_src = GT_src\n",
    "                            labels[\"GT\"] = labels.pop(GT_src)\n",
    "                        # close file handle\n",
    "                        label_file.close()\n",
    "\n",
    "        # load cameras\n",
    "        cameras = dict()\n",
    "        if \"CAMERA\" in data2load.keys() and os.path.exists(camera_file_name):\n",
    "            camera_file = h5py.File(camera_file_name, \"r\")\n",
    "            for cam_id in dp_meta_data.aligned_fids[\"CAMERA\"]:\n",
    "                if dp_meta_data.aligned_fids[\"CAMERA\"][cam_id] is not None:\n",
    "                    if data2load[\"CAMERA\"] == [] or cam_id in data2load[\"CAMERA\"]:\n",
    "                        if data_cache_relevant and data_cache[1].aligned_fids[\"CAMERA\"][cam_id] == dp_meta_data.aligned_fids[\"CAMERA\"][cam_id]:\n",
    "                            cameras[cam_id] = data_cache[0].CAMERAS[cam_id]\n",
    "                        else:\n",
    "                            # get handle\n",
    "                            cam_name = camera_file[\"camera_idx_to_name\"][cam_id]\n",
    "                            cam_gp = camera_file[\"sensors\"][cam_name]\n",
    "                            # get current frame\n",
    "                            frame = dp_meta_data.aligned_fids[\"CAMERA\"][cam_id]\n",
    "                            # get image\n",
    "                            img = cv2.imdecode(cam_gp[\"data\"][frame], cv2.IMREAD_COLOR)\n",
    "                            # get calib\n",
    "                            # read inner calibration\n",
    "                            K = cam_gp[\"calib\"][\"intrinsic\"][()].T\n",
    "                            P = np.matmul(K, np.eye(3, 4))\n",
    "\n",
    "                            # combine with L2V to get camera to vehicle\n",
    "                            T_V2C = cam_gp[\"calib\"][\"extrinsic\"][()].T\n",
    "\n",
    "                            ####### some code to change/test camera calibration #######\n",
    "                            # from scipy.spatial.transform import Rotation as Rot\n",
    "                            # correction_angles_degree = {\n",
    "                            #     'ITC_GoFast_Ladybug_Front': [0, 0, -5],\n",
    "                            #     'ITC_GoFast_Ladybug_Rear_Left': [0, 0, 0],\n",
    "                            #     'ITC_GoFast_Ladybug_Front_Left': [0, 0, 0],\n",
    "                            #     'ITC_GoFast_Ladybug_Rear_Right': [0, 0, 0],\n",
    "                            #     'ITC_GoFast_Ladybug_Front_Right': [0, 0, 0],\n",
    "                            # }\n",
    "                            # T_C2V = np.linalg.inv(T_V2C)\n",
    "                            # R_correction = Rot.from_euler('xyz', correction_angles_degree[cam_name.decode('utf-8')], degrees=True).as_matrix()\n",
    "                            # R_axis = np.array([[0, 0, 1],\n",
    "                            #                    [1, 0, 0],\n",
    "                            #                    [0, 1, 0]])\n",
    "                            # R_correction = R_correction @ R_axis\n",
    "                            #\n",
    "                            # T_C2V[:3, :3] = T_C2V[:3, :3] @ R_correction\n",
    "                            # T_V2C = np.linalg.inv(T_C2V)\n",
    "                            # print(cam_name.decode('utf-8'), Rot.from_matrix(T_C2V[:3, :3]).as_quat())\n",
    "\n",
    "                            # create camera structure\n",
    "                            cameracalib = CameraCalib(\n",
    "                                data={\"img_width\": cam_gp[\"calib\"][\"img_size\"][0], \"img_height\": cam_gp[\"calib\"][\"img_size\"][1], \"T_V2C\": T_V2C, \"P\": P}\n",
    "                            )\n",
    "                            # create camera object\n",
    "                            cameras[cam_id] = Camera(\n",
    "                                image=img,\n",
    "                                camera_calib=cameracalib,\n",
    "                                timestamp=dp_meta_data.aligned_timestamps[\"CAMERA\"][cam_id],\n",
    "                                frame_id=dp_meta_data.aligned_fids[\"CAMERA\"][cam_id],\n",
    "                                scan_id=dp_meta_data.aligned_sids[\"CAMERA\"][cam_id],\n",
    "                                sensor_name=dp_meta_data.aligned_sensor_names[\"CAMERA\"][cam_id],\n",
    "                            )\n",
    "\n",
    "                            # load egomotion\n",
    "                            ego_gr = cam_gp[\"ego_motion\"]\n",
    "                            cameras[cam_id].ego_transform = ego_gr[\"T0\"][frame].T\n",
    "                            cameras[cam_id].ego_transform_to_previous = ego_gr[\"T\"][frame].T\n",
    "                            cameras[cam_id].v_lon, cameras[cam_id].v_lat, cameras[cam_id].yawrate = ego_gr[\"ego_scalars\"][frame]\n",
    "\n",
    "            # close handle\n",
    "            camera_file.close()\n",
    "\n",
    "        # get meta information\n",
    "        data_info = None\n",
    "        vehicle_info = None\n",
    "        if os.path.exists(meta_file_name):\n",
    "            meta_file = h5py.File(meta_file_name, \"r\")\n",
    "            data_info = dict(meta_file[\"info\"].attrs)\n",
    "            vehicle_info = dict(meta_file[\"vehicle_info\"].attrs)\n",
    "            meta_file.close()\n",
    "\n",
    "        # create result structure\n",
    "        ret = {\n",
    "            \"META\": {\n",
    "                \"sensor_type\": dp_meta_data.sensor_type,\n",
    "                \"sensor_id\": dp_meta_data.sensor_id,\n",
    "                \"sensor_name\": dp_meta_data.sensor_name,\n",
    "                \"sensor_frame_id\": dp_meta_data.aligned_fids[dp_meta_data.sensor_type][dp_meta_data.sensor_id],\n",
    "                \"folder\": dp_meta_data.folder,\n",
    "                \"sequence_name\": dp_meta_data.sequence_name,\n",
    "                \"timestamp\": dp_meta_data.aligned_timestamps[dp_meta_data.sensor_type][dp_meta_data.sensor_id],\n",
    "                \"GT_source\": label_GT_src,\n",
    "                \"data_info\": data_info,\n",
    "                \"vehicle_info\": vehicle_info,\n",
    "            },\n",
    "            \"LIDARS\": lidars,\n",
    "            \"RADARS\": radars,\n",
    "            \"FUSEDRADARS\": fusedradars,\n",
    "            \"CAMERAS\": cameras,\n",
    "            \"LABEL\": labels,\n",
    "        }\n",
    "        return ret\n",
    "\n",
    "    @staticmethod\n",
    "    def read_cbor(dp_meta_data: DataProviderMetaData, data2load: dict, version2load: dict = {}, data_cache: Optional[dict] = None) -> dict:\n",
    "        \"\"\"\n",
    "        The cbor read function:\n",
    "        Every read function should return the data in the structure the DataProvider expects. This means a dictionary with\n",
    "        the following members:\n",
    "          LIDARS: Dict of lidar objects, each containing a pointcloud and a lidar calibration object\n",
    "          RADARS: Dict of radar objects, each containing a pointcloud and a radar calibration object\n",
    "          FUSEDRADARS: Dict of virtual radar sensors combining the data of a list of real sensors\n",
    "          CAMERAS: Dict of camera objects, each containing a pointcloud and a camera calibration object\n",
    "          LABEL: Dict of Label layers, each containing a label list\n",
    "\n",
    "        Args:\n",
    "            dp_meta_data:   meta data structure\n",
    "            data2load:      which data schall be loaded\n",
    "            version2load:   optional specifications of data versions to load\n",
    "            data_cache:     cached data\n",
    "\n",
    "        Returns:\n",
    "            data dictionary\n",
    "        \"\"\"\n",
    "\n",
    "        # check if data cache contains relevant information\n",
    "        data_cache_relevant = (\n",
    "            data_cache is not None and data_cache[1].folder == dp_meta_data.folder and data_cache[1].sequence_name == dp_meta_data.sequence_name\n",
    "        )\n",
    "\n",
    "        # get handle to cbor handler\n",
    "        cbor_handler = dp_meta_data.data_handlers[\"cbor_handler\"]\n",
    "\n",
    "        # load radar\n",
    "        radars = dict()\n",
    "        radar_data = cbor_handler.get_data().get(\"radar\", None)\n",
    "        if \"RADAR\" in data2load.keys() and radar_data:\n",
    "            for rad_id, rad_name in dp_meta_data.aligned_sensor_names[\"RADAR\"].items():\n",
    "                if dp_meta_data.aligned_fids[\"RADAR\"][rad_id] is not None:\n",
    "                    if data_cache_relevant and data_cache[1].aligned_fids[\"RADAR\"][rad_id] == dp_meta_data.aligned_fids[\"RADAR\"][rad_id]:\n",
    "                        radars[rad_id] = data_cache[0].RADARS[rad_id]\n",
    "                    else:\n",
    "                        rad_gp = radar_data[rad_name]\n",
    "                        # get current frame\n",
    "                        frame = dp_meta_data.aligned_fids[\"RADAR\"][rad_id]\n",
    "                        ## get pointcloud\n",
    "                        radar_version = version2load.get(\"RADAR\", \"v0\")  # deactivate pointcloud if version is None\n",
    "                        if radar_version is not None and rad_gp[\"data\"][frame].shape[1] > 0:\n",
    "                            # get pointcloud\n",
    "                            radar_pc = np.zeros([15, rad_gp[\"data\"][frame].shape[1]], dtype=np.float32)\n",
    "                            # get data\n",
    "                            radar_pc[:5, :] = rad_gp[\"data\"][frame][:5, :]  # ['x', 'y', 'z', 'rcs', 'rangeRate']\n",
    "                            # radar_pc[5:10, :] = ['doppl_propasal_0', 'doppl_propasal_1', 'doppl_propasal_2', 'doppl_propasal_3', 'doppl_propasal_4']\n",
    "                            radar_pc[10, :] = rad_gp[\"looktype\"][frame]\n",
    "                            # radar_pc[11, :] ['isSingleTarget']\n",
    "                            radar_pc[12, :] = rad_id\n",
    "                            radar_pc[13, :] = rad_gp[\"timestamps\"][frame]\n",
    "                            radar_pc[14, :] = frame\n",
    "                        else:\n",
    "                            radar_pc = None\n",
    "                        # create radar calib\n",
    "                        radar_calib = RadarCalib()\n",
    "                        radar_calib.T_V2R = rad_gp[\"calib\"][\"T_V2R\"]\n",
    "                        radar_calib.T_R2V = np.linalg.inv(radar_calib.T_V2R)\n",
    "                        radar_calib.aperture_angle = rad_gp[\"calib\"][\"aperture_angle\"]\n",
    "                        # create radar and fill meta information\n",
    "                        radars[rad_id] = Radar(\n",
    "                            pointcloud=radar_pc,\n",
    "                            radar_calib=radar_calib,\n",
    "                            timestamp=dp_meta_data.aligned_timestamps[\"RADAR\"][rad_id],\n",
    "                            frame_id=dp_meta_data.aligned_fids[\"RADAR\"][rad_id],\n",
    "                            scan_id=dp_meta_data.aligned_sids[\"RADAR\"][rad_id],\n",
    "                            sensor_name=dp_meta_data.aligned_sensor_names[\"RADAR\"][rad_id],\n",
    "                            version=None,\n",
    "                            subsensors=None,\n",
    "                        )\n",
    "                        radars[rad_id].is_fused = False\n",
    "\n",
    "                        # load egomotion\n",
    "                        ego_gp = rad_gp[\"ego_motion\"]\n",
    "                        radars[rad_id].ego_transform = ego_gp[\"T0\"][frame]\n",
    "                        radars[rad_id].ego_transform_to_previous = ego_gp[\"T\"][frame]\n",
    "                        radars[rad_id].v_lon, radars[rad_id].v_lat, radars[rad_id].yawrate = ego_gp[\"ego_scalars\"][frame]\n",
    "\n",
    "        # load fused radar\n",
    "        fusedradars = dict()\n",
    "        radar_data = cbor_handler.get_data().get(\"radar\", None)\n",
    "        if \"FUSEDRADAR\" in data2load.keys() and radar_data:\n",
    "            if dp_meta_data.aligned_fids[\"FUSEDRADAR\"][0] is not None:\n",
    "                if data_cache_relevant and data_cache[1].aligned_fids[\"FUSEDRADAR\"][0] == dp_meta_data.aligned_fids[\"FUSEDRADAR\"][0]:\n",
    "                    fusedradars[0] = data_cache[0].FUSEDRADARS[0]\n",
    "                else:\n",
    "                    frame = dp_meta_data.aligned_fids[\"FUSEDRADAR\"][0]\n",
    "                    ## get pointcloud and calib\n",
    "                    radar_pc = None\n",
    "                    radar_version = version2load.get(\"FUSEDRADAR\", \"v0\")  # deactivate pointcloud if version is None\n",
    "                    radar_calib = dict()\n",
    "\n",
    "                    # fill local pointcloud\n",
    "                    pointclouds = []\n",
    "                    for rad_id, rad in dp_meta_data.aligned_subsensors[\"FUSEDRADAR\"][0].items():\n",
    "                        if rad is not None:\n",
    "                            rad_gp = radar_data[rad[\"sensor_name\"]]\n",
    "                            # get slice\n",
    "                            radar_frm = rad[\"frame_id\"]\n",
    "                            if radar_frm >= 0:\n",
    "                                if radar_version is not None:\n",
    "                                    # get pointcloud\n",
    "                                    radar_pc_loc = np.zeros([15, rad_gp[\"data\"][radar_frm].shape[1]], dtype=np.float32)\n",
    "                                    # get data\n",
    "                                    radar_pc_loc[:5, :] = rad_gp[\"data\"][radar_frm][:5, :]  # ['x', 'y', 'z', 'rcs', 'rangeRate']\n",
    "                                    # radar_pc[5:10, :] = ['doppl_propasal_0', 'doppl_propasal_1', 'doppl_propasal_2', 'doppl_propasal_3', 'doppl_propasal_4']\n",
    "                                    radar_pc_loc[10, :] = rad_gp[\"looktype\"][radar_frm]\n",
    "                                    # radar_pc[11, :] ['isSingleTarget']\n",
    "                                    radar_pc_loc[12, :] = rad_id\n",
    "                                    radar_pc_loc[13, :] = rad_gp[\"timestamps\"][radar_frm]\n",
    "                                    radar_pc_loc[14, :] = radar_frm\n",
    "                                    pointclouds.append(radar_pc_loc)\n",
    "                            # fill calib\n",
    "                            T_V2R = rad_gp[\"calib\"][\"T_V2R\"]\n",
    "                            radar_calib[rad_id] = RadarCalib()\n",
    "                            radar_calib[rad_id].T_V2R = T_V2R\n",
    "                            radar_calib[rad_id].T_R2V = np.linalg.inv(T_V2R)\n",
    "                            radar_calib[rad_id].aperture_angle = rad_gp[\"calib\"][\"aperture_angle\"]\n",
    "                    # concatenate pointclouds\n",
    "                    if len(pointclouds) > 0:\n",
    "                        radar_pc = np.hstack(pointclouds)\n",
    "\n",
    "                    # create radar and fill meta information\n",
    "                    fusedradars[0] = Radar(\n",
    "                        pointcloud=radar_pc,\n",
    "                        radar_calib=radar_calib,\n",
    "                        timestamp=dp_meta_data.aligned_timestamps[\"FUSEDRADAR\"][0],\n",
    "                        frame_id=frame,\n",
    "                        scan_id=None,\n",
    "                        sensor_name=dp_meta_data.aligned_sensor_names[\"FUSEDRADAR\"][0],\n",
    "                        version=None,\n",
    "                        subsensors=dp_meta_data.aligned_subsensors[\"FUSEDRADAR\"][0],\n",
    "                    )\n",
    "                    fusedradars[0].is_fused = True\n",
    "\n",
    "                    # load egomotion\n",
    "                    ego_gr = cbor_handler.get_data()[\"network_input\"][\"network\"][\"ego_motion\"]\n",
    "                    fusedradars[0].ego_transform = ego_gr[\"T0\"][frame]\n",
    "                    fusedradars[0].ego_transform_to_previous = ego_gr[\"T\"][frame]\n",
    "                    fusedradars[0].v_lon, fusedradars[0].v_lat, fusedradars[0].yawrate = ego_gr[\"ego_scalars\"][frame]\n",
    "\n",
    "        # load lidar\n",
    "        lidars = dict()\n",
    "        lidar_data = cbor_handler.get_data().get(\"lidar\", None)\n",
    "        if \"LIDAR\" in data2load.keys() and lidar_data:\n",
    "            for lid_id, lid_name in dp_meta_data.aligned_sensor_names[\"LIDAR\"].items():\n",
    "                if dp_meta_data.aligned_fids[\"LIDAR\"][lid_id] is not None:\n",
    "                    if data_cache_relevant and data_cache[1].aligned_fids[\"LIDAR\"][lid_id] == dp_meta_data.aligned_fids[\"LIDAR\"][lid_id]:\n",
    "                        lidars[lid_id] = data_cache[0].LIDARS[lid_id]\n",
    "                    else:\n",
    "                        lid_gp = lidar_data[lid_name]\n",
    "                        # get current frame\n",
    "                        frame = dp_meta_data.aligned_fids[\"LIDAR\"][lid_id]\n",
    "                        # get data\n",
    "                        pc = lid_gp[\"data\"][frame]\n",
    "                        # reshape\n",
    "                        lidar_pc = np.zeros([9, pc.shape[1]], dtype=np.float64)\n",
    "                        lidar_pc[0:4, :] = pc[:4, :]\n",
    "                        # normalize\n",
    "                        lidar_pc[3, :] /= 255\n",
    "                        # set timestamp\n",
    "                        if lidar_pc.size > 0:\n",
    "                            lidar_pc[4, :] = (lid_gp[\"timestamps\"][frame] + (pc[4, :] - np.min(pc[4, :]))) * 1000\n",
    "                        # set file id\n",
    "                        lidar_pc[5, :] = frame\n",
    "                        # prepare label, instance_id, stationary_flag\n",
    "                        lidar_pc[6:9, :] = -1\n",
    "                        # create calib\n",
    "                        lidar_calib = LidarCalib()\n",
    "                        lidar_calib.T_V2L = lid_gp[\"calib\"][\"T_V2L\"]\n",
    "                        lidar_calib.T_L2V = lid_gp[\"calib\"][\"T_L2V\"]\n",
    "                        lidars[lid_id] = Lidar(\n",
    "                            pointcloud=lidar_pc,\n",
    "                            lidar_calib=lidar_calib,\n",
    "                            timestamp=dp_meta_data.aligned_timestamps[\"LIDAR\"][lid_id],\n",
    "                            frame_id=dp_meta_data.aligned_fids[\"LIDAR\"][lid_id],\n",
    "                            scan_id=dp_meta_data.aligned_sids[\"LIDAR\"][lid_id],\n",
    "                            sensor_name=dp_meta_data.aligned_sensor_names[\"LIDAR\"][lid_id],\n",
    "                        )\n",
    "\n",
    "                        # load egomotion\n",
    "                        ego_gr = lid_gp[\"ego_motion\"]\n",
    "                        lidars[lid_id].ego_transform = ego_gr[\"T0\"][frame]\n",
    "                        lidars[lid_id].ego_transform_to_previous = ego_gr[\"T\"][frame]\n",
    "                        lidars[lid_id].v_lon, lidars[lid_id].v_lat, lidars[lid_id].yawrate = ego_gr[\"ego_scalars\"][frame]\n",
    "\n",
    "        # load labels\n",
    "        labels = {\"GT\": []}\n",
    "        label_GT_src = None\n",
    "        # if 'LABEL' in data2load.keys():\n",
    "        #     if data_cache_relevant and data_cache[1].aligned_fids['LABEL'] == dp_meta_data.aligned_fids['LABEL']:\n",
    "        #         labels = data_cache[0].LABEL\n",
    "        #     else:\n",
    "        #         if dp_meta_data.aligned_fids['LABEL'] is not None:\n",
    "        #             labels_filename = dp_meta_data.aligned_fids['LABEL']['file']\n",
    "        #             if os.path.exists(labels_filename):\n",
    "        #                 label_file = h5py.File(labels_filename, 'r')\n",
    "        #                 # create h5 handles for all sensors to load\n",
    "        #                 if 'sensors' in label_file:\n",
    "        #                     # for Lidar and single radars we use sensorwise label file\n",
    "        #                     label_sensor_gp = label_file['sensors'][dp_meta_data.aligned_fids['LABEL']['sensor']]\n",
    "        #                 else:\n",
    "        #                     # for everything else use the FUSEDRADAR label group\n",
    "        #                     label_sensor_gp = label_file\n",
    "        #                 # get unique list combining ground truth source from dataspec and additional sources from data2load\n",
    "        #                 GT_src = dp_meta_data.aligned_fids['LABEL']['source']\n",
    "        #                 sources2load = list(set([GT_src] + data2load['LABEL']))\n",
    "        #                 # cycle sources\n",
    "        #                 for lab_src in sources2load:\n",
    "        #                     # skip label source None\n",
    "        #                     if lab_src is None:\n",
    "        #                         continue\n",
    "        #                     labels[lab_src] = []\n",
    "        #                     if lab_src.encode('utf-8') in label_sensor_gp['label_idx_to_name']:\n",
    "        #                         label_gp = label_sensor_gp['label_sources'][lab_src]\n",
    "        #                         frame_range = label_gp['frame_range']\n",
    "        #                         # capture trivial case were no labels are available at all\n",
    "        #                         if label_gp['data'].shape[0] == 0:\n",
    "        #                             continue\n",
    "        #                         data_UUID_mapping = [uuid.decode('utf-8') for uuid in label_gp['data_UUID_mapping']]\n",
    "        #                         # get frame\n",
    "        #                         frame = dp_meta_data.aligned_fids['LABEL']['frame']\n",
    "        #                         if frame_range[0] <= frame <= frame_range[1]:\n",
    "        #                             # slice\n",
    "        #                             slice = get_slice(label_gp, frame)\n",
    "        #                             data = label_gp['data'][slice[0]:slice[1], :]\n",
    "        #                             cols = label_gp['data'].attrs['columns'].split(';')\n",
    "        #                             lookup = dict(zip(cols, range(len(cols))))\n",
    "        #                             for i in range(data.shape[0]):\n",
    "        #                                 label = Label()\n",
    "        #                                 # class\n",
    "        #                                 if 'labels_mapping' in label_gp:\n",
    "        #                                     label.type = label_gp['labels_mapping'][data[i][lookup['Label']].astype(int)].decode('utf-8')\n",
    "        #                                 else:\n",
    "        #                                     label.type = label_gp['data_Label_mapping'][data[i][lookup['Label']].astype(int)].decode('utf-8')\n",
    "        #                                 # position\n",
    "        #                                 label.loc_3d[0] = data[i][lookup['XPos']]\n",
    "        #                                 label.loc_3d[1] = data[i][lookup['YPos']]\n",
    "        #                                 label.loc_3d[2] = data[i][lookup['ZPos']]\n",
    "        #                                 # dimension (length, width, height)\n",
    "        #                                 label.dim_3d[0] = data[i][lookup['XDim']]\n",
    "        #                                 label.dim_3d[1] = data[i][lookup['YDim']]\n",
    "        #                                 label.dim_3d[2] = data[i][lookup['ZDim']]\n",
    "        #                                 # rotation as 3d matrix vectorized along columns\n",
    "        #                                 # cx = math.cos(data['Roll'][i])\n",
    "        #                                 # sx = math.sin(data['Roll'][i])\n",
    "        #                                 # cy = math.cos(data['Pitch'][i])\n",
    "        #                                 # sy = math.sin(data['Pitch'][i])\n",
    "        #                                 yaw = data[i][lookup['Yaw']]\n",
    "        #                                 cz = math.cos(yaw)\n",
    "        #                                 sz = math.sin(yaw)\n",
    "        #                                 # label.rot_3d[0, 0] = cy * cz\n",
    "        #                                 # label.rot_3d[0, 1] = sx * sy * cz - cx * sz\n",
    "        #                                 # label.rot_3d[0, 2] = cx * sy * cz + sx * sz\n",
    "        #                                 # label.rot_3d[1, 0] = cy * sz\n",
    "        #                                 # label.rot_3d[1, 1] = sx * sy * sz + cx * cz\n",
    "        #                                 # label.rot_3d[1, 2] = cx * sy * sz - sx * cz\n",
    "        #                                 # label.rot_3d[2, 0] = -sy\n",
    "        #                                 # label.rot_3d[2, 1] = sx * cy\n",
    "        #                                 # label.rot_3d[2, 2] = cx * cy\n",
    "        #                                 label.rot_3d[0, 0] = cz\n",
    "        #                                 label.rot_3d[0, 1] = -sz\n",
    "        #                                 label.rot_3d[0, 2] = 0.0\n",
    "        #                                 label.rot_3d[1, 0] = sz\n",
    "        #                                 label.rot_3d[1, 1] = cz\n",
    "        #                                 label.rot_3d[1, 2] = 0.0\n",
    "        #                                 label.rot_3d[2, 0] = 0.0\n",
    "        #                                 label.rot_3d[2, 1] = 0.0\n",
    "        #                                 label.rot_3d[2, 2] = 1.0\n",
    "        #                                 # velocity and acceleration\n",
    "        #                                 label.vel_3d_filtered[0] = data[i][lookup['XVelFiltered']]\n",
    "        #                                 label.vel_3d_filtered[1] = data[i][lookup['YVelFiltered']]\n",
    "        #                                 label.vel_3d_filtered[2] = 0\n",
    "        #                                 label.acc_3d_filtered[0] = data[i][lookup['XAccFiltered']]\n",
    "        #                                 label.acc_3d_filtered[1] = data[i][lookup['YAccFiltered']]\n",
    "        #                                 label.acc_3d_filtered[2] = 0\n",
    "        #                                 if 'XVel' in lookup:\n",
    "        #                                     label.vel_3d[0] = data[i][lookup['XVel']]\n",
    "        #                                     label.vel_3d[1] = data[i][lookup['YVel']]\n",
    "        #                                     label.vel_3d[2] = data[i][lookup['ZVel']]\n",
    "        #                                 else:\n",
    "        #                                     label.vel_3d = label.vel_3d_filtered.copy()\n",
    "        #                                 if 'XAcc' in lookup:\n",
    "        #                                     label.acc_3d[0] = data[i][lookup['XAcc']]\n",
    "        #                                     label.acc_3d[1] = data[i][lookup['YAcc']]\n",
    "        #                                     label.acc_3d[2] = data[i][lookup['ZAcc']]\n",
    "        #                                 else:\n",
    "        #                                     label.acc_3d = label.acc_3d_filtered.copy()\n",
    "        #                                 # track id\n",
    "        #                                 label.track_id = data_UUID_mapping[int(data[i][lookup['UUID']])] #int(data[i][lookup['UUID']])\n",
    "        #                                 # no care flag\n",
    "        #                                 if 'NoCare' in lookup:\n",
    "        #                                     label.no_care = data[i][lookup['NoCare']]\n",
    "        #                                 else:\n",
    "        #                                     label.no_care = False\n",
    "        #                                 if 'geom_NoCare' in lookup:\n",
    "        #                                     label.geom_no_care = data[i][lookup['geom_NoCare']]\n",
    "        #                                 else:\n",
    "        #                                     label.geom_no_care = False\n",
    "        #                                 # visibility\n",
    "        #                                 vis_entries = [vv for vv in lookup if 'Visibility' in vv]\n",
    "        #                                 if len(vis_entries) > 0:\n",
    "        #                                     for ve in vis_entries:\n",
    "        #                                         label.visibility = {int(ve.split('_')[1]): data[i][lookup[ve]]}\n",
    "        #                                 # mark as annotation\n",
    "        #                                 label.breed = 'annotation'\n",
    "        #\n",
    "        #                                 labels[lab_src].append(label)\n",
    "        #                 # rename round truth source to 'GT'\n",
    "        #                 if GT_src in labels:\n",
    "        #                     label_GT_src = GT_src\n",
    "        #                     labels['GT'] = labels.pop(GT_src)\n",
    "        #                 # close file handle\n",
    "        #                 label_file.close()\n",
    "\n",
    "        # load cameras\n",
    "        cameras = dict()\n",
    "        camera_data = cbor_handler.get_data().get(\"camera\", None)\n",
    "        if \"CAMERA\" in data2load.keys() and camera_data:\n",
    "            for cam_id, cam_name in dp_meta_data.aligned_sensor_names[\"CAMERA\"].items():\n",
    "                if dp_meta_data.aligned_fids[\"CAMERA\"][cam_id] is not None:\n",
    "                    if data2load[\"CAMERA\"] == [] or cam_id in data2load[\"CAMERA\"]:\n",
    "                        if data_cache_relevant and data_cache[1].aligned_fids[\"CAMERA\"][cam_id] == dp_meta_data.aligned_fids[\"CAMERA\"][cam_id]:\n",
    "                            cameras[cam_id] = data_cache[0].CAMERAS[cam_id]\n",
    "                        else:\n",
    "                            # get handle\n",
    "                            cam_gp = camera_data[cam_name]\n",
    "                            # get current frame\n",
    "                            frame = dp_meta_data.aligned_fids[\"CAMERA\"][cam_id]\n",
    "                            # get image\n",
    "                            # img = cam_gp['data'][frame]\n",
    "                            img = cv2.imdecode(cam_gp[\"data\"][frame], cv2.IMREAD_COLOR)\n",
    "                            # if cam_gp['type'] == 'pandora-camera':\n",
    "                            #     img = img[:, :, [2, 1, 0]]\n",
    "                            # create camera structure\n",
    "                            cameracalib = CameraCalib(\n",
    "                                data={\n",
    "                                    \"img_width\": cam_gp[\"calib\"][\"width\"],\n",
    "                                    \"img_height\": cam_gp[\"calib\"][\"height\"],\n",
    "                                    \"T_V2C\": cam_gp[\"calib\"][\"T_V2C\"],\n",
    "                                    \"P\": cam_gp[\"calib\"][\"P\"],\n",
    "                                }\n",
    "                            )\n",
    "                            # create camera object\n",
    "                            cameras[cam_id] = Camera(\n",
    "                                image=img,\n",
    "                                camera_calib=cameracalib,\n",
    "                                timestamp=dp_meta_data.aligned_timestamps[\"CAMERA\"][cam_id],\n",
    "                                frame_id=dp_meta_data.aligned_fids[\"CAMERA\"][cam_id],\n",
    "                                scan_id=dp_meta_data.aligned_sids[\"CAMERA\"][cam_id],\n",
    "                                sensor_name=dp_meta_data.aligned_sensor_names[\"CAMERA\"][cam_id],\n",
    "                            )\n",
    "\n",
    "                            # load egomotion\n",
    "                            ego_gr = cam_gp[\"ego_motion\"]\n",
    "                            cameras[cam_id].ego_transform = ego_gr[\"T0\"][frame]\n",
    "                            cameras[cam_id].ego_transform_to_previous = ego_gr[\"T\"][frame]\n",
    "                            cameras[cam_id].v_lon, cameras[cam_id].v_lat, cameras[cam_id].yawrate = ego_gr[\"ego_scalars\"][frame]\n",
    "\n",
    "        # create result structure\n",
    "        ret = {\n",
    "            \"META\": {\n",
    "                \"sensor_type\": dp_meta_data.sensor_type,\n",
    "                \"sensor_id\": dp_meta_data.sensor_id,\n",
    "                \"sensor_name\": dp_meta_data.sensor_name,\n",
    "                \"sensor_frame_id\": dp_meta_data.aligned_fids[dp_meta_data.sensor_type][dp_meta_data.sensor_id],\n",
    "                \"folder\": dp_meta_data.folder,\n",
    "                \"sequence_name\": dp_meta_data.sequence_name,\n",
    "                \"timestamp\": dp_meta_data.aligned_timestamps[dp_meta_data.sensor_type][dp_meta_data.sensor_id],\n",
    "                \"GT_source\": label_GT_src,\n",
    "                \"data_info\": None,\n",
    "                \"vehicle_info\": None,\n",
    "            },\n",
    "            \"LIDARS\": lidars,\n",
    "            \"RADARS\": radars,\n",
    "            \"FUSEDRADARS\": fusedradars,\n",
    "            \"CAMERAS\": cameras,\n",
    "            \"LABEL\": labels,\n",
    "        }\n",
    "        return ret\n",
    "\n",
    "    def read_numpy_data_dict(self, data: dict):\n",
    "        \"\"\"\n",
    "        directly read data from data dict\n",
    "\n",
    "        Args:\n",
    "            data: data dict to read from\n",
    "        \"\"\"\n",
    "        # parse lidar\n",
    "        lidar = dict()\n",
    "        if \"LIDAR\" in data:\n",
    "            lidar_pc = data[\"LIDAR\"].astype(np.float32)\n",
    "            lidar[0] = Lidar(lidar_pc, LidarCalib())\n",
    "\n",
    "        # parse radar\n",
    "        radar = dict()\n",
    "        if \"RADAR\" in data:\n",
    "            radar_pc = data[\"RADAR\"].astype(np.float32)\n",
    "            radar[0] = Radar(radar_pc, RadarCalib())\n",
    "\n",
    "        # parse radar\n",
    "        fusedradars = dict()\n",
    "        if \"FUSEDRADARS\" in data:\n",
    "            radar_pc = data[\"FUSEDRADARS\"].astype(np.float32)\n",
    "            fusedradars[0] = Radar(radar_pc, RadarCalib())\n",
    "\n",
    "        # parse camera\n",
    "        camera = dict()\n",
    "        if \"CAMERAS\" in data:\n",
    "            camera_img = data[\"CAMERAS\"].astype(np.float32)\n",
    "            camera[0] = Camera(camera_img, CameraCalib())\n",
    "\n",
    "        # parse label\n",
    "        label = dict()\n",
    "        if \"LABEL\" in data:\n",
    "            label = data[\"LABEL\"]\n",
    "\n",
    "        self.LIDARS = lidar\n",
    "        self.RADARS = radar\n",
    "        self.FUSEDRADARS = fusedradars\n",
    "        self.CAMERAS = camera\n",
    "        self.LABEL = label\n",
    "\n",
    "    @staticmethod\n",
    "    def quat2rot(q: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "        assistance for quaternions to rotation matrix\n",
    "\n",
    "        Args:\n",
    "            q: quaternion\n",
    "\n",
    "        Returns:\n",
    "            rotation matrix (4x4)\n",
    "        \"\"\"\n",
    "        a, b, c, d = (q[0], q[1], q[2], q[3])\n",
    "        rotation = np.array(\n",
    "            [\n",
    "                [1 - 2 * (c**2 + d**2), 2 * (b * c - a * d), 2 * (b * d + a * c)],\n",
    "                [2 * (b * c + a * d), 1 - 2 * (b**2 + d**2), 2 * (c * d - a * b)],\n",
    "                [2 * (b * d - a * c), 2 * (c * d + a * b), 1 - 2 * (b**2 + c**2)],\n",
    "            ]\n",
    "        )\n",
    "        return rotation\n",
    "\n",
    "\n",
    "def get_slice(dat: h5py.File, frm: int) -> list:\n",
    "    \"\"\"\n",
    "    helper function to extract slice start and exclusive end\n",
    "\n",
    "    Args:\n",
    "        dat: h5py data handle\n",
    "        frm: frame to get data slice for\n",
    "\n",
    "    Returns:\n",
    "        [slice start, slice end]\n",
    "    \"\"\"\n",
    "\n",
    "    data_slices = dat[\"data_slices\"]\n",
    "    slice = [data_slices[frm, 0], 0]\n",
    "    if frm < data_slices.shape[0] - 1:\n",
    "        slice[1] = data_slices[frm + 1, 0]\n",
    "    else:\n",
    "        slice[1] = dat[\"data\"].shape[0]\n",
    "    return slice\n",
    "\n",
    "\n",
    "class Data_package:\n",
    "    \"\"\"\n",
    "    Assistance structure for ordering of unaligned data\n",
    "    \"\"\"\n",
    "\n",
    "    # order of sensors\n",
    "    sort_key_type_lookup = {\"FUSEDRADAR\": 0.01, \"RADAR\": 0.02, \"CAMERA\": 0.03, \"LIDAR\": 0.04}\n",
    "\n",
    "    def __init__(self, typ: str, sensor_id: int, sensor_name: str, time: int, data_idx: int, scan_id: int, subsensors: Optional[dict] = None):\n",
    "        \"\"\"\n",
    "        inits Data_package\n",
    "\n",
    "        Args:\n",
    "            typ:            sensor type\n",
    "            sensor_id:      sensor id\n",
    "            sensor_name:    sensor name\n",
    "            time:           acquisition time\n",
    "            data_idx:       data index\n",
    "            scan_id:        scan id\n",
    "            subsensors:     subsensor dictionary\n",
    "        \"\"\"\n",
    "        self.type = typ\n",
    "        self.sensor_id = sensor_id\n",
    "        self.sensor_name = sensor_name\n",
    "        self.time = time\n",
    "        self.data_idx = data_idx\n",
    "        self.scan_id = scan_id\n",
    "        self.subsensors = subsensors\n",
    "\n",
    "    def create_sort_key(self) -> float:\n",
    "        \"\"\"\n",
    "        create sort key, lidar after camera after radar after unknown if same time is given\n",
    "        Returns:\n",
    "            sort key\n",
    "        \"\"\"\n",
    "        return self.time + Data_package.sort_key_type_lookup.get(self.type, 0.0)\n",
    "\n",
    "\n",
    "class DataProviderGenerator(object):\n",
    "    \"\"\"\n",
    "    Generator module for data providers\n",
    "    \"\"\"\n",
    "\n",
    "    def __enter__(self) -> \"DataProviderGenerator\":\n",
    "        \"\"\"\n",
    "        implementation of __enter__ function\n",
    "\n",
    "        Returns:\n",
    "            self handle\n",
    "        \"\"\"\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type: Optional[Type[BaseException]], exc_val: Optional[BaseException], exc_tb: Optional[TracebackType]):\n",
    "        \"\"\"\n",
    "        implementation of __exit__ function\n",
    "\n",
    "        Args:\n",
    "            exc_type: indicates class of exception\n",
    "            exc_val: indicates type of exception\n",
    "            exc_tb: traceback is a report which has all of the information needed to solve the exception\n",
    "        \"\"\"\n",
    "        # remove datasets from cbor handlers\n",
    "        for dsets in self.datasets:\n",
    "            CBOR_Handler.release_handler(dsets[0], dsets[1])\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_spec: List[DataProviderDataSpec],\n",
    "        data2load: dict = {\"LIDAR\": [], \"RADAR\": [], \"FUSEDRADAR\": [], \"CAMERA\": [], \"LABEL\": []},\n",
    "        type2generate: Type = DataProvider,\n",
    "        cache_size: int = 0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        inits DataProviderGenerator\n",
    "\n",
    "        Args:\n",
    "            data_spec:      (List of) DataProviderDataSpec holding the data to load\n",
    "            data2load:      dict of sensortypes holding lists of sensor ids for which data should be loaded, 'LABEL' with layers (e.g. 'GT') is also part\n",
    "                            (empty list means all data is loaded, missing sensortype or None entry means no data is loaded)\n",
    "            type2generate:  Data package class that shall be generated\n",
    "            cache_size:     Storage in MB allowed for caching data\n",
    "        \"\"\"\n",
    "\n",
    "        def init(data_spec: List[DataProviderDataSpec]):\n",
    "            \"\"\"\n",
    "            nested init function\n",
    "\n",
    "            Args:\n",
    "                data_spec: (List of) DataProviderDataSpec holding the data to load\n",
    "            \"\"\"\n",
    "\n",
    "            # store datasets used by this generator\n",
    "            self.datasets = []\n",
    "\n",
    "            # check type2generate\n",
    "            if not issubclass(type2generate, DataProvider):\n",
    "                raise ValueError(\"type2generate needs to be or derive from data_provider.DataProvider in DataProviderGenerator.__init__\")\n",
    "            self.type2generate = type2generate\n",
    "\n",
    "            # store file packages selected for loading, spare sensor_type holding None, as data for this type shall not be loaded\n",
    "            self.data2load = {sensor_type: id_list for sensor_type, id_list in data2load.items() if id_list is not None}\n",
    "\n",
    "            # check input\n",
    "            input_valid = False\n",
    "            if type(data_spec) is DataProviderDataSpec:\n",
    "                # either we have a DataProviderDataSpec (convert to one element list in this case)\n",
    "                data_spec = [data_spec]\n",
    "                input_valid = True\n",
    "            elif type(data_spec) is list:\n",
    "                # or a list of DataProviderDataSpec\n",
    "                input_valid = True\n",
    "                for dsp in data_spec:\n",
    "                    if type(dsp) is not DataProviderDataSpec:\n",
    "                        input_valid = False\n",
    "                        break\n",
    "            if not input_valid:\n",
    "                raise ValueError(\n",
    "                    \"Input needs to be either data_provider.DataProviderDataSpec or list of  \"\n",
    "                    \"data_provider.DataProviderDataSpec in DataProviderGenerator.__init__\"\n",
    "                )\n",
    "\n",
    "            # store all source types in data_spec list\n",
    "            self.source_types = []\n",
    "            for dps in data_spec:\n",
    "                if dps.type not in self.source_types:\n",
    "                    self.source_types.append(dps.type)\n",
    "\n",
    "            # check for forbidden 'trainingTool2' + FUSEDRADAR[0] configuration\n",
    "            for st in self.source_types:\n",
    "                if st in [\"trainingTool2\"]:\n",
    "                    if \"FUSEDRADAR\" in self.data2load and 0 in self.data2load[\"FUSEDRADAR\"]:\n",
    "                        raise ValueError(\n",
    "                            \"FUSEDRADAR[0] is reserved for the artificial fused Radar sensor, thus its not allowed to load from\"\n",
    "                            \" a sensor with id 0. Either load list of sensors you want to fuse (e.g. [1,3,5]),\"\n",
    "                            \"or load all ([]), in DataProviderGenerator.__init__\"\n",
    "                        )\n",
    "\n",
    "            # check data to load is int or in case of 'trainingTool2' it might be string\n",
    "            for sensor_type in self.data2load.values():\n",
    "                for sensor_id in sensor_type:\n",
    "                    # in case only 'trainingTool2' is given, sensors might be given as strings or int\n",
    "                    if self.source_types == [\"trainingTool2\"]:\n",
    "                        if type(sensor_id) not in [int, str]:\n",
    "                            raise ValueError(\n",
    "                                'data2load sensor entries must be empty lists or lists of type int (sensor_id), (or in case of \"trainingTool2\" optionally of type str (sensor_name)), in DataProviderGenerator.__init__'\n",
    "                            )\n",
    "                    else:\n",
    "                        # only int is allowed otherwise\n",
    "                        if type(sensor_id) is not int:\n",
    "                            raise ValueError(\n",
    "                                'data2load sensor entries must be empty lists or lists of type int (sensor_id), (or in case of \"trainingTool2\" of type str (sensor_name), optionally), in DataProviderGenerator.__init__'\n",
    "                            )\n",
    "\n",
    "            # check for data and create lookups and meta structures\n",
    "            self.data_spec = data_spec\n",
    "            self.file_meta_data_list = None\n",
    "            self.sequence_lookup = None\n",
    "            self.triggerable = dict()\n",
    "            self.radar_versions = [\"v0\"]\n",
    "            self.current_radar_version_index = 0\n",
    "            self.create_file_meta_data()\n",
    "\n",
    "            # init skip list, this list is either None, or a boolean list of size of self.trigger2total, which tells generator to skip data_packages at\n",
    "            # at indices holding False, calling self.set_data2trigger will rest it to None to keep sizes consistant\n",
    "            self.skip_list = None\n",
    "\n",
    "            # initially trigger on everything\n",
    "            self.trigger2total = []\n",
    "            self.total2trigger = []\n",
    "            self.trigger_samples_per_sequence = dict()\n",
    "            self.first_trigger_sample_per_sequence = dict()\n",
    "            self.set_data2trigger(self.triggerable)\n",
    "\n",
    "            # init label converter empty or by files if specified in dataspecs\n",
    "            self.label_converter = ClassMapper()\n",
    "            for dps in data_spec:\n",
    "                if dps.mapping_file not in [None, \"\"]:\n",
    "                    self.label_converter.update(dps.mapping_file)\n",
    "\n",
    "            # init data cache\n",
    "            self.data_cache_size_cur = 0\n",
    "            self.data_cache = dict()\n",
    "            self.data_cache_queue = collections.deque()\n",
    "            if cache_size > 0:\n",
    "                self.enable_data_cache(cache_size)\n",
    "            else:\n",
    "                self.disable_data_cache()\n",
    "\n",
    "            # init data manipulator list\n",
    "            self.data_manipulators = []\n",
    "\n",
    "        init(data_spec)\n",
    "\n",
    "    def clear_data_cache(self):\n",
    "        \"\"\"\n",
    "        clear data cache\n",
    "        \"\"\"\n",
    "        self.data_cache_size_cur = 0\n",
    "        self.data_cache.clear()\n",
    "        self.data_cache_queue.clear()\n",
    "\n",
    "    def enable_data_cache(self, cache_size: int):\n",
    "        \"\"\"\n",
    "        enable data cache\n",
    "\n",
    "        Args:\n",
    "            cache_size: cache size in megabytes\n",
    "        \"\"\"\n",
    "        self.data_cache_size_max = cache_size * 1000000\n",
    "        self.clear_data_cache()\n",
    "\n",
    "    def disable_data_cache(self):\n",
    "        \"\"\"\n",
    "        disable data cache\n",
    "        \"\"\"\n",
    "        self.data_cache_size_max = 0\n",
    "        self.clear_data_cache()\n",
    "\n",
    "    def generate(self, sample_number: int) -> Optional[Type]:\n",
    "        \"\"\"\n",
    "        generate data_provider package for sample number\n",
    "\n",
    "        Returns:\n",
    "            data sample\n",
    "        \"\"\"\n",
    "        if sample_number >= 0 and sample_number < self.size_trigger():\n",
    "\n",
    "            # check if frame shall be skipped\n",
    "            if self.skip_list is not None and self.skip_list[sample_number] is False:\n",
    "                print(\"Frame %d is skipped due to skip_list\" % sample_number)\n",
    "                return \"SKIPPED\"\n",
    "\n",
    "            # get sample number it total list\n",
    "            total_sample_number = self.trigger2total[sample_number]\n",
    "\n",
    "            # caching\n",
    "            if self.data_cache_size_max > 0:\n",
    "                # with caching, query cache for sample\n",
    "                dp_cache_obj = self.data_cache.get(total_sample_number, None)\n",
    "                if dp_cache_obj is None:\n",
    "                    # add last sample to allow to copy reused data and spare disk loading\n",
    "                    if len(self.data_cache_queue) > 0:\n",
    "                        data_cache = self.data_cache[self.data_cache_queue[-1]]\n",
    "                    else:\n",
    "                        data_cache = None\n",
    "                    # create sample\n",
    "                    dp_cache = self.type2generate(\n",
    "                        dp_meta_data=self.file_meta_data_list[total_sample_number],\n",
    "                        tag=self.get_tag(sample_number),\n",
    "                        data2load=self.data2load,\n",
    "                        label_converter=self.label_converter,\n",
    "                        data_cache=data_cache,\n",
    "                        version2load={\"RADAR\": self.get_current_radar_version(), \"FUSEDRADAR\": self.get_current_radar_version()},\n",
    "                        data_manipulators=self.data_manipulators,\n",
    "                    )\n",
    "                    # insert into cache\n",
    "                    dp_size = dp_cache.get_byte_size()\n",
    "                    self.data_cache_size_cur += dp_size\n",
    "                    self.data_cache[total_sample_number] = (dp_cache, self.file_meta_data_list[total_sample_number], dp_size)\n",
    "                    self.data_cache_queue.append(total_sample_number)\n",
    "                    # do not exceed cache size\n",
    "                    while self.data_cache_size_cur > self.data_cache_size_max:\n",
    "                        self.data_cache_size_cur -= self.data_cache[self.data_cache_queue[0]][2]\n",
    "                        del self.data_cache[self.data_cache_queue[0]]\n",
    "                        self.data_cache_queue.popleft()\n",
    "                else:\n",
    "                    # get from cache\n",
    "                    dp_cache = dp_cache_obj[0]\n",
    "                    # call cache independent data manipulations\n",
    "                    dp_cache.update_data_manipulators_cache_independent(\n",
    "                        data_cache=dp_cache_obj,\n",
    "                        registered_data_manipulators=self.data_manipulators,\n",
    "                        data2load=self.data2load,\n",
    "                        version2load={\"RADAR\": self.get_current_radar_version(), \"FUSEDRADAR\": self.get_current_radar_version()},\n",
    "                        label_converter=self.label_converter,\n",
    "                        data_handlers=self.file_meta_data_list[total_sample_number].data_handlers,\n",
    "                    )\n",
    "\n",
    "                # copy data from cache to prevent user changes cache copy down the pipeline\n",
    "                dp = copy.deepcopy(dp_cache)\n",
    "            else:\n",
    "                # no caching, generate new sample\n",
    "                dp = self.type2generate(\n",
    "                    dp_meta_data=self.file_meta_data_list[total_sample_number],\n",
    "                    tag=self.get_tag(sample_number),\n",
    "                    data2load=self.data2load,\n",
    "                    label_converter=self.label_converter,\n",
    "                    version2load={\"RADAR\": self.get_current_radar_version(), \"FUSEDRADAR\": self.get_current_radar_version()},\n",
    "                    data_manipulators=self.data_manipulators,\n",
    "                )\n",
    "\n",
    "            # # print sample info\n",
    "            # ts = None\n",
    "            # if self.file_meta_data_list[total_sample_number].aligned_timestamps is not None:\n",
    "            #     ts = self.file_meta_data_list[total_sample_number].aligned_timestamps[self.file_meta_data_list[total_sample_number].sensor_type][self.file_meta_data_list[total_sample_number].sensor_id]\n",
    "            # print('GEN', total_sample_number, self.get_tag(sample_number), ts, '   CACHE_SIZE:', len(self.data_cache), self.data_cache_size_cur)\n",
    "\n",
    "            # return data provider\n",
    "            return dp\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def set_skiplist(self, skip_list: Optional[list]):\n",
    "        \"\"\"\n",
    "        set skiplist to tell generator to ignore certain frames\n",
    "\n",
    "        Args:\n",
    "            skip_list: boolean list of size trigger2total holding False for frame ids to skip, None to reset\n",
    "        \"\"\"\n",
    "        # check input\n",
    "        if skip_list is not None:\n",
    "            for val in skip_list:\n",
    "                if type(val) is not bool:\n",
    "                    raise ValueError(\"Input <skip_list> must be either None or list of bool in DataProviderGenerator.set_skiplist\")\n",
    "\n",
    "            # check length\n",
    "            if len(skip_list) != len(self.trigger2total):\n",
    "                raise ValueError(\"Length of list <skip_list> must match number of tigger frames in DataProviderGenerator.set_skiplist\")\n",
    "\n",
    "        # set skip_list\n",
    "        self.skip_list = skip_list\n",
    "\n",
    "    def set_data2trigger(self, data2trigger: Optional[Dict[str, List]] = None):\n",
    "        \"\"\"\n",
    "        set trigger list\n",
    "\n",
    "        Args:\n",
    "            data2trigger: if not None: dict of sensortypes holding lists of sensor ids for which data should be triggered\n",
    "                          (empty list means all sensor ids trigger)\n",
    "                          'trigger' means a data package is generated, 'loaded' means data is added to a package of the next triggered sensor package\n",
    "                          if None: set to all triggerable data\n",
    "        \"\"\"\n",
    "        # reset if input is None\n",
    "        if data2trigger is None:\n",
    "            self.set_data2trigger(self.triggerable)\n",
    "\n",
    "        # reset skiplist in case it was set, to prevent inconsistancy in frame counting\n",
    "        if self.skip_list is not None:\n",
    "            self.skip_list = None\n",
    "            print(\"WARNING: Skiplist reseted due to call of set_data2trigger!\")\n",
    "\n",
    "        # check sanity of data2load and data2trigger (no trigger which is not loaded)\n",
    "        for sensor_type, sensor_ids in data2trigger.items():\n",
    "            if sensor_type not in self.data2load:\n",
    "                raise ValueError(\"Each sensor type in data2trigger must be present in data2load DataProviderGenerator.set_data2trigger\")\n",
    "            if sensor_ids == [] and self.data2load[sensor_type] != []:\n",
    "                if sensor_type == \"FUSEDRADAR\" and \"trainingTool2\" in self.source_types and \"FUSEDRADAR\" in self.data2load:\n",
    "                    # in case radar is fused to a single sensor with id 0, we can trigger on fusedradar if its available in data2load independent on which senseors it was fused from\n",
    "                    pass\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        \"%s triggers on all sensor ids (data2trigger[%s]=[]), so all data must be loaded as well (data2load[%s]=[]) in data2load DataProviderGenerator.set_data2trigger\"\n",
    "                        % (sensor_type, sensor_type, sensor_type)\n",
    "                    )\n",
    "            for id in sensor_ids:\n",
    "                if self.data2load[sensor_type] != [] and id not in self.data2load[sensor_type]:\n",
    "                    if (\n",
    "                        sensor_type == \"FUSEDRADAR\"\n",
    "                        and (sensor_ids == [] or sensor_ids == [0])\n",
    "                        and \"trainingTool2\" in self.source_types\n",
    "                        and \"FUSEDRADAR\" in self.data2load\n",
    "                    ):\n",
    "                        # in case radar is fused to a single sensor with id 0, we can trigger on fusedradar if its available in data2load independent on which senseors it was fused from\n",
    "                        pass\n",
    "                    else:\n",
    "                        raise ValueError(\n",
    "                            \"%s triggers on sensor id %s, so id must be loaded as well (in data2load[%s]) in DataProviderGenerator.set_data2trigger\"\n",
    "                            % (sensor_type, id, sensor_type)\n",
    "                        )\n",
    "\n",
    "        # check against triggerable\n",
    "        self.data2trigger = dict()\n",
    "        for key, dat in data2trigger.items():\n",
    "            # check sensor\n",
    "            if key not in self.triggerable:\n",
    "                print(\"WARNING: Sensor %s is not triggerable in this dataset\" % key)\n",
    "            else:\n",
    "                # add sensor\n",
    "                if key not in self.data2trigger:\n",
    "                    self.data2trigger[key] = []\n",
    "                # add all sensor ids\n",
    "                if dat == []:\n",
    "                    self.data2trigger[key] = self.triggerable[key]\n",
    "                else:\n",
    "                    # add single sensor ids\n",
    "                    for id in dat:\n",
    "                        if id not in self.triggerable[key]:\n",
    "                            print(\"WARNING: Sensor %s id %d is not triggerable in this dataset\" % (key, id))\n",
    "                        else:\n",
    "                            self.data2trigger[key].append(id)\n",
    "\n",
    "        # change data lookup\n",
    "        self.trigger2total = []\n",
    "        self.total2trigger = [None] * self.size_total()\n",
    "        self.trigger_samples_per_sequence = dict()\n",
    "        self.first_trigger_sample_per_sequence = dict()\n",
    "        trigger_cnt = 0\n",
    "        for i in range(len(self.file_meta_data_list)):\n",
    "            if (\n",
    "                self.file_meta_data_list[i].sensor_type in self.data2trigger\n",
    "                and self.file_meta_data_list[i].sensor_id in self.data2trigger[self.file_meta_data_list[i].sensor_type]\n",
    "            ):\n",
    "                # fill lookup lists\n",
    "                self.trigger2total.append(i)\n",
    "                self.total2trigger[i] = trigger_cnt\n",
    "                # increase counters\n",
    "                if self.file_meta_data_list[i].sequence_name not in self.trigger_samples_per_sequence:\n",
    "                    self.trigger_samples_per_sequence[self.file_meta_data_list[i].sequence_name] = 0\n",
    "                    self.first_trigger_sample_per_sequence[self.file_meta_data_list[i].sequence_name] = trigger_cnt\n",
    "                self.trigger_samples_per_sequence[self.file_meta_data_list[i].sequence_name] += 1\n",
    "                trigger_cnt += 1\n",
    "\n",
    "    def get_trigger_sample_number(self, sequence: Optional[str]) -> int:\n",
    "        \"\"\"\n",
    "        get number of trigger samples for given sequence (get total number if None is entered)\n",
    "\n",
    "        Args:\n",
    "            sequence: sequence to check, if None: add up samples of all sequences\n",
    "\n",
    "        Returns:\n",
    "            number of trigger samples\n",
    "        \"\"\"\n",
    "        if sequence is None:\n",
    "            cnt = 0\n",
    "            for tsps in self.trigger_samples_per_sequence.value():\n",
    "                cnt += tsps\n",
    "            return cnt\n",
    "        else:\n",
    "            return self.trigger_samples_per_sequence.get(sequence, 0)\n",
    "\n",
    "    def get_first_trigger_sample_id(self, sequence: str) -> Optional[int]:\n",
    "        \"\"\"\n",
    "        get first trigger sample id of a sequence\n",
    "\n",
    "        Args:\n",
    "            sequence: sequence name\n",
    "\n",
    "        Returns:\n",
    "            first trigger sample if exists, None otherwise\n",
    "        \"\"\"\n",
    "        return self.first_trigger_sample_per_sequence.get(sequence, None)\n",
    "\n",
    "    def get_id_total_from_trigger(self, idx: int) -> Optional[int]:\n",
    "        \"\"\"\n",
    "        get total package list id from current trigger list id, return None when out of bounds\n",
    "\n",
    "        Args:\n",
    "            idx: trigger id\n",
    "\n",
    "        Returns:\n",
    "            aligned total package list id if exists, None otherwise\n",
    "        \"\"\"\n",
    "        if idx < 0 or idx >= self.size_trigger():\n",
    "            return None\n",
    "        else:\n",
    "            return self.trigger2total[idx]\n",
    "\n",
    "    def get_id_trigger_from_total_next(self, idx: int) -> Optional[int]:\n",
    "        \"\"\"\n",
    "        get next valid trigger package list id from current total list id, return None when out of bounds\n",
    "\n",
    "        Args:\n",
    "            idx: total list id\n",
    "\n",
    "        Returns:\n",
    "            next valid trigger id if exists, None otherwise\n",
    "        \"\"\"\n",
    "        if idx < 0 or idx >= self.size_total():\n",
    "            return None\n",
    "        while self.total2trigger[idx] is None:\n",
    "            idx += 1\n",
    "            if idx < 0 or idx >= self.size_total():\n",
    "                return None\n",
    "        return self.total2trigger[idx]\n",
    "\n",
    "    def get_id_trigger_from_total_previous(self, idx: int) -> Optional[int]:\n",
    "        \"\"\"\n",
    "        get previous valid trigger package list id from current total list id, return None when out of bounds\n",
    "\n",
    "        Args:\n",
    "            idx: total list id\n",
    "\n",
    "        Returns:\n",
    "            previous valid trigger id if exists, None otherwise\n",
    "        \"\"\"\n",
    "        if idx < 0 or idx >= self.size_total():\n",
    "            return None\n",
    "        while self.total2trigger[idx] is None:\n",
    "            idx -= 1\n",
    "            if idx < 0 or idx >= self.size_total():\n",
    "                return None\n",
    "        return self.total2trigger[idx]\n",
    "\n",
    "    def get_first_trigger_id_of_next_sequence(self, query_id: int) -> Optional[int]:\n",
    "        \"\"\"\n",
    "        get first trigger id of the next sequence in loaded data list\n",
    "\n",
    "        Args:\n",
    "            query_id: query id\n",
    "\n",
    "        Returns:\n",
    "            first trigger id of the next sequence if exists, None otherwise\n",
    "        \"\"\"\n",
    "        # check input validity\n",
    "        if query_id < 0 or query_id >= self.size_trigger() - 1:\n",
    "            return None\n",
    "        # search for id of sample from other sequence\n",
    "        cur_seq = self.file_meta_data_list[self.trigger2total[query_id]].sequence_number\n",
    "        for id in range(query_id + 1, self.size_trigger()):\n",
    "            total_id = self.trigger2total[id]\n",
    "            if self.file_meta_data_list[total_id].sequence_number != cur_seq:\n",
    "                return id\n",
    "        # did not find further sequences\n",
    "        return None\n",
    "\n",
    "    def get_first_trigger_id_of_previous_sequence(self, query_id: int) -> Optional[int]:\n",
    "        \"\"\"\n",
    "        get first trigger id of the previous sequence in loaded data list\n",
    "\n",
    "        Args:\n",
    "            query_id: query id\n",
    "\n",
    "        Returns:\n",
    "            irst trigger id of the previous sequence if exists, None otherwise\n",
    "        \"\"\"\n",
    "        # check input validity\n",
    "        if query_id < 1 or query_id >= self.size_trigger():\n",
    "            return None\n",
    "        # search for id of sample from other sequence\n",
    "        cur_seq = self.file_meta_data_list[self.trigger2total[query_id]].sequence_number\n",
    "        prev_seq = None\n",
    "        for id in range(query_id, -1, -1):\n",
    "            total_id = self.trigger2total[id]\n",
    "            if self.file_meta_data_list[total_id].sequence_number != cur_seq:\n",
    "                prev_seq = self.file_meta_data_list[total_id].sequence_number\n",
    "                break\n",
    "        # search for first trigger id with found sequence\n",
    "        if prev_seq is not None:\n",
    "            for id in self.sequence_lookup[prev_seq]:\n",
    "                if self.total2trigger[id] is not None:\n",
    "                    return self.total2trigger[id]\n",
    "        # did not find further sequences\n",
    "        return None\n",
    "\n",
    "    def get_triggerable(self) -> Dict[str, List]:\n",
    "        \"\"\"\n",
    "        get dict of triggerable sensors\n",
    "\n",
    "        Returns:\n",
    "            dict of triggerable sensors {sensor_type: list of sensor_ids}\n",
    "        \"\"\"\n",
    "        return copy.deepcopy(self.triggerable)\n",
    "\n",
    "    def get_current_triggers(self) -> Dict[str, List]:\n",
    "        \"\"\"\n",
    "        get current trigger sensors\n",
    "\n",
    "        Returns:\n",
    "            current trigger sensors {sensor_type: list of sensor_ids}\n",
    "        \"\"\"\n",
    "        return copy.deepcopy(self.data2trigger)\n",
    "\n",
    "    def add_static_class_mapping(self, src: str, dst: str, no_care: bool = False):\n",
    "        \"\"\"\n",
    "        add a static class mapping rule\n",
    "\n",
    "        Args:\n",
    "            src:        source class\n",
    "            dst:        target class\n",
    "            no_care:    no_care flag for target class\n",
    "        \"\"\"\n",
    "        self.label_converter.add_static_mapping(src=src, dst=dst, no_care=no_care)\n",
    "\n",
    "    def add_dynamic_class_mapping(\n",
    "        self, src: str, speed_threshold, dst_moving: str, dst_stationary, no_care_moving: bool = False, no_care_stationary: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        add a dynamic class mapping rule\n",
    "\n",
    "        Args:\n",
    "            src:                source class\n",
    "            speed_threshold:    threshold in m/s for absolute speed to decide between moving or stationary class mapping\n",
    "            dst_moving:         target class for moving\n",
    "            dst_stationary:     target class for stationary\n",
    "            no_care_moving:     no_care flag for moving target class\n",
    "            no_care_stationary: no_care flag for stationary target class\n",
    "        \"\"\"\n",
    "        self.label_converter.add_dynamic_mapping(\n",
    "            src=src,\n",
    "            speed_threshold=speed_threshold,\n",
    "            dst_moving=dst_moving,\n",
    "            dst_stationary=dst_stationary,\n",
    "            no_care_moving=no_care_moving,\n",
    "            no_care_stationary=no_care_stationary,\n",
    "        )\n",
    "\n",
    "    def set_label_converter(self, lab_conv: Dict[str, str]):\n",
    "        \"\"\"\n",
    "        clear and set a label converter (add dict to static label mapping)\n",
    "\n",
    "        Args:\n",
    "            lab_conv: dict holding the mappings to add\n",
    "        \"\"\"\n",
    "        self.label_converter.reset()\n",
    "        for src, dst in lab_conv.items():\n",
    "            self.label_converter.add_static_mapping(src, dst)\n",
    "\n",
    "    def get_label_converter(self) -> ClassMapper:\n",
    "        \"\"\"\n",
    "        get a copy of the label converter\n",
    "\n",
    "        Returns:\n",
    "            class mapper handle\n",
    "        \"\"\"\n",
    "        return copy.deepcopy(self.label_converter)\n",
    "\n",
    "    def get_full_sample_tag_data(self, sample_number: int) -> Optional[DataProviderMetaData]:\n",
    "        \"\"\"\n",
    "        get meta data list entry\n",
    "\n",
    "        Args:\n",
    "            sample_number: query sample number\n",
    "\n",
    "        Returns:\n",
    "            file meta data if exists, None otherwise\n",
    "        \"\"\"\n",
    "        if sample_number >= 0 and sample_number < self.size_trigger():\n",
    "            total_sample_number = self.trigger2total[sample_number]\n",
    "            return self.file_meta_data_list[total_sample_number]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_tag(self, sample_number: int) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        get sample tag (identifier created from sequence name and file id)\n",
    "\n",
    "        Args:\n",
    "            sample_number: query sample number\n",
    "\n",
    "        Returns:\n",
    "            sample tag if exists, None otherwise\n",
    "        \"\"\"\n",
    "        if sample_number >= 0 and sample_number < self.size_trigger():\n",
    "            total_sample_number = self.trigger2total[sample_number]\n",
    "            fid = (\n",
    "                self.file_meta_data_list[total_sample_number].sequence_name\n",
    "                + \"_\"\n",
    "                + self.file_meta_data_list[total_sample_number].sensor_type\n",
    "                + \"_\"\n",
    "                + str(self.file_meta_data_list[total_sample_number].sensor_id)\n",
    "                + \"_\"\n",
    "            )\n",
    "            if type(self.file_meta_data_list[total_sample_number].aligned_fids) is int:\n",
    "                fid += str(self.file_meta_data_list[total_sample_number].aligned_fids)\n",
    "            else:\n",
    "                fid += str(\n",
    "                    self.file_meta_data_list[total_sample_number].aligned_fids[self.file_meta_data_list[total_sample_number].sensor_type][\n",
    "                        self.file_meta_data_list[total_sample_number].sensor_id\n",
    "                    ]\n",
    "                )\n",
    "            return fid\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def size_total(self) -> int:\n",
    "        \"\"\"\n",
    "        get total number of data packages\n",
    "\n",
    "        Returns:\n",
    "            total number of sample packages\n",
    "        \"\"\"\n",
    "        return len(self.file_meta_data_list)\n",
    "\n",
    "    def size_trigger(self) -> int:\n",
    "        \"\"\"\n",
    "        get current number of triggerable datapackages\n",
    "\n",
    "        Returns:\n",
    "            current number of triggerable datapackages\n",
    "        \"\"\"\n",
    "        return len(self.trigger2total)\n",
    "\n",
    "    def get_first_trigger_number_after_time(self, log_name: str, timestamp: float) -> Optional[int]:\n",
    "        \"\"\"\n",
    "        get trigger number next after timestamp for sequence\n",
    "\n",
    "        Args:\n",
    "            log_name:   sequence name\n",
    "            timestamp:  query timestamp\n",
    "\n",
    "        Returns:\n",
    "            first trigger sample after requested timestamp if exists, None otherwise\n",
    "        \"\"\"\n",
    "        # get first trigger data package equal or above querytimestamp\n",
    "        for i in range(len(self.trigger2total)):\n",
    "            meta_data = self.file_meta_data_list[self.trigger2total[i]]\n",
    "            if meta_data.sequence_name == log_name:\n",
    "                for trigger_sensor, trigger_ids in self.data2trigger.items():\n",
    "                    sensor_times = meta_data.aligned_timestamps[trigger_sensor]\n",
    "                    for sensor_id, time in sensor_times.items():\n",
    "                        if trigger_ids == [] or sensor_id in trigger_ids:\n",
    "                            if timestamp <= time:\n",
    "                                return i\n",
    "        # if none was found return None\n",
    "        return None\n",
    "\n",
    "    def create_file_meta_data(self):\n",
    "        \"\"\"\n",
    "        get list of file meta data located in specified database\n",
    "        \"\"\"\n",
    "        # cycle over list of inputs\n",
    "        self.file_meta_data_list = []\n",
    "        sequence_count = 0\n",
    "        for log_spec in [ls for ds in self.data_spec for ls in ds.get_log_specs()]:\n",
    "            if log_spec[\"log_type\"] in [\"trainingTool2\"]:\n",
    "                # deal with hdf5 data\n",
    "                sequence_count = self.create_meta_data_from_hdf5(\n",
    "                    log_spec[\"log_type\"], log_spec[\"log_folder\"], log_spec[\"log_name\"], log_spec[\"label_source\"], sequence_count\n",
    "                )\n",
    "            elif log_spec[\"log_type\"] in [\"cbor\"]:\n",
    "                # deal with cbor data\n",
    "                sequence_count = self.create_meta_data_from_cbor(\n",
    "                    log_spec[\"log_type\"], log_spec[\"log_folder\"], log_spec[\"log_name\"], log_spec[\"label_source\"], sequence_count\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(\"Unknown source type %s in DataProviderGenerator.create_file_meta_data\" % log_spec[\"log_type\"])\n",
    "        # add None to radar versions\n",
    "        self.radar_versions.append(None)\n",
    "\n",
    "        # check if something was found and raise error\n",
    "        if len(self.file_meta_data_list) == 0:\n",
    "            self.file_meta_data_list = None\n",
    "        if self.file_meta_data_list is None:\n",
    "            raise ValueError(\"No valid files found for specified source type and folder\")\n",
    "\n",
    "        # create subfolder lookup, fill framenumbers and identify triggerable sensors\n",
    "        self.sequence_lookup = dict()\n",
    "        last_seq = -1\n",
    "        for i in range(len(self.file_meta_data_list)):\n",
    "            meta = self.file_meta_data_list[i]\n",
    "            # if sequence changes, add new entry\n",
    "            if meta.sequence_number != last_seq:\n",
    "                last_seq = meta.sequence_number\n",
    "                self.sequence_lookup[meta.sequence_number] = [i]\n",
    "                sequence_counter = 0\n",
    "            else:\n",
    "                self.sequence_lookup[meta.sequence_number].append(i)\n",
    "\n",
    "            # replace file counter in meta data list\n",
    "            self.file_meta_data_list[i] = self.file_meta_data_list[i]._replace(framenumber=sequence_counter)\n",
    "            sequence_counter += 1\n",
    "\n",
    "            # check triggerable and add if not yet done\n",
    "            if self.file_meta_data_list[i].sensor_type not in self.triggerable:\n",
    "                self.triggerable[self.file_meta_data_list[i].sensor_type] = []\n",
    "            if self.file_meta_data_list[i].sensor_id not in self.triggerable[self.file_meta_data_list[i].sensor_type]:\n",
    "                self.triggerable[self.file_meta_data_list[i].sensor_type].append(self.file_meta_data_list[i].sensor_id)\n",
    "\n",
    "    @staticmethod\n",
    "    def natural_sort(l: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        natural sort\n",
    "\n",
    "        Args:\n",
    "            l: list to sort\n",
    "\n",
    "        Returns:\n",
    "            sorted list\n",
    "        \"\"\"\n",
    "        convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "        alphanum_key = lambda key: [convert(c) for c in re.split(\"([0-9]+)\", key)]\n",
    "        return sorted(l, key=alphanum_key)\n",
    "\n",
    "    \"\"\"\n",
    "    create meta data for hdf5 trainingtool2 data\n",
    "    \"\"\"\n",
    "\n",
    "    def create_meta_data_from_hdf5(self, styp: str, sloc: str, snam: str, lsrc: str, sequence_count: int) -> int:\n",
    "        \"\"\"\n",
    "        create meta data for hdf5 trainingtool2 data\n",
    "\n",
    "        Args:\n",
    "            styp:           sequence data type (e.g.: 'trainingTool2')\n",
    "            sloc:           data folder\n",
    "            snam:           sequence name\n",
    "            lsrc:           GT label source\n",
    "            sequence_count: current number of sequences\n",
    "\n",
    "        Returns:\n",
    "            new number of sequences\n",
    "        \"\"\"\n",
    "        # load sensor mappings\n",
    "        lidar_file_name = sloc + \"/Lidar/\" + snam + \".h5\"\n",
    "        meta_file_name = sloc + \"/Meta/\" + snam + \".h5\"\n",
    "        radar_detections_file_name = sloc + \"/Radar_Detections/\" + snam + \".h5\"\n",
    "        camera_file_name = sloc + \"/Camera/\" + snam + \".h5\"\n",
    "        # cam_time_files = [sloc + '/Camera%dFrameTime__v0/' % c + sn + '/frame_time.csv' for c in range(5)]\n",
    "\n",
    "        # create data package list\n",
    "        data_packages = []\n",
    "        # use offset corrections to put lidar timestamps to front pass to minimize difference between lidar and radar (and align camera)\n",
    "        offset_correction_lidar = 0  # 50\n",
    "        # parse cameras\n",
    "        if \"CAMERA\" in self.data2load:\n",
    "            if os.path.exists(camera_file_name):\n",
    "                camera_file = h5py.File(camera_file_name, \"r\")\n",
    "                # resolve sensor name entries in data2load\n",
    "                self.resolve_str_in_data2load(camera_file[\"camera_idx_to_name\"], \"CAMERA\")\n",
    "                # get data\n",
    "                for cam_id, cam_name in enumerate(camera_file[\"camera_idx_to_name\"]):\n",
    "                    if self.data2load[\"CAMERA\"] == [] or cam_id in self.data2load[\"CAMERA\"]:\n",
    "                        cam_gp = camera_file[\"sensors\"][cam_name]\n",
    "                        time = cam_gp[\"timestamps\"][:, 0] * 1000\n",
    "                        cam_name = cam_name.decode(\"UTF-8\")\n",
    "                        data_packages += [\n",
    "                            Data_package(typ=\"CAMERA\", sensor_id=cam_id, sensor_name=cam_name, time=time[i], data_idx=i, scan_id=None)\n",
    "                            for i in range(time.shape[0])\n",
    "                        ]\n",
    "                # close file handles\n",
    "                camera_file.close()\n",
    "            else:\n",
    "                print(\"\\033[93mWARNING: No camera file found for log %s!\\033[0m\" % snam)\n",
    "\n",
    "        # parse lidar\n",
    "        if \"LIDAR\" in self.data2load:\n",
    "            lidar_file = h5py.File(lidar_file_name, \"r\")\n",
    "            # resolve sensor name entries in data2load\n",
    "            self.resolve_str_in_data2load(lidar_file[\"lidar_idx_to_name\"], \"LIDAR\")\n",
    "            # get data\n",
    "            for lid_id, lid_name in enumerate(lidar_file[\"lidar_idx_to_name\"]):\n",
    "                if self.data2load[\"LIDAR\"] == [] or lid_id in self.data2load[\"LIDAR\"]:\n",
    "                    lid_gp = lidar_file[\"sensors\"][lid_name]\n",
    "                    lid_name = lid_name.decode(\"UTF-8\")\n",
    "                    time = lid_gp[\"timestamps\"][:, 0] * 1000 + offset_correction_lidar\n",
    "                    data_packages += [\n",
    "                        Data_package(typ=\"LIDAR\", sensor_id=lid_id, sensor_name=lid_name, time=time[i], data_idx=i, scan_id=None) for i in range(time.shape[0])\n",
    "                    ]\n",
    "            # close file handles\n",
    "            lidar_file.close()\n",
    "\n",
    "        # parse fused radar\n",
    "        if \"FUSEDRADAR\" in self.data2load:\n",
    "            meta_file = h5py.File(meta_file_name, \"r\")\n",
    "            if os.path.exists(radar_detections_file_name):\n",
    "                radar_file = h5py.File(radar_detections_file_name, \"r\")\n",
    "                batch2radar_map = radar_file[\"batch_to_radar_frame_map\"]\n",
    "            else:\n",
    "                radar_file = None\n",
    "                batch2radar_map = None\n",
    "            # resolve sensor name entries in data2load\n",
    "            self.resolve_str_in_data2load(meta_file[\"radar_idx_to_name\"], \"FUSEDRADAR\")\n",
    "            # load meta data\n",
    "            time = meta_file[\"timestamps\"][:, 0] * 1000\n",
    "            scan_id = meta_file[\"scan_id\"][:, 0]\n",
    "            # get data\n",
    "            for i in range(meta_file[\"timestamps\"].shape[0]):\n",
    "                # add subsensor information\n",
    "                sub_sensor_packages = dict()\n",
    "                for sd, rad_name in enumerate(meta_file[\"radar_idx_to_name\"]):\n",
    "                    if self.data2load[\"FUSEDRADAR\"] == [] or sd in self.data2load[\"FUSEDRADAR\"]:\n",
    "                        subsensor_scan_id = meta_file[\"scan_id\"][i, sd]\n",
    "                        if subsensor_scan_id >= 0:\n",
    "                            radar_frm = batch2radar_map[i, sd] if radar_file else -1\n",
    "                            sub_sensor_packages[sd] = {\n",
    "                                \"sensor_name\": rad_name.decode(\"UTF-8\"),\n",
    "                                \"timestamp\": meta_file[\"radar_timestamps\"][i, sd] * 1000,\n",
    "                                \"frame_id\": radar_frm,\n",
    "                                \"scan_id\": subsensor_scan_id,\n",
    "                            }\n",
    "                        else:\n",
    "                            sub_sensor_packages[sd] = None\n",
    "\n",
    "                # add data package\n",
    "                data_packages.append(\n",
    "                    Data_package(\n",
    "                        typ=\"FUSEDRADAR\", sensor_id=0, sensor_name=\"Fused\", time=time[i], data_idx=i, scan_id=scan_id[i], subsensors=sub_sensor_packages\n",
    "                    )\n",
    "                )\n",
    "            # close file handles\n",
    "            if radar_file:\n",
    "                radar_file.close()\n",
    "            meta_file.close()\n",
    "        # parse radar\n",
    "        if \"RADAR\" in self.data2load:\n",
    "            radar_file = h5py.File(radar_detections_file_name, \"r\")\n",
    "            # resolve sensor name entries in data2load\n",
    "            self.resolve_str_in_data2load(radar_file[\"radar_idx_to_name\"], \"RADAR\")\n",
    "            # get data\n",
    "            for rad_id, rad_name in enumerate(radar_file[\"radar_idx_to_name\"]):\n",
    "                if self.data2load[\"RADAR\"] == [] or rad_id in self.data2load[\"RADAR\"]:\n",
    "                    rad_gp = radar_file[\"sensors\"][rad_name]\n",
    "                    name = rad_name.decode(\"UTF-8\")\n",
    "                    time = rad_gp[\"timestamps\"][:, 0] * 1000\n",
    "                    scan_id = rad_gp[\"scan_id\"][:, 0]\n",
    "                    for i in range(rad_gp[\"timestamps\"].shape[0]):\n",
    "                        data_packages.append(Data_package(typ=\"RADAR\", sensor_id=rad_id, sensor_name=name, time=time[i], data_idx=i, scan_id=scan_id[i]))\n",
    "            # close file handles\n",
    "            radar_file.close()\n",
    "\n",
    "        # get sensor map\n",
    "        sensor_map = {\"LABEL\": None}\n",
    "        timestamp_map = {}\n",
    "        subsensor_map = {}\n",
    "        scanid_map = {}\n",
    "        sensor_name_map = {}\n",
    "        for dp in data_packages:\n",
    "            if dp.type not in sensor_map:\n",
    "                sensor_map[dp.type] = {}\n",
    "                timestamp_map[dp.type] = {}\n",
    "                subsensor_map[dp.type] = {}\n",
    "                scanid_map[dp.type] = {}\n",
    "                sensor_name_map[dp.type] = {}\n",
    "            if dp.sensor_id not in sensor_map[dp.type]:\n",
    "                sensor_map[dp.type][dp.sensor_id] = None\n",
    "                timestamp_map[dp.type][dp.sensor_id] = None\n",
    "                subsensor_map[dp.type][dp.sensor_id] = None\n",
    "                scanid_map[dp.type][dp.sensor_id] = None\n",
    "                sensor_name_map[dp.type][dp.sensor_id] = None\n",
    "\n",
    "        # sort by timestamps\n",
    "        data_packages.sort(key=lambda x: x.create_sort_key())\n",
    "\n",
    "        # feed data packages to meta data list and fill aligned data\n",
    "        framenumber = 0\n",
    "        last_label_location = None\n",
    "        for dp in data_packages:\n",
    "            # update maps\n",
    "            timestamp_map[dp.type][dp.sensor_id] = dp.time\n",
    "            sensor_map[dp.type][dp.sensor_id] = dp.data_idx\n",
    "            subsensor_map[dp.type][dp.sensor_id] = dp.subsensors\n",
    "            scanid_map[dp.type][dp.sensor_id] = dp.scan_id\n",
    "            sensor_name_map[dp.type][dp.sensor_id] = dp.sensor_name\n",
    "            # create respective labelfile name\n",
    "            if \"LABEL\" in self.data2load:\n",
    "                if dp.type == \"FUSEDRADAR\":\n",
    "                    sensor_map[\"LABEL\"] = {\"file\": sloc + \"/Labels/\" + snam + \".h5\", \"frame\": dp.data_idx, \"source\": lsrc, \"sensor\": dp.sensor_name}\n",
    "                    last_label_location = sensor_map[\"LABEL\"]\n",
    "                elif dp.type == \"LIDAR\":\n",
    "                    # sensor_map['LABEL'] = last_label_location\n",
    "                    sensor_map[\"LABEL\"] = {\"file\": sloc + \"/Labels_Sensorwise/\" + snam + \".h5\", \"frame\": dp.data_idx, \"source\": lsrc, \"sensor\": dp.sensor_name}\n",
    "                    last_label_location = sensor_map[\"LABEL\"]\n",
    "                elif dp.type == \"CAMERA\":\n",
    "                    sensor_map[\"LABEL\"] = last_label_location\n",
    "                elif dp.type == \"RADAR\":\n",
    "                    sensor_map[\"LABEL\"] = {\"file\": sloc + \"/Labels_Sensorwise/\" + snam + \".h5\", \"frame\": dp.data_idx, \"source\": lsrc, \"sensor\": dp.sensor_name}\n",
    "                    last_label_location = sensor_map[\"LABEL\"]\n",
    "\n",
    "            # create meta data entries, field framenumber will be replaced afterwards\n",
    "            self.file_meta_data_list.append(\n",
    "                DataProviderMetaData(\n",
    "                    sensor_type=dp.type,\n",
    "                    sensor_id=dp.sensor_id,\n",
    "                    sensor_name=dp.sensor_name,\n",
    "                    source_type=styp,\n",
    "                    folder=sloc,\n",
    "                    sequence_name=snam,\n",
    "                    sequence_number=sequence_count,\n",
    "                    aligned_sensor_names=copy.deepcopy(sensor_name_map),\n",
    "                    aligned_fids=copy.deepcopy(sensor_map),\n",
    "                    aligned_timestamps=copy.deepcopy(timestamp_map),\n",
    "                    aligned_sids=copy.deepcopy(scanid_map),\n",
    "                    aligned_subsensors=copy.deepcopy(subsensor_map),\n",
    "                    framenumber=framenumber,\n",
    "                    data_handlers=None,\n",
    "                    flags=dict(),\n",
    "                )\n",
    "            )\n",
    "            # increase frame counter\n",
    "            framenumber += 1\n",
    "\n",
    "        # increment sequence counter\n",
    "        sequence_count += 1\n",
    "\n",
    "        # report current sequence counter\n",
    "        return sequence_count\n",
    "\n",
    "    def create_meta_data_from_cbor(self, styp: str, sloc: str, snam: str, lsrc: str, sequence_count: int) -> int:\n",
    "        \"\"\"\n",
    "        create meta data for cbor data\n",
    "\n",
    "        Args:\n",
    "            styp:           sequence data type (e.g.: 'cbor')\n",
    "            sloc:           data folder\n",
    "            snam:           sequence name\n",
    "            lsrc:           GT label source\n",
    "            sequence_count: current number of sequences\n",
    "\n",
    "        Returns:\n",
    "            new number of sequences\n",
    "        \"\"\"\n",
    "        # remember handlers to release them when Generator is closed\n",
    "        self.datasets.append((sloc, snam))\n",
    "        # parse cbor file\n",
    "        cbor_handler = CBOR_Handler.get_handler(sloc, snam)\n",
    "\n",
    "        # create data package list\n",
    "        data_packages = []\n",
    "        # use offset corrections to put lidar timestamps to front pass to minimize difference between lidar and radar (and align camera)\n",
    "        offset_correction_lidar = 0  # 50\n",
    "        # parse cameras\n",
    "        if \"CAMERA\" in self.data2load:\n",
    "            camera_data = cbor_handler.get_data().get(\"camera\", None)\n",
    "            if camera_data:\n",
    "                # resolve sensor name entries in data2load\n",
    "                camera_idx_to_name = [None] * len(camera_data)\n",
    "                for cd in camera_data.values():\n",
    "                    camera_idx_to_name[cd[\"internal-identifier\"]] = cd[\"name\"]\n",
    "                self.resolve_str_in_data2load(camera_idx_to_name, \"CAMERA\")\n",
    "                # get data\n",
    "                for cam_id, cam_name in enumerate(camera_idx_to_name):\n",
    "                    if self.data2load[\"CAMERA\"] == [] or cam_id in self.data2load[\"CAMERA\"]:\n",
    "                        cam_gp = camera_data[cam_name]\n",
    "                        time = np.array(cam_gp[\"timestamps\"]) * 1000\n",
    "                        data_packages += [\n",
    "                            Data_package(typ=\"CAMERA\", sensor_id=cam_id, sensor_name=cam_name, time=time[i], data_idx=i, scan_id=None)\n",
    "                            for i in range(time.shape[0])\n",
    "                        ]\n",
    "            else:\n",
    "                print(\"\\033[93mWARNING: No camera file found for log %s!\\033[0m\" % snam)\n",
    "\n",
    "        # parse lidar\n",
    "        if \"LIDAR\" in self.data2load:\n",
    "            lidar_data = cbor_handler.get_data().get(\"lidar\", None)\n",
    "            if lidar_data:\n",
    "                # resolve sensor name entries in data2load\n",
    "                lidar_idx_to_name = sorted(lidar_data.keys())\n",
    "                self.resolve_str_in_data2load(lidar_idx_to_name, \"LIDAR\")\n",
    "                # get data\n",
    "                for lid_id, lid_name in enumerate(lidar_idx_to_name):\n",
    "                    if self.data2load[\"LIDAR\"] == [] or lid_id in self.data2load[\"LIDAR\"]:\n",
    "                        lid_gp = lidar_data[lid_name]\n",
    "                        time = np.array(lid_gp[\"timestamps\"]) * 1000 + offset_correction_lidar\n",
    "                        data_packages += [\n",
    "                            Data_package(typ=\"LIDAR\", sensor_id=lid_id, sensor_name=lid_name, time=time[i], data_idx=i, scan_id=None)\n",
    "                            for i in range(time.shape[0])\n",
    "                        ]\n",
    "\n",
    "        # parse fused radar\n",
    "        if \"FUSEDRADAR\" in self.data2load:\n",
    "            network_data = cbor_handler.get_data().get(\"network_input\", None)\n",
    "            radar_data = cbor_handler.get_data().get(\"radar\", None)\n",
    "            if network_data and radar_data and \"network\" in network_data:\n",
    "                # get handles\n",
    "                metadata = network_data[\"network\"][\"metadata\"]\n",
    "                # resolve sensor name entries in data2load\n",
    "                radar_idx_to_name = [None] * len(radar_data)\n",
    "                for rd in radar_data.values():\n",
    "                    radar_idx_to_name[rd[\"internal-identifier\"]] = rd[\"name\"]\n",
    "                self.resolve_str_in_data2load(radar_idx_to_name, \"FUSEDRADAR\")\n",
    "                batch2radar_map = -np.ones([len(metadata), len(radar_idx_to_name)], dtype=int)\n",
    "                max_scan_id = 2**16 - 1\n",
    "                scan_ids = np.array([md[\"scan_idx\"] for md in metadata])\n",
    "                assert np.max(scan_ids) <= max_scan_id, \"Scan IDs are expected to be 16bit only\"\n",
    "                scan_ids_maxed = np.maximum.accumulate(scan_ids, axis=0)\n",
    "                scan_ids_extended = scan_ids.copy()\n",
    "                scan_ids_extended[np.logical_and(scan_ids_extended < scan_ids_maxed, scan_ids_extended != -1)] += max_scan_id + 1\n",
    "                fr_timestamps = network_data[\"network\"][\"timestamps\"]\n",
    "                for rad_id, rad_name in enumerate(radar_idx_to_name):\n",
    "                    # get left neighbors in time for each \"radar batch\" timestamp\n",
    "                    rad_scan_ids = np.array(radar_data[rad_name][\"scan_id\"])\n",
    "                    assert np.max(rad_scan_ids) <= max_scan_id, \"Scan IDs are expected to be 16bit only\"\n",
    "                    valid = scan_ids[:, rad_id] != -1\n",
    "                    rad_scan_ids_maxed = np.maximum.accumulate(rad_scan_ids)\n",
    "                    rad_scan_ids_expanded = rad_scan_ids.copy()\n",
    "                    rad_scan_ids_expanded[np.logical_and(rad_scan_ids < rad_scan_ids_maxed, rad_scan_ids != -1)] += max_scan_id + 1\n",
    "                    batch2radar_map[valid, rad_id] = np.searchsorted(rad_scan_ids_expanded, scan_ids_extended[valid, rad_id])\n",
    "\n",
    "                # load batch to radar frame mapping\n",
    "                time = np.array(fr_timestamps) * 1000\n",
    "                scan_id = scan_ids[:, 0]\n",
    "                # get data\n",
    "                for i in range(len(fr_timestamps)):\n",
    "                    # add subsensor information\n",
    "                    sub_sensor_packages = dict()\n",
    "                    for sd, rad_name in enumerate(radar_idx_to_name):\n",
    "                        if self.data2load[\"FUSEDRADAR\"] == [] or sd in self.data2load[\"FUSEDRADAR\"]:\n",
    "                            if batch2radar_map[i, sd] >= 0:\n",
    "                                radar_frm = batch2radar_map[i, sd]\n",
    "                                sub_sensor_packages[sd] = {\n",
    "                                    \"sensor_name\": rad_name,\n",
    "                                    \"timestamp\": radar_data[rad_name][\"timestamps\"][radar_frm] * 1000,\n",
    "                                    \"frame_id\": radar_frm,\n",
    "                                    \"scan_id\": scan_ids[i, sd],\n",
    "                                }\n",
    "                            else:\n",
    "                                sub_sensor_packages[sd] = None\n",
    "                    # add data package\n",
    "                    data_packages.append(\n",
    "                        Data_package(\n",
    "                            typ=\"FUSEDRADAR\", sensor_id=0, sensor_name=\"Fused\", time=time[i], data_idx=i, scan_id=scan_id[i], subsensors=sub_sensor_packages\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        # parse radar\n",
    "        if \"RADAR\" in self.data2load:\n",
    "            radar_data = cbor_handler.get_data().get(\"radar\", None)\n",
    "            if radar_data:\n",
    "                # resolve sensor name entries in data2load\n",
    "                radar_idx_to_name = [None] * len(radar_data)\n",
    "                for rd in radar_data.values():\n",
    "                    radar_idx_to_name[rd[\"internal-identifier\"]] = rd[\"name\"]\n",
    "                self.resolve_str_in_data2load(radar_idx_to_name, \"RADAR\")\n",
    "                # get data\n",
    "                for rad_id, rad_name in enumerate(radar_idx_to_name):\n",
    "                    if self.data2load[\"RADAR\"] == [] or rad_id in self.data2load[\"RADAR\"]:\n",
    "                        rad_gp = radar_data[rad_name]\n",
    "                        time = np.array(rad_gp[\"timestamps\"]) * 1000\n",
    "                        scan_id = rad_gp[\"scan_id\"]\n",
    "                        for i in range(len(rad_gp[\"timestamps\"])):\n",
    "                            data_packages.append(\n",
    "                                Data_package(typ=\"RADAR\", sensor_id=rad_id, sensor_name=rad_name, time=time[i], data_idx=i, scan_id=scan_id[i])\n",
    "                            )\n",
    "\n",
    "        # get sensor map\n",
    "        sensor_map = {\"LABEL\": None}\n",
    "        timestamp_map = {}\n",
    "        subsensor_map = {}\n",
    "        scanid_map = {}\n",
    "        sensor_name_map = {}\n",
    "        for dp in data_packages:\n",
    "            if dp.type not in sensor_map:\n",
    "                sensor_map[dp.type] = {}\n",
    "                timestamp_map[dp.type] = {}\n",
    "                subsensor_map[dp.type] = {}\n",
    "                scanid_map[dp.type] = {}\n",
    "                sensor_name_map[dp.type] = {}\n",
    "            if dp.sensor_id not in sensor_map[dp.type]:\n",
    "                sensor_map[dp.type][dp.sensor_id] = None\n",
    "                timestamp_map[dp.type][dp.sensor_id] = None\n",
    "                subsensor_map[dp.type][dp.sensor_id] = None\n",
    "                scanid_map[dp.type][dp.sensor_id] = None\n",
    "                sensor_name_map[dp.type][dp.sensor_id] = None\n",
    "\n",
    "        # sort by timestamps\n",
    "        data_packages.sort(key=lambda x: x.create_sort_key())\n",
    "\n",
    "        # feed data packages to meta data list and fill aligned data\n",
    "        framenumber = 0\n",
    "        # last_label_location = None\n",
    "        for dp in data_packages:\n",
    "            # update maps\n",
    "            timestamp_map[dp.type][dp.sensor_id] = dp.time\n",
    "            sensor_map[dp.type][dp.sensor_id] = dp.data_idx\n",
    "            subsensor_map[dp.type][dp.sensor_id] = dp.subsensors\n",
    "            scanid_map[dp.type][dp.sensor_id] = dp.scan_id\n",
    "            sensor_name_map[dp.type][dp.sensor_id] = dp.sensor_name\n",
    "            # # create respective labelfile name\n",
    "            # if 'LABEL' in self.data2load:\n",
    "            #     if dp.type == 'FUSEDRADAR':\n",
    "            #         sensor_map['LABEL'] = {'file': sloc + '/Labels/' + sn + '.h5', 'frame': dp.data_idx, 'source': lsrc, 'sensor': dp.sensor_name}\n",
    "            #         last_label_location = sensor_map['LABEL']\n",
    "            #     elif dp.type == 'LIDAR':\n",
    "            #         # sensor_map['LABEL'] = last_label_location\n",
    "            #         sensor_map['LABEL'] = {'file': sloc + '/Labels_Sensorwise/' + sn + '.h5', 'frame': dp.data_idx, 'source': lsrc, 'sensor': dp.sensor_name}\n",
    "            #         last_label_location = sensor_map['LABEL']\n",
    "            #     elif dp.type == 'CAMERA':\n",
    "            #         sensor_map['LABEL'] = last_label_location\n",
    "            #     elif dp.type == 'RADAR':\n",
    "            #         sensor_map['LABEL'] = {'file': sloc + '/Labels_Sensorwise/' + sn + '.h5', 'frame': dp.data_idx, 'source': lsrc, 'sensor': dp.sensor_name}\n",
    "            #         last_label_location = sensor_map['LABEL']\n",
    "\n",
    "            # create meta data entries, field framenumber will be replaced afterwards\n",
    "            self.file_meta_data_list.append(\n",
    "                DataProviderMetaData(\n",
    "                    sensor_type=dp.type,\n",
    "                    sensor_id=dp.sensor_id,\n",
    "                    sensor_name=dp.sensor_name,\n",
    "                    source_type=styp,\n",
    "                    folder=sloc,\n",
    "                    sequence_name=snam,\n",
    "                    sequence_number=sequence_count,\n",
    "                    aligned_sensor_names=copy.deepcopy(sensor_name_map),\n",
    "                    aligned_fids=copy.deepcopy(sensor_map),\n",
    "                    aligned_timestamps=copy.deepcopy(timestamp_map),\n",
    "                    aligned_sids=copy.deepcopy(scanid_map),\n",
    "                    aligned_subsensors=copy.deepcopy(subsensor_map),\n",
    "                    framenumber=framenumber,\n",
    "                    data_handlers={\"cbor_handler\": cbor_handler},\n",
    "                    flags=dict(),\n",
    "                )\n",
    "            )\n",
    "            # increase frame counter\n",
    "            framenumber += 1\n",
    "\n",
    "        # increment sequence counter\n",
    "        sequence_count += 1\n",
    "\n",
    "        # report current sequence counter\n",
    "        return sequence_count\n",
    "\n",
    "    def resolve_str_in_data2load(self, idx_to_name: List[str], sensor_type: str):\n",
    "        \"\"\"\n",
    "        resolve string name entries in data2load\n",
    "\n",
    "        Args:\n",
    "            idx_to_name: name lookup\n",
    "            sensor_type: sensor type\n",
    "        \"\"\"\n",
    "        # resolve sensor name entries in data2load\n",
    "        sensor_lookup = {sname.decode(\"UTF-8\") if type(sname) == bytes else sname: sid for sid, sname in enumerate(idx_to_name)}\n",
    "        for i, sid in enumerate(self.data2load[sensor_type]):\n",
    "            if type(sid) is str:\n",
    "                if sid in sensor_lookup:\n",
    "                    self.data2load[sensor_type][i] = sensor_lookup[sid]\n",
    "                else:\n",
    "                    print(\"WARNING: Sensor name %s not found in data\" % sid)\n",
    "\n",
    "    def get_current_radar_version(self) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        get current radar data version\n",
    "\n",
    "        Returns:\n",
    "            radar data version if exists, None otherwise\n",
    "        \"\"\"\n",
    "        return self.radar_versions[self.current_radar_version_index]\n",
    "\n",
    "    \"\"\"\n",
    "    set current radar version\n",
    "    \"\"\"\n",
    "\n",
    "    def set_current_radar_version(self, ver: str):\n",
    "        \"\"\"\n",
    "        set current radar data version\n",
    "\n",
    "        Args:\n",
    "            ver: radar data version\n",
    "        \"\"\"\n",
    "        if ver in self.radar_versions:\n",
    "            # set version\n",
    "            self.current_radar_version_index = self.radar_versions.index(ver)\n",
    "            # kill cache to prevent loading data of old version from cache\n",
    "            self.clear_data_cache()\n",
    "        else:\n",
    "            print(\"WARNING: Radar version {} not available\".format(ver))\n",
    "\n",
    "    \"\"\"\n",
    "    toggle radar version\n",
    "    \"\"\"\n",
    "\n",
    "    def toggle_current_radar_version(self):\n",
    "        \"\"\"\n",
    "        toggle radar data version\n",
    "        \"\"\"\n",
    "        self.set_current_radar_version(self.radar_versions[(self.current_radar_version_index + 1) % len(self.radar_versions)])\n",
    "\n",
    "    def get_radar_versions(self) -> List[Optional[str]]:\n",
    "        \"\"\"\n",
    "        get list of radar data versions\n",
    "\n",
    "        Returns:\n",
    "            copy of radar data versions list\n",
    "        \"\"\"\n",
    "        return copy.deepcopy(self.radar_versions)\n",
    "\n",
    "    def add_data_manipulator(self, dm_class: DataManipulator, dm_params: Dict[Any, Any] = dict()):\n",
    "        \"\"\"\n",
    "        add a data manipulator\n",
    "\n",
    "        Args:\n",
    "            dm_class:   manipulator to register\n",
    "            dm_params:  individual manipulator constuctor input arguments\n",
    "        \"\"\"\n",
    "        # check input\n",
    "        if not (issubclass(dm_class, DataManipulator) and type(dm_params) is dict):\n",
    "            raise ValueError(\"Input class derived from DataManipulator and argument dict for calling constructor in DataProviderGenerator.add_data_manipulator\")\n",
    "        self.data_manipulators.append((dm_class, dm_params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Radar(object):\n",
    "\n",
    "    # static variables\n",
    "    pointcloud_rows = [\n",
    "        \"X\",\n",
    "        \"Y\",\n",
    "        \"elevation_deg_vcs\",\n",
    "        \"RCS\",\n",
    "        \"Speed\",\n",
    "        \"doppl_propasal_0\",\n",
    "        \"doppl_propasal_1\",\n",
    "        \"doppl_propasal_2\",\n",
    "        \"doppl_propasal_3\",\n",
    "        \"doppl_propasal_4\",\n",
    "        \"looktype\",\n",
    "        \"isSingleTarget\",\n",
    "        \"radar_idx\",\n",
    "        \"timestamp\",\n",
    "        \"fid\",\n",
    "        \"label\"\n",
    "    ]\n",
    "\n",
    "    def __init__(self, pointcloud, radar_calib, timestamp=None, frame_id=None, scan_id=None, sensor_name=None, version=\"v0\", subsensors=None):\n",
    "        self.pointcloud = pointcloud  # see Radar.pointcloud_rows\n",
    "        self.radar_calib = radar_calib  # dict: key=sensor, value=RadarCalib\n",
    "        self.ego_transform = np.identity(4)\n",
    "        self.ego_transform_to_previous = np.identity(4)\n",
    "        self.v_lon = 0\n",
    "        self.v_lat = 0\n",
    "        self.yawrate = 0\n",
    "        self.looktype = None\n",
    "        self.is_fused = subsensors is not None  # fused from multiple sensors?\n",
    "        self.subsensors = subsensors  # information of sensors Radar is fused from\n",
    "        self.timestamp = timestamp\n",
    "        self.scan_id = scan_id\n",
    "        self.frame_id = frame_id\n",
    "        self.sensor_name = sensor_name\n",
    "        self.version = version\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        equal = np.array_equal(self.pointcloud, other.pointcloud)\n",
    "        equal = equal & np.array_equal(self.ego_transform, other.ego_transform)\n",
    "        equal = equal & np.array_equal(self.ego_transform_to_previous, other.ego_transform_to_previous)\n",
    "        equal = equal & (self.v_lon == other.v_lon)\n",
    "        equal = equal & (self.v_lat == other.v_lat)\n",
    "        equal = equal & (self.yawrate == other.yawrate)\n",
    "        # equal = equal & (self.radar_calib == other.radar_calib)\n",
    "        equal = equal & (self.looktype == other.looktype)\n",
    "        equal = equal & (self.is_fused == other.is_fused)\n",
    "        equal = equal & (self.subsensors == other.subsensors)\n",
    "        equal = equal & (self.timestamp == other.timestamp)\n",
    "        equal = equal & (self.frame_id == other.frame_id)\n",
    "        equal = equal & (self.scan_id == other.scan_id)\n",
    "        equal = equal & (self.sensor_name == other.sensor_name)\n",
    "        equal = equal & (self.version == other.version)\n",
    "\n",
    "        return equal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
